--- 
title: "Statistical Inference"
subtitle: "Data Analyst Handbook"
author: "Michael Foley"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output:
  bookdown::gitbook:
    toc_depth: 2
    fig_caption: true
    lib_dir: assets
    split_by: section
    config:
      toc:
        collapse: subsection
        scroll_highlight: yes
        before: null
        after: null
      toolbar:
        position: static
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "Notes cobbled together from books, online classes, etc. to be used as quick reference for common work projects."
---

# Preface {-}

These are notes from books, classes, tutorials, vignettes, etc. They contain mistakes, are poorly organized, and are sloppy on fundamentals. They should improve over time, but that's all I can say for it. *Use at your own risk.*

The focus of this handbook is statistical inference, including population estimates, group comparisons, and regression modeling. Not included here: [probability](https://bookdown.org/mpfoley1973/probability/), [supervised ML](https://bookdown.org/mpfoley1973/supervised-ml/), [unsupervised ML](https://bookdown.org/mpfoley1973/unsupervised-ml/), [text mining](https://bookdown.org/mpfoley1973/text-mining/), [time series](https://bookdown.org/mpfoley1973/time-series/), [survey analysis](https://bookdown.org/mpfoley1973/survey/), or [survival analysis](https://bookdown.org/mpfoley1973/survival/). These subjects frequently arise at work, but are distinct enough and large enough to warrant separate handbooks.

Statistical inference is the use of a sample's distribution to describe the population distribution. Hypothesis tests, confidence intervals, and effect size estimates are all examples of statistical inference. 

We wary of published study results. Identical studies might produce significant and non-significant results, yet only the significant result is likely to reach publication (publication bias). The researcher may have tortured the data until they found a statistically significant result. The study might suffer from low statistical power. Applying principles of inference can mitigate these problems.

There are at least three approaches to establishing statistical inference: frequentist, likelihood, and Bayesian. Think of them philosophically. The frequentist approach is the path of *action*. It rejects a null hypothesis if the *p*-value is low because repeated sample analyses are likely to agree. The likelihood approach is the path of *knowledge*. It compares the observed summary measure to the likelihoods of the other possible realities. The Bayesian approach is the path of *belief*. It uses a summary measure to update the prior belief.

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')

knitr::opts_chunk$set(fig.height = 3.5)
```

<!--chapter:end:index.Rmd-->

# Frequentist Statistics

```{r include=FALSE}
library(tidyverse)
library(glue)
library(pwr)
library(scales)

set.seed(123456)

theme_set(
  theme_light()
)
```

## Central Tendancy and Dispersion

Suppose you have a data set with $j = [1 .. p]$ variables. Each variable will have a distribution that can be characterized by its mean and variance. If you consider them together, you can see how variance and covariance are related. For the matrix algebra that follows, assume the data are organized in _rows_, so $X_j$ is a _row vector_ of $n$ observations. $X_{ij}$ refers to column $i$ of row $j$.

The mean of row vector $X_j$ is $\bar{x}_j = \frac{1}{n} \sum_{i = 1}^n X_{ij}$. $\bar{x}_j$ estimates the population mean, $\mu_j = E(X_j)$. The collection of means are a column vector.

$$\boldsymbol{\bar{x}} = \begin{pmatrix} \bar{x}_1 \\ \bar{x}_2 \\ \cdots \\ \bar{x}_p  \end{pmatrix}$$

The variance of row vector $X_j$ is the average squared difference from the mean, $s_j^2 = \frac{1}{n-1} \sum_{i=1}^n (X_{ij} - \bar{x}_j)^2$. $s_j^2$ estimates the population variance, $\sigma_j^2 = E(X_j - \mu_j)^2$. Again, the collection is represented as a column vector,

$$\boldsymbol{s}^2 = \begin{pmatrix} s_1^2 \\ s_2^2 \\ \cdots \\ s_p^2  \end{pmatrix}$$

The square root of $s^2$ is called the standard deviation. The concept of variance can be extended to pairs of variables, $j$ and $k$. The covariance of $X_j$ and $X_k$ is the average product of differences from their respective means, $s_{jk} = \frac{1}{n-1} \sum_{i=1}^n (X_{ij} - \bar{x}_j) (X_{ik} - \bar{x}_k)$. $s_{jk}$ estimates the population covariance, $\sigma_{jk} = E\{ (X_{ij} - \mu_j) (X_{ik} - \mu_k)\}$. Notice how the sign of $s_{jk}$ tells you how the variables relate. It's positive if when one variable is larger than its mean, so is the other. It's zero if the value of one variable tells you nothing about the other. It can be shown that $s_{jk}$ is equivalently expressed as 

$$
s_{jk} = \frac{1}{n-1} \left[ \sum_{i=1}^n X_{ij}X_{ik} - \frac{\sum_{i = 1}^n X_{ij} \sum_{i = 1}^n X_{ik}}{n} \right]
$$

This is how it is actually calculated. The first term is dot product, $X_j \cdot X_k$. The second term is the product of the averages. Use matrix algebra to generalize across all $p$ variables to form the _variance-covariance matrix_.

$$
\begin{align}
S &= \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{x}) (X_i - \bar{x})' \\
&= \frac{1}{n-1} \left[ \sum_{i=1}^n X_i X_i^{'} - \frac{\sum_{i = 1}^n X_i \sum_{i = 1}^n X_i}{n} \right]
\end{align}
$$

$S$ estimates the population variance-covariance matrix, $\boldsymbol{\Sigma}$. Divide the covariances by their product of their standard deviations to get their _correlation_, $r_{jk} = \frac{s_{jk}}{s_j s_k}$. $r_{jk}$ estimates the population correlation, $\rho_{jk} = \frac{\sigma_{jk}}{\sigma_j \sigma_k}$.

## Hypothesis Testing

*P*-values express how surprising a summary measure is given the null hypothesis (H0). Suppose you hypothesize that IQs have increased from the intended mean of $\mu$ = 100, $\sigma$ = 15. H0 is $\mu_0$ = 100 with alternative H1 that $\mu$ > 100. Suppose also that you are *right*: $\mu$ is actually 106.

```{r}
set.seed(123456)

mu_0 <- 100
sigma <- 15
mu <- 106

# Create a population of size N with values plucked from normal dist centered at
# mu_0 and mu.
N <- 1E6 # a big number
pop_100 <- data.frame(person = seq(1, N), iq = rnorm(N, mu_0, sigma))
pop_106 <- data.frame(person = seq(1, N), iq = rnorm(N, mu, sigma))
```

The presumed distribution of IQs might look like this.

```{r echo=FALSE}
pop_100 %>%
  ggplot(aes(x = iq)) +
  geom_density() +
  labs(title = "H0: population IQs are centered at M = 100, SD = 15.")
```

Take a random sample of *n* = 30 IQs from the population. 

```{r collapse=TRUE}
n <- 30

x <- sample(pop_106$iq, n)

(x_bar <- mean(x)) %>% scales::comma(.1)
(s <- sd(x)) %>% scales::comma(.1)
```

The sample has $\bar{x}$ = `r comma(x_bar, .1)`, SD = `r comma(s, .1)`. How surprising is this if H0 is true? I.e., what is the *probability* of observing `r comma(x_bar, .1)` when $\mu$ is `r comma(mu_0, 1)`? According to the Central Limit Theorem (CLM), repeated samples of size *n* from a large population will yield $\bar{x}$ values that approach a normal distribution centered at $\mu$ with a standard deviation equal to the the *standard error*, $SE = \frac{\sigma}{\sqrt{n}}$ where $\sigma$ is the population SD. In this example, *n* = 30, so *SE* = $15/\sqrt{30}$ = `r comma(sigma / sqrt(n), .1)` and repeated samples would be expected to be distributed $\sim N(`r comma(mu_0, .1)`, `r comma(sigma/sqrt(n), .1)`^2).$ You can verify this empirically with a simulation.

```{r collapse=TRUE, echo=FALSE}
# 1000 sample means centered at the presumed mean of 100 with SD = 15/
sim <- replicate(1000, mean(sample(pop_100$iq, n)))

# 90% of the values are within these limits.
x_quantiles <- quantile(sim, probs = c(.05, .95))

data.frame(iq = sim) %>%
  ggplot(aes(x = iq)) +
  geom_density() +
  geom_vline(xintercept = x_quantiles["5%"], linetype = 2, color = "firebrick") +
  geom_vline(xintercept = mean(sim), linetype = 2, color = "firebrick") +
  geom_vline(xintercept = x_quantiles["95%"], linetype = 2, color = "firebrick") +
  scale_y_continuous(expand = expansion(mult = c(0, .1))) +
  scale_x_continuous(breaks = c(x_quantiles, mean(sim)), label = comma_format(.1)) +
  theme(
    panel.grid = element_blank(),
    axis.text.y = element_blank(), 
    axis.ticks.y = element_blank()
  ) +
  labs(
    title = glue("Distribution of 1,000 samples from popluation with M = 100, SD = 15."),
    subtitle = glue("Sample means are distributed M = {comma(mean(sim), .1)}, ",
                 "SD = {comma(sd(sim), .1)}.") %>% str_wrap(80),
    caption = "90% of values are within within dashed lines."
  )
```

But you measured $\bar{x}$ = `r comma(x_bar, .1)`. The probability of measuring a value $z = \frac{\bar{x} - \mu_0}{\sigma / \sqrt{n}} = \frac{105.6 - 100}{2.7}$ >= `r comma((x_bar - mu_0) / (sigma / sqrt(n)), .1)` standard errors greater than 100 is `r pnorm((x_bar - mu_0) / (sigma / sqrt(n)), lower.tail = FALSE) %>% comma(.001)`. A common threshold probability is $\alpha$ = .05 level of significance. A sample of `r n` from a population with $\mu$ = 100 that has a mean greater than `qnorm(.05, 100, 15 / sqrt(30), lower.tail = FALSE)` = `r qnorm(.05, mu_0, sigma / sqrt(n), lower.tail = FALSE) %>% comma(.1)` would occur with less than 5% probability.

```{r collapse=TRUE}
z <- (x_bar - mu_0) / (sigma / sqrt(n))
critical_value <- qnorm(.95, mu_0, sigma / sqrt(n))
```

```{r warning=FALSE, echo=FALSE}
tibble(
  iq = seq(90, 120, .01),
  density = dnorm(iq, mean = mu_0,  sd = sigma / sqrt(n))
) %>%
  mutate(area = if_else(iq >= critical_value, density, NA_real_)) %>%
  ggplot(aes(x = iq)) +
  geom_line(aes(y = density), color = "gray40") +
  geom_area(aes(y = area), show.legend = FALSE, fill = "gray80") +
  geom_vline(xintercept = mu_0, linetype = 2, color = "gray40") +
  geom_vline(xintercept = x_bar, linetype = 2, color = "forestgreen") +
  annotate("text", x = critical_value + .5, y = .0055, label = "alpha", parse = TRUE, 
           size = 5, color = "gray40") +
  scale_y_continuous(expand = expansion(mult = c(0, .1))) +
  scale_x_continuous(breaks = c(90, mu_0, critical_value, x_bar, 110, 120), 
                     label = comma_format(.1)) +
  theme(
    panel.grid = element_blank(),
    axis.text.y = element_blank(), 
    axis.ticks.y = element_blank(),
    axis.text.x = element_text(angle = 90, vjust = .5)
  ) +
  labs(title = "X-bar is in the .05 significance level region of H0.", x = "IQ")
```

The sampling mean is $\bar{x}$ = `r comma(x_bar, .1)`, well into the $\alpha$ <= .05 shaded region. The probability of measuring a mean IQ of `r comma(x_bar, .1)` from a sample of size *n* = `r n` when the population mean is `r mu_0` is *p* = `r comma(pnorm(z, lower.tail = FALSE), .001)`, meaning only `r percent(pnorm(z, lower.tail = FALSE), .1)` of the area under the distribution is to the right of `r comma(x_bar, .1)`. Using an $\alpha$ = .05 level of significance, reject H0. This is a *true positive* because the population mean is `r mu`, not `r mu_0`. Imagine a stricter level of significance, shrinking the shaded region right of `r comma(x_bar, .1)`. You would mistakenly fail to reject H0, a *false negative* (Type II error). The $\beta$ blue-shaded region is Type II error probability. Any measured mean in this region would be greater than $mu_0$ = 100, yet not be rejected by the statistical test. 

### Type I and II Errors {-}

Either H0 or H1 is correct, and you must choose to either reject or not reject H0. That means there are four possible states at the end of your analysis. If your summary measure is extreme enough for you to declare a "positive" result and reject H0, you are either correct (true positive) or incorrect (false positive). False positives are called *Type I errors*. Alternatively, if it is *not* extreme enough, you are either correct (true negative) or incorrect (false negative). False negatives are called *Type II errors*.

The probabilities of landing in these four states depend on your chosen significance level, $\alpha$, and on the statistical power of the study, 1 - $\beta$.

|                        |H0 True                       |H1 True                      |
|------------------------|------------------------------|-----------------------------|
|Significance test is positive, so you reject H0.| False Positive <br>Type I Error<br> Probability = $\alpha$    | True Positive <br>Good Call! <br> Probability = 1 - $\beta$ |
|Significance test is negative, so you do *not* reject H0.   | True Negative <br> Good Call!<br> Probability = ($1 - \alpha$)    | False Negative <br>Type II Error <br>Probability = $\beta$ |

$\alpha$ is the expected Type I error rate - extreme summary measures occurring by chance. $\beta$ is the expected Type II error rate - summary measures that by chance were not extreme enough to reject H0. In the IQ example, if the population mean was $\mu$ = `r mu_0`, any sample mean greater than `qnorm(.95, 100, 15/sqrt(n))` = `r scales::number(qnorm(.95, mu_0, sigma / sqrt(n)), accuracy = .1)` would be mistakenly rejected at the $\alpha$ = .05 significance level, a false positive. If the population mean was $\mu$ = `r mu`, any sample mean less than `r comma(qnorm(.95, mu_0, sigma / sqrt(n)), .1)` would be mistakenly _not_ rejected, a false negative.

The plot below shows how the sampling distributions of the null hypothesis and a potentially different true population overlap to create the four zones. The first facet uses a sample size of *n* = 30. The second uses *n* = 50, resulting in tighter distributions.

- If the presumed (H0) population value is accurate, then your risk is in accidentally rejecting H0. Any sample mean in the $\alpha$ region would incorrectly reject H0 (Type I).

- If the presumed (H0) population value is wrong, and the true value is actually the (unknown) gold color population to the right, then your risk is in failing to reject H0. Any sample mean in the $\beta$ region would incorrectly fail to reject H0 (Type II).

```{r warning=FALSE, echo=FALSE}
lbl <- tibble(
  `Sample Size` = c(rep("n = 30", 2), rep("n = 50", 2)),
  iq = c(103, 105, 102.5, 104),
  lbl = c("beta", "alpha", "beta", "alpha")
)

tibble(
  iq = rep(seq(90, 120, .01), 2),
  n = c(rep(30, 3001), rep(50, 3001)),
  `Sample Size` = map_chr(n, ~paste("n =", .)),
  Presumed = map2_dbl(iq, n, ~dnorm(.x, mean = mu_0, sd = sigma / sqrt(.y))),
  Alternative = map2_dbl(iq, n, ~dnorm(.x, mean = mu, sd = sigma / sqrt(.y))),
  iq_crit = map_dbl(n, ~qnorm(.95, mean = mu_0, sd = sigma / sqrt(.x)))
) %>%
  pivot_longer(cols = -c(`Sample Size`, iq, iq_crit, n), 
               names_to = "curve", values_to = "density") %>%
  mutate(area = if_else(iq >= iq_crit & curve == "Presumed" | 
                          iq < iq_crit & curve == "Alternative", 
                        density, NA_real_)) %>%
  ggplot(aes(x = iq)) +
  geom_area(aes(y = area, fill = curve), show.legend = FALSE) +
  geom_line(aes(y = density, color = curve)) +
  geom_vline(xintercept = mu_0, linetype = 2, color = "gray40") +
  geom_vline(xintercept = mu, linetype = 2, color = "goldenrod") +
  geom_vline(xintercept = x_bar, linetype = 2, color = "forestgreen") +
  geom_text(data = lbl, aes(y = .015, label = lbl), parse = TRUE, size = 4.5, color = "gray40") +
  scale_x_continuous(breaks = c(90, mu_0, 
                                qnorm(.95, mean = mu_0, sd = sigma / sqrt(30)),
                                qnorm(.95, mean = mu_0, sd = sigma / sqrt(50)),
                                x_bar, 110, 120), 
                     label = comma_format(.1)) +
  scale_y_continuous(expand = expansion(mult = c(0, .1))) +
  scale_color_manual(values = c("goldenrod", "gray40")) +
  scale_fill_manual(values = c("lightgoldenrod", "gray80")) +
  facet_wrap(facets = vars(`Sample Size`), ncol = 1) +
  theme(
    panel.grid = element_blank(),
    axis.text.y = element_blank(), 
    axis.ticks.y = element_blank(),
    axis.text.x = element_text(angle = 90, vjust = .5),
    legend.position = "top"
  ) +
  labs(title = "X-bar is in the .05 significance level region of H0.", 
       color = NULL, x = "IQ")
```

You don't see much discussion of $\beta$ is reports because $\beta$ is an unknown value based on the true population whose center you do not know. $\beta$ is relevant at the _design_ stage where it informs how large your sample size needs to be in order to reject H0. In this example, $\mu$ is `r mu - mu_0` points larger than $\mu_0$. With a sample size of *n* = 30, the sample mean needs to be at least `r qnorm(.95, mean = mu_0, sd = sigma / sqrt(30)) %>% comma(.1)` for you to reject H0 at the $\alpha$ = .05 level. If you want to reject H0 with a smaller difference, you need a larger sample size. A sample size of *n* = 50 lets you reject H0 with a sample mean of only `r qnorm(.95, mean = mu_0, sd = sigma / sqrt(50)) %>% comma(.1)`. Interestingly, you can see how there can be such a thing as too much power. If *n* is large enough, you can reject H0 with a sample mean that is only trivially larger than the presumed value.

### Statistical Power {-}

The ability to detect a difference when it exists (the true positive) is called the power of the test. Its measured by the area outside of $\beta$. Changing *n* from 30 to 50 reduced the area in the $\beta$ region, and therefore increased the power of the test.

Statistical power is an increasing function of sample size, effect size, and significance level. The positive association with significance level means there is a trade-off between Type I and Type II error rates. A small $\alpha$ sets a high bar for rejecting H0, but you run the risk of failing to appreciate a *real* difference. On the other hand, a large $\alpha$ sets a low bar for rejecting H0, but you run the risk of mistaking a random difference as real.

The 1 - $\beta$ statistical power threshold is usually set at .80, similar to the $\alpha$ = .05 level of significance threshold convention. Given a real effect, a study with a statistical power of .80 will only find a positive test result 80% of the time. You may think more power is better, but beware that with a large enough sample size, even trivial effect sizes may yield a positive test result. You need to consider both sides of this coin.

A *power analysis* is frequently used to determine the sample size required to detect a threshold effect size given an $\alpha$ level of significance. A power analysis expresses the relationship among four components. If you know any three, it tells you the fourth: The components are i) 1 - $\beta$, ii) *n*, iii) $\alpha$, and iv) expected effect size, Cohen's d = $(\bar{x} - \mu_0)/\sigma$.

Suppose you set power at .80, significance level at .05, and *n* = 30. What effect size will this design detect?

```{r collapse=TRUE}
(pwr <- pwr::pwr.t.test(
  n = 30,
  sig.level = .05,
  power = 0.80,
  type = "one.sample",
  alternative = "greater"
))
```

An effect size of *d* = `r comma(pwr$d, .001)` will fall in the $\alpha$ = .05 region with probability 1 - $\beta$ = .80 if the sample size is *n* = 30. Multiply *d* = `r comma(pwr$d, .001)` by $\sigma$ = `r sigma` to convert to the IQ units, `r comma(pwr$d * sigma, .1)`. More likely, you will use the power test to detect the required sample size. Suppose you set $1 - \beta$ = .8 and $\alpha = .05$, and want to detect an effect size of $5 / 15$.

```{r collapse=TRUE}
(pwr <- pwr::pwr.t.test(
  d = 5 / 15,
  sig.level = .05,
  power = 0.80,
  type = "one.sample",
  alternative = "greater"
))
```

You need a larger sample, *n* = `r comma(ceiling(pwr$n), 1)`. You can use the power test formula for various *n* sizes to see the relationship with effect size. *Note: the y-axis multiplies Cohen's d by $\sigma$ to get the effect size in original units*.

```{r echo=FALSE}
data.frame(n = rep(10:150, 2),
           power = c(rep(.80, 141), rep(.60, 141))) %>%
  mutate(d = map2_dbl(n, power, ~ pwr.t.test(n = .x, 
                                     sig.level = .05, 
                                     power = .y, 
                                     type = "one.sample", 
                                     alternative = "greater") %>% pluck("d")),
         delta = d * sigma
  ) %>%
  ggplot(aes(x = n, y = delta, group = as.factor(power), color = as.factor(power))) +
  geom_line() +
  annotate("segment", x = 30, xend = 30, y = 0, yend = 7, linetype = 2, size = 1, color = "steelblue") +
  annotate("segment", x = 0, xend = 30, y = 7, yend = 7, linetype = 2, size = 1, color = "steelblue") +
  scale_y_continuous(limits = c(0, NA), breaks = 1:100, expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 155), breaks = seq(0, 150, 10), expand = c(0, 0)) +
  scale_color_manual(values = c("0.6" = "gray80", "0.8" = "steelblue")) +
  theme_light() +
  theme(panel.grid.minor = element_blank(), legend.position = "right") +
  labs(title = paste("A sample size of 30 is required to detect an effect size",
                     "of 7 at a .05 significance \nlevel with 80% probability."),
       subtitle = "Power analysis with power = .80 vs .60 and significance = .05.",
       color = "Power")
```

The dashed lines show a sample size of 30 is required to detect an effect size of 7 at a .05 significance level with 80% probability.

### What p-values would you expect? {-}

What distribution of p-values would you expect if there *is* a true effect and you repeated the study many times? What if there is *no* true effect? The answer is completely determined by the statistical power of the study.^[This section is based on ideas I learned from homework assignment 1 in Daniel Lakens's Coursera class [Improving your statistical inferences](https://www.coursera.org/learn/statistical-inferences/home/welcome).]

To see this, run 100,000 simulations of an experiment measuring the average IQ from a sample of size *n* = 26. The samples will be 26 random values from the normal distribution centered at 106 with a standard deviation of 15. H0 is $\mu$ = 100.

```{r}
# 100,000 random samples of IQ simulations from a normal distribution where
# sigma = 15. True population value is 100, but we'll try other values.
n_sims <- 1E5
mu <- 100
sigma <- 15

run_sim <- function(mu_0 = 106, n = 26) {
  data.frame(i = 1:n_sims) %>%
    mutate(
      x = map(i, ~ rnorm(n = n, mean = mu_0, sd = sigma)),
      z = map(x, ~ t.test(., mu = mu)),
      p = map_dbl(z, ~ .x$p.value),
      x_bar = map_dbl(x, mean)
    ) %>%
    select(x_bar, p)
}
```

```{r echo=FALSE}
plot_sim <- function(x, mu_0 = 106, n = 26, 
                     n_breaks = 20, x_lim = c(0, 1), 
                     p_max = 1.0) {
  test_power = sum(x$p < .05) / n_sims
  x %>% 
    filter(p <= p_max) %>%
    ggplot(aes(x = p)) +
    geom_bar(fill = "lightgoldenrod") + 
    scale_x_binned(n.breaks = n_breaks, limits = x_lim, labels = waiver()) +
    geom_hline(yintercept = n_sims / n_breaks * p_max, color = "firebrick", linetype = 2) +
    annotate("text", x = .8, y = n_sims / n_breaks * p_max, label = "5%") +
    theme_light() +
    scale_y_continuous(labels = scales::comma_format(), limits = c(0, n_sims)) +
    labs(
      title = glue(
        "Measured p-values in {n_sims} simulations of samples of size {n} distributed \n",
        "N({mu_0}, {sigma}^2) with H0 = 100. Power of the test is {scales::percent(test_power)}."
      )
    )
}
```

The null hypothesis is that the average IQ is 100. Our rigged simulation finds an average IQ of 106 - an effect size of 6. 

```{r collapse=TRUE}
sim_106_26 <- run_sim(mu_0 = 106, n = 26)

glimpse(sim_106_26)

mean(sim_106_26$x_bar)
```

The statistical power achieved by the simulations is 50%. That is, the typical simulation detected the effect size of 6 at the .05 significance level about 50% of the time. 

```{r}
pwr.t.test(
  n = 26,
  d = (106 - 100) / 15,
  sig.level = .05,
  type = "one.sample",
  alternative = "two.sided"
)
```

That means that given a population with an average IQ of 106, a two-sided hypothesis test of H0: $\mu$ = 100 from a sample of size 26 will measure an $\bar{x}$ with a *p*-value under .05 only 50% of the time. You can see that in this histogram of *p*-values.

```{r}
sim_106_26 %>% plot_sim()
```

Had there been no effect to observe, you'd expect all *p*-values to be equally likely, so the 20 bins would all have been 5% of the number of simulations -- i.e., *uniformly distributed under the null*. This is called "0 power", although 5% of the *p*-values will still be significant at the .05 level. The 5% of *p*-values < .05 is the Type II error rate - that probability of a positive test result when there is no actual effect to observe.

```{r}
run_sim(mu_0 = 100, n = 26) %>%
  plot_sim(mu_0 = 100)
```

If you want a higher powered study that would detect the effect at least 80% of the time (the normal standard), you'll need a higher sample size. How high? Conduct the power analysis again, but specify the power while leaving out the sample size.

```{r}
pwr.t.test(
  power = 0.80,
  d = (106 - 100) / 15,
  sig.level = .05,
  type = "one.sample",
  alternative = "two.sided"
)
```

You need 51 people (technically, you might want to round up to 52). Here's what that looks like. 80% of *p*-values are below .05 now.

```{r}
run_sim(mu_0 = 106, n = 51) %>%
  plot_sim(mu_0 = 106)
```

So far, we've discovered that when there *is* an effect, the probability that the measure *p*-value is under the $\alpha$ significance level equals the *power of the study*, 1 - $\beta$ - the *true positive* rate, and $\beta$ will be above the $\alpha$ level - the *false negative* rate. We've also discovered that when there is *no* effect, all *p*-values are equally likely, so $\alpha$ of them will be below the $alpha$ level of significance - the *false positive* rate, and 1 - $\alpha$ will be above $\alpha$ - the *true negative* rate.

It's not the case that all p-values below 0.05 are support for the alternative hypothesis. If the statistical power is high enough, a *p*-value just under .05 can be even *less* likely under the null hypothesis.

```{r}
run_sim(mu_0 = 108, n = 51)  %>%
    mutate(bin = case_when(p < .01 ~ "0.00 - 0.01",
                         p < .02 ~ "0.01 - 0.02",
                         p < .03 ~ "0.02 - 0.03",
                         p < .04 ~ "0.03 - 0.04",
                         p < .05 ~ "0.04 - 0.05",
                         TRUE ~ "other")
  ) %>%
  janitor::tabyl(bin)
```

(Recall that under H0, all *p*-values are equally likely, so each of the percentile bins would contain 1% of *p*-values.)

In fact, at best, a *p*-value between .04 and .05 can only be about four times as likely under the alternative hypothesis as the null hypothesis. If your *p*-value is just under .05, it is at best weak support for the alternative hypothesis.

### Further Reading

Pritha Bhandari has two nice posts on [Type I and Type II errors](https://www.scribbr.com/statistics/type-i-and-type-ii-errors/) and [Statistical Power](https://www.scribbr.com/statistics/statistical-power/). Daniel Lakens's Coursera class [Improving your statistical inferences](https://www.coursera.org/learn/statistical-inferences/home/welcome) has a great *p*-value simulation exercise in Week 1 (assignment)

<!--chapter:end:01-frequentist-statistics.Rmd-->

# Likelihood Statistics

```{r include=FALSE}
library(tidyverse)
```

Likelihood functions are an approach to statistical inference (along with Frequentist and Bayesian). Likelihoods are functions of a data distribution parameter. For example, the binomial likelihood function is 

$$L(\theta) = \frac{n!}{x!(n-x)!}\cdot \theta^x \cdot (1-\theta)^{n-x}$$

You can use the binomial likelihood function to assess the likelihoods of various hypothesized population probabilities, $\theta$. Suppose you sample *n* = 10 coin flips and observe *x* = 8 successful events (heads) for an estimated heads probability of .8. The likelihood of a fair coin, $\theta$ = .05 given the evidence is only `r scales::number(dbinom(8, 10, .5), accuracy = .001)`.

```{r}
dbinom(8, 10, .5)
```

You can see from the plot below that the likelihood function is maximized at $\theta$ = 0.8 (likelihood = `r scales::number(dbinom(8, 10, .8), accuracy = .001)`). The actual value of the likelihood is unimportant - it's a density.

```{r echo=FALSE}
data.frame(
  theta = seq(0, 1, .01)
) %>%
  mutate(likelihood = map_dbl(theta, ~ dbinom(x = 8, size = 10, prob = .))) %>%
  ggplot(aes(x = theta, y = likelihood)) +
  geom_line() +
  theme_light() +
  labs(title = "Population proportion likelihoods given 8 successes in sample of n = 10",
       x = expression(theta))
```

You can combine likelihood estimates by multiplying them. Suppose one experiment finds 4 of 10 heads and a second experiment finds 8 of 10 heads. You'd hope two experiments could be combined to achieve the same result as a single experiment with 12 of 20 heads, and that is indeed the case.

```{r collapse=TRUE}
x <- dbinom(4, 10, seq(0, 1, .1))
y <- dbinom(8, 10, seq(0, 1, .1))
z <- dbinom(12, 20, seq(0, 1, .1))

round((x / max(x)) * (y / max(y)), 3)
round(z, 3)
```

Compare competing estimates of $\theta$ with the *likelihood ratio*. The likelihood of $\theta$ = .8 vs $\theta$ = .5 (fair coin) is $\frac{L(\theta = 0.8)}{L(\theta = 0.5)}$ = `r scales::number(dbinom(8, 10, .8) / dbinom(8, 10, .5), accuracy = .01)`.

A likelihood ratio of >= 8 is *moderately strong* evidence for an alternative hypothesis. A likelihood ratio of >= 32 is *strong* evidence for the alternative hypothesis. Keep in mind that likelihood ratios are *relative* evidence of H1 vs H0 - both hypotheses may be quite unlikely!

A set of studies usually include both positive and negative test results. You can see this from the likelihood plots below. These are the likelihood curves produced from x = [0..3] successes in a sample of 3. Think of this as the likelihood of [0..3] positive findings in 3 studies based on an $\alpha$ = .05 level of significance and a .80 1 - $\beta$ statistical power of the study. 

The yellow line at .05 is the likelihood of a Type I error of concluding there is an effect when H1 is false. The yellow line at .80 is the likelihood of a Type II error of concluding there is no effect when H1 is true. The likelihood of 0 of 3 experiments reporting a positive effect under $\alpha$ = .05, 1 - $\beta$ = .80 is much higher under H0 ($\theta$ = .05) than under H1 ($\theta$ = .80): `r scales::number(dbinom(x = 0, size = 3, prob = .05), accuracy = .001)` vs `r scales::number(dbinom(x = 0, size = 3, prob = .80), accuracy = .001)` for a likelihood ratio of `r scales::number(dbinom(x = 0, size = 3, prob = .05) / dbinom(x = 0, size = 3, prob = .80), accuracy = 1)`. The likelihood of 1 of 3 experiments reporting a positive effect is still higher under H0 than under H1: `r scales::number(dbinom(x = 1, size = 3, prob = .05), accuracy = .001)` vs `r scales::number(dbinom(x = 1, size = 3, prob = .80), accuracy = .001)` for a likelihood ratio of `r scales::number(dbinom(x = 1, size = 3, prob = .05) / dbinom(x = 1, size = 3, prob = .80), accuracy = .01)`. For 2 of 3 experiments reporting a positive effect the likelihood ratio is `r scales::number(dbinom(x = 2, size = 3, prob = .05) / dbinom(x = 2, size = 3, prob = .80), accuracy = .001)`, and for 3 of 3 experiments reporting a positive effect the likelihood ratio is `r scales::number(dbinom(x = 3, size = 3, prob = .05) / dbinom(x = 3, size = 3, prob = .80), accuracy = .00001)`.

```{r echo=FALSE}
data.frame(
  theta = seq(0, 1, .01)
) %>%
  mutate(
    `0 of 3` = map_dbl(theta, ~ dbinom(x = 0, size = 3, prob = .)),
    `1 of 3` = map_dbl(theta, ~ dbinom(x = 1, size = 3, prob = .)),
    `2 of 3` = map_dbl(theta, ~ dbinom(x = 2, size = 3, prob = .)),
    `3 of 3` = map_dbl(theta, ~ dbinom(x = 3, size = 3, prob = .))
  ) %>%
  pivot_longer(cols = ends_with(" of 3"), values_to = "likelihood") %>%
  ggplot(aes(x = theta, y = likelihood, linetype = name)) +
  geom_line() +
  geom_vline(xintercept = .05, linetype = 2, size = 1, color = "goldenrod") +
  geom_vline(xintercept = .80, linetype = 2, size = 1, color = "goldenrod") +
  geom_vline(xintercept = 1 - 3 / (3 + 1), linetype = 2, size = .75, color = "steelblue") +
  geom_vline(xintercept = 3 / (3 + 1), linetype = 2, size = .75, color = "steelblue") +
  theme_light() +
  theme(panel.grid = element_blank()) +
  labs(title = "Population proportion likelihoods given x successes in sample of n = 3",
       subtitle = "Yellow lines at .05 and .80, blue at .25 and .75.",
       x = expression(theta), linetype = NULL)
```

The blue lines demarcates the points where mixed results are as likely as unanimous results. A set of studies are likely to produce unanimous results only if the number of studies is fairly high $(\gt 1 - n / (n+1))$ or low $(< n / (n + 1))$. 

## Maximum Likelihood Estimation

Suppose a process $T$ is the time to event of a process following an exponential probability distribution ([notes](https://bookdown.org/mpfoley1973/probability/exponential.html)), $f(T = t; \lambda) = \lambda e^{-\lambda t}$. Fitting a model to the data means estimating the distribution's parameter, $\lambda$. The way this is typically done is by the process of *maximum likelihood estimation* (MLE). MLE compares the observed outcomes to those produced by the range of possible parameter values within the parameter space $\lambda \in \Lambda$ and chooses the parameter value that maximizes the likelihood of producing the observed outcome, $\hat{\lambda} = \underset{\lambda \in \Lambda}{\arg\max} \hat{L}_t(\lambda, t)$.

For the exponential distribution, the likelihood that $\lambda$ produces the observed outcomes is the product of the probability densities for each observation because they are a sequence of independent variables.

$$\begin{eqnarray}
L(\lambda; t_1, t_2, \dots, t_n) &=& f(t_1; \lambda) \cdot f(t_2; \lambda) \cdots f(t_n; \lambda) \\
&=& \Pi_{i=1}^n f(t_i; \lambda) \\
&=& \Pi_{i=1}^n \lambda e^{-\lambda t_i} \\
&=& \lambda^n \exp \left(-\lambda \sum_{i=1}^n t_i \right)
\end{eqnarray}$$

That is difficult to optimize, but the log of it is simple.

$$l(\lambda; t_1, t_2, \dots, t_n) = n \ln(\lambda) - \lambda \sum_{i=1}^n t_i$$

Maximize the log-likelihood equation by setting its derivative to zero and solving for $\lambda$.

$$\begin{eqnarray}
\frac{d}{d \lambda} l(\lambda; t_1, t_2, \dots, t_n) &=& \frac{d}{d \lambda} \left( n \ln(\lambda) - \lambda \sum_{i=1}^n t_i \right) \\
0 &=& \frac{n}{\lambda} - \sum_{i=1}^n t_i \\
\lambda &=& \frac{n}{\sum_{i=1}^n t_i}
\end{eqnarray}$$

$\lambda$ is the reciprocal of the sample mean.

<!--chapter:end:02-likelihood-statistics.Rmd-->

# Bayesian Statistics

```{r include=FALSE}
library(tidyverse)
library(patchwork)
library(glue)
```

Bayesian inference estimates the probability, $\theta$, that an hypothesis is true. It differs from classical (aka, frequentist) inference in its insistence that *all* uncertainties be described by probabilities. Bayesian inference updates the prior probability distribution in light of new information.

Bayesian inference differs from classical inference in three respects. Both methods assume a data generating mechanism expressed as a likelihood. However, classical inference assumes the nature of the mechanism is infinitely repeatable while Bayesian inference treats each event as unique. classical estimates a single population parameter (usually the mean), while Bayesian inference estimates a parameter _distribution_. Finally, the machinery of classical inference is maximizing likelihood and for Bayesian inference it is updating the prior distribution.

Bayesian inference builds on Bayes' Theorem, so let's start there.

## Bayes' Theorem

Bayes' Theorem, is the inverse conditional probability, the probability of the condition given the observed outcome. It reorganizes the relationship between joint probability and conditional probability, $P(\theta D) = P(\theta|D)P(D) = P(D|\theta)P(\theta)$, into 

$$P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}.$$

In common English, the probability that $\theta$ is after observing $D$ is equal to the probability of observing $D$ when $\theta$ is true divided by the probability of observing $D$ under _any_ circumstance.

* $P(\theta)$ is the strength of your belief in $\theta$ *prior* to considering the data $D$.

* $P(D|\theta)$ is the *likelihood* of observing $D$ from a generative model with parameter $\theta$. Note that the likelihood is a probability density, and is not quite the same as probability. For a continuous variable, likelihoods can sum to greater than 1. E.g., `dbinom(seq(1, 100, 1), 100, .5)` sums to `r sum(dbinom(seq(1, 100, 1), 100, .5))`, but `dnorm(seq(0,50,.001), 10, 10)` sums to `r scales::number(sum(dnorm(seq(0,50,.001), 10, 10)), accuracy = 1)`.

* $P(D)$ is the likelihood of observing $D$ from *any* prior. It is the *marginal distribution*, or *prior predictive distribution* of $D$. The likelihood divided by the marginal distribution is the proportional adjustment made to the prior in light of the data.

* $P(\theta|D)$ is the strength of your belief in $\theta$ *posterior* to considering $D$.

Bayes' Theorem is useful for evaluating medical tests. A test's *sensitivity* is its probability of yielding a positive result $D$ when condition $\theta$ exists. $P(D|\theta)$ is a test sensitivity, $\mathrm{sens}$. $P(\theta)$ is the probability of $\theta$ prior to the test (e.g., the general rate in society), $\mathrm{prior}$. The numerator of Bayes' Theorem, the joint probability $P(D \theta) = P(D|\theta)P(\theta)$, is $\mathrm{sens\times prior}$. A test's *specificity* is the probability of observing negative test result $\hat{D}$ when the condition does not exist, $\hat{\theta}$. The specificity is the compliment of a *false positive* test result, $P(\hat{D} | \hat{\theta}) = 1 - P(D | \hat{\theta})$. The denominator of Bayes' Theorem is the overall probability of a positive test result, $P(D) = P(D|\theta)P(\theta) + P(D|\hat\theta)P(\hat\theta)$ or in terms of sensitivity and specificity, $P(D) = \mathrm{(sens \times prior) + (1 - spec)(1 - prior)}$.

**Example.** Suppose E. Coli is typically present in 4.5% of samples, and an E. Coli screen has a sensitivity of 0.95 and a specificity of 0.99. Given a positive test result, what is the probability that E. Coli is actually present?

$$P(\theta|D) = \frac{.95\cdot .045}{.95\cdot .045 + (1 - .99)(1 - .045)} = \frac{.04275}{.04275 + .00955} = \frac{.04275}{.05230} = 81.7\%.$$

The elements of Bayes' Theorem come directly from the contingency table. The first row is the positive test result. The probability of E. Coli is the joint probability of E. coli and a positive test divided by the probability of a positive test

|      |E. Coli|Safe   |Total  |
|------|-------|-------|-------|
|+ Test|.95 * .045 = 0.04275|.01 * .955 = 0.00955|0.05230|
|- Test|.05 * .045 = 0.00225|.99 * .955 = 0.94545|0.94770|
|Total |0.04500|0.95500|1.00000|

## Bayesian Inference

Bayesian inference extends the logic of Bayes' Theorem by replacing the prior probability *estimate* that $\theta$ is true with a prior probability *distribution* that $\theta$ is true. Rather than saying, "I am *x*% certain $\theta$ is true," you say "I believe the probability that $\theta$ is true is somewhere in a range that has maximum likelihood at *x*%".

$$
f(\theta | D) = \frac{f(D|\theta) f(\theta)}{\int_\Theta f(D|\theta) f(\theta) d\theta}
$$

This formula expresses the posterior distribution of $\theta$ as a function of the prior distribution and new information.

Let $\Pi(\theta)$ be the prior probability function. $\Pi(\theta)$ has a PMF or PDF $P(\theta) = f(\theta)$, and a set of conditional distributions, $\{f_\theta(D) = f(D|\theta), \theta \in \Omega\}$, called the *generative model*. $f_\theta(D)$ is the *likelihood* of observing $D$ given $\theta$. The numerator is the product of the likelihood and the PMF/PDF, $f_\theta(D)P(\theta)$, the joint distribution of $(D, \theta)$. The denominator is the marginal distribution, or prior predictive distribution, of $D$: $m(D) = \int_\Omega f_\theta(D)P(\theta) d\theta$. For discrete cases, replace the integral with a sum, $m(D) = \sum\nolimits_\Omega f_\theta(D) P(\theta)$. The *posterior* probability distribution of $\theta$, conditioned on the observance of $D$, $\Pi(\cdot|D)$, is the joint density, $f_\theta(D) P(\theta),$ divided by the the marginal density, $m(D)$.

$$P(\theta | D) = \frac{f_\theta(D) P(\theta)}{m(D)}$$

The numerator makes the posterior proportional to the prior. The denominator is a normalizing constant that scales the likelihood into a proper density function (whose values sum to 1).

It is easier to see how observed evidence shifts the probabilities of the priors into their posterior probabilities by working with discrete priors first. From there it is straight-forward to grasp the more abstract case of continuous prior and posterior distributions.

## Discrete Cases

Suppose you have a string of numbers $[1,1,1,1,0,0,1,1,1,0]$ (7 ones and 3 zeros) produced by a Bernoulli random number generator. What parameter $p$ was used in the Bernoulli function?^[Example borrowed from [chris' sandbox](http://chrisstrelioff.ws/sandbox/2014/12/11/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations.html)]

```{r}
D <- c(1, 1, 1, 1, 0, 0, 1, 1, 1, 0)
```

Your best guess is $p = 0.7$, but how confident are you? Posit eleven competing priors, $\theta = [.0, .1, \ldots, 1]$ with equal prior probabilities, $P(\theta) = [1/11, \ldots]$.

```{r}
theta <- seq(0, 1, by = 0.1)
prior <- rep(1/11, 11)
```

Using a Bernoulli generative model, the likelihood of observing 7 ones and 3 zeros are $P(D|\theta) = \theta^7 + (1-\theta)^3.$

```{r fig.height=2.5}
likelihood <- theta^7 * (1 - theta)^3

data.frame(theta, likelihood) %>% 
  ggplot() +
  geom_segment(aes(x = theta, xend = theta, y = 0, yend = likelihood), 
               linetype = 2, color = "steelblue") +
  geom_point(aes(x = theta, y = likelihood), color = "steelblue", size = 3) +
  scale_x_continuous(breaks = theta) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank()) +
  labs(title = expression(paste("Maximum likelihood of observing D is at ", theta, " = 0.7.")),
       x = expression(theta), y = expression(f[theta](D)))
```

The posterior probability is the likelihood divided by the marginal probability of observing $D$ multiplied by the prior, $P(\theta|D) = \frac{P(D|\theta)}{P(D)}\cdot P(\theta).$ In this case, the marginal probability is straight-forward to calculate: it is the sum-product of the priors and their associated likelihoods.

```{r}
posterior <- likelihood / sum(likelihood * prior) * prior
```

```{r fig.height=3, echo=FALSE}
data.frame(theta, prior, posterior) %>% 
  pivot_longer(cols = c(prior, posterior), values_to = "pi") %>%
  ggplot() +
  geom_point(aes(x = theta, y = pi, color = name), size = 1) +
  geom_line(aes(x = theta, y = pi, color = name), size = 1) +
  scale_x_continuous(breaks = theta) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank()) +
  labs(title = "Posterior probabilities are adjusted priors.", 
       color = NULL,
       x = expression(theta), y = "P")
```

What would the posterior look like if we started with an educated guess on $P(\theta)$ that more heavily weights $\theta = 0.7$?

```{r}
prior <- c(.05, .05, .05, .05, .05, .10, .15, .20, .15, .10, .05)
posterior <- likelihood / sum(likelihood * prior) * prior
```

```{r fig.height=3, echo=FALSE}
data.frame(theta, prior, posterior) %>% 
  pivot_longer(cols = c(prior, posterior), values_to = "pi") %>%
  ggplot() +
  geom_point(aes(x = theta, y = pi, color = name), size = 1) +
  geom_line(aes(x = theta, y = pi, color = name), size = 1) +
  scale_x_continuous(breaks = theta) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank()) +
  labs(title = "Smarter priors increase posterior probability at maximum.", 
       color = NULL, x = expression(theta), y = "P")
```

What if we now employ a larger data set? To see, generate a sample of 100 Bernoulli(.7) observations.

```{r}
D100 <- rbernoulli(100, p = 0.7) %>% as.numeric()
likelihood <- theta^sum(D100) * (1 - theta)^(length(D100)-sum(D100))
posterior <- likelihood / sum(likelihood * prior) * prior
```

```{r fig.height=3, echo=FALSE}
data.frame(theta, prior, posterior) %>% 
  pivot_longer(cols = c(prior, posterior), values_to = "pi") %>%
  ggplot() +
  geom_point(aes(x = theta, y = pi, color = name), size = 1) +
  geom_line(aes(x = theta, y = pi, color = name), size = 1) +
  scale_x_continuous(breaks = theta) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank()) +
  labs(title = "More observations tighten the posterior distribution.",
       color = NULL, x = expression(theta), y = "P")
```

What would it look like if it also had more competing hypotheses, $\theta \in (0, .01, .02, \ldots, 1)$.

```{r}
theta <- seq(0, 1, by = .01)
prior <- rep(1/100, 101)
likelihood <- theta^sum(D100) * (1 - theta)^(length(D100)-sum(D100))
posterior <- likelihood / sum(likelihood * prior) * prior
```

```{r fig.height=3, echo=FALSE}
data.frame(theta, prior, posterior) %>% 
  pivot_longer(cols = c(prior, posterior), values_to = "pi") %>%
  ggplot() +
  geom_point(aes(x = theta, y = pi, color = name), size = 1) +
  geom_line(aes(x = theta, y = pi, color = name), size = 1) +
  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank()) +
  labs(title = "More priors smooths the distribution.", 
       color = NULL, x = expression(theta), y = "P")
```

## Continuous Cases

Continuing the example of inferring the parameter $p$ used in the Bernoulli process, what if we considered *all* values between 0 and 1?^[[Chris's Sandbox again](http://chrisstrelioff.ws/sandbox/2014/12/11/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations.html).]

When prior beliefs are best described in continuous distributions, express them using the beta, gamma, or normal distribution so that the posterior distributions are conjugates of the prior distributions with new parameter values. Otherwise, the marginal distribution is difficult to calculate. In this case, use the beta distribution, described by shape parameters, $\alpha$ and $\beta$.

$$P(\theta|D,\alpha,\beta) = \frac{f_\theta(D) P(\theta|\alpha,\beta)}{\int_0^1 f_\theta(D)P(\theta|\alpha, \beta)d\theta}$$

As with the discrete case, the numerator is the likelihood of observing $D$ if $\theta$ is true multiplied by the prior probability, but now the prior is a Beta($\alpha$, $\beta$) distribution. The denominator, sometimes called the *evidence*, is the marginal probability of $D$.

The likelihood of observing $D$ = $a$ successes and $b$ non-successes given a success probability of $p$ = $\theta$ is

$$f_\theta(D) = \theta^a(1-\theta)^b$$

The prior distribution is the probability density function of the beta distribution

$$P(\theta|\alpha,\beta) = \frac{1}{\mathrm{B}(\alpha, \beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}$$

where $\mathrm{B}(\alpha, \beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)}$ is the beta function.

The marginal distribution is

$$\int_0^1 f_\theta(D)P(\theta|\alpha, \beta)d\theta = \frac{\mathrm{B}(\alpha + a, \beta + b)}{\mathrm{B}(\alpha, \beta)}$$

Putting this all together, the posterior distribution is

$$P(\theta|D, \alpha, \beta) = \frac{1}{\mathrm{B}(\alpha + a, \beta + b)} \theta^{\alpha-1+a}(1-\theta)^{\beta-1+b}$$

The posterior equals the prior with shape parameters incremented by the observed counts, $a$ and $b.$

```{r}
plot_bayes <- function(alpha, beta, a, b) {

  prior_ev <- (alpha / (alpha + beta)) %>% round(2)
  posterior_ev <- ((alpha + a) / (alpha + beta + a + b)) %>% round(2)
  dat <- data.frame(theta = seq(0, 1, by = .01)) %>%
    mutate(prior = (1 / beta(alpha, beta)) * theta^(alpha-1) * (1-theta)^(beta-1),
           prior_ci = theta > qbeta(.025, alpha, beta) & 
                      theta < qbeta(.975, alpha, beta),
           likelihood = theta^a * (1-theta)^b,
           posterior = (1 / beta(alpha + a, beta + b)) * theta^(alpha-1+a) * (1-theta)^(beta-1+b),
           posterior_ci = theta > qbeta(.025, alpha + a, beta + b) & 
                      theta < qbeta(.975, alpha + a, beta + b))
  
  p_prior <- dat %>%
    ggplot(aes(x = theta, y = prior)) + 
    geom_line(color = "steelblue") +
    geom_area(aes(alpha = prior_ci), fill = "steelblue") +
    geom_vline(xintercept = prior_ev, color = "steelblue") +
    scale_alpha_manual(values = c(.1, .5)) +
    theme_minimal() +
    theme(legend.position = "none") +
    labs(x = NULL)
  
  p_likelihood <- dat %>%
    ggplot(aes(x = theta, y = likelihood)) + 
    geom_line(color = "steelblue") +
    theme_minimal() +
    theme(legend.position = "none") +
    labs(x = NULL)
  
  p_posterior <- dat %>%
    ggplot(aes(x = theta, y = posterior)) + 
    geom_line(color = "steelblue") +
    geom_area(aes(alpha = posterior_ci), fill = "steelblue") +
    geom_vline(xintercept = posterior_ev, color = "steelblue") +
    scale_alpha_manual(values = c(.1, .5))  +
    theme_minimal() +
    theme(legend.position = "none") +
    labs(x = expression(theta))
  
  out <- p_prior / p_likelihood / p_posterior +
    plot_annotation(
      title = glue("Beta({alpha}, {beta}) prior with observed evidence a = {a} ",
                   "and b = {b}"),
      subtitle = "with shaded 95% credible interval.",
      caption = glue("Prior expected value = {prior_ev}; Posterior expected ",
                     "value = {posterior_ev}"))
  out
}
```

Suppose you claim complete ignorance and take a uniform Beta(1, 1) prior. Recall that you observed a = 7 ones and b = 3 zeros. The posterior expected value is still pretty close!

```{r fig.height=7}
plot_bayes(alpha = 10, beta = 10, a = 7, b = 3)
```

Suppose you had prior reason to believe *p* = 0.7. You would model that as $\alpha$ = 7, $\beta$ = 3. The prior probability distribution would be $P(\theta|\alpha = 7,\beta = 3) = \frac{1}{\mathrm{B}(7, 3)}\theta^{7-1}(1-\theta)^{3-1}$. Then after observing a = 7 ones and b = 3 zeros, the posterior probability distribution would be $P(\theta|\alpha = 7+7,\beta = 3+3) = \frac{1}{\mathrm{B}(7+7, 3+3)}\theta^{7+7-1}(1-\theta)^{3+3-1}$.

```{r fig.height=7}
plot_bayes(alpha = 7, beta = 3, a = 7, b = 3)
```

## Bayes Factors

The Bayes Factor (BF) is a measure of the relative evidence of one model over another. Take another look at Bayes' formula:

$$P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}.$$

Suppose you want to compare how two models explain and observed data outcome, $D$. Model $M_1:f_1(D|\theta_1)$ says the observed data $D$ was produced by a generative model with pdf $f_1$ parameterized by $\theta_2$. Model $M_2:f_2(D|\theta_2)$ says it was produced by a generative model with pdf $f_2$ parameterized by $\theta_2$. In each model you specify a prior probability distribution for the parameter

If you take the ratio of the posterior probabilities, the posterior odds, the $P(D)$ terms cancel and you have

$$\frac{P(\theta_1|D)}{P(\theta_2|D)} = \frac{P(D|\theta_1)}{P(D|\theta_2)} \cdot \frac{P(\theta_1)}{P(\theta_2)}$$

The posterior odds equals the ratio of the likelihoods multiplied by the prior odds. That likelihood ratio is the Bayes Factor (BF). Rearranging, BF is the odds ratio of the posterior and prior odds.

$$BF = \frac{P(D|\theta_1)}{P(D|\theta_2)} = \mathrm{\frac{Posterior Odds}{Prior Odds}}$$

Return to the example of observing $D$ = 7 ones and 3 zeros. You can compare an hypothesized $\theta$ of .5 to a completely agnostic model where $\theta$ is uniform over [0, 1]. The likelihood of observing $D$ when $\theta$ = .5 is $P(D|\theta_1) = 5^7(1-.5)^3$ = `r scales::number(dbinom(7, 10, .5), accuracy = .001)`. The likelihood of observing $D$ where $\theta$ is uniform on [0, 1] is $P(D|\theta_2) = \int_0^1 \binom{10}{3}q^7(1-q)^3dq$

```{r}
.5^1 * .5^1
dbinom(1, 1, .5)
dbinom(11, 11, .5)
beta(11, 11)
```



with a uniform Beta(1, 1) prior (i.e., complete agnosticism). 


The Bayes factor at $\theta$ = .7 quantifies how much the odds of H0: $\theta$ = .7 over H1: $\hat{\theta}$ = .7.

```{r}
prior <- function(theta, alpha, beta) {
  (1 / beta(alpha, beta)) * theta^(alpha-1) * (1-theta)^(beta-1)
}
posterior <- function(theta, alpha, beta, a, b) {
  (1 / beta(alpha + a, beta + b)) * theta^(alpha-1+a) * (1-theta)^(beta-1+b)
}

prior(.5, 115, 85) 
posterior(.5, 1, 1, 10, 10)

posterior(.5, 1, 1, 10, 10) / prior(.5, 1, 1) 

1 / beta(115, 85)

# Posterior Distribution 
1/beta(1+10, 1+10) * .5^(1-1+10) * (1-.5)^(1-1+10)
dbeta(.5, 11, 11)

# Prior Beta Distributions
1/beta(1, 1) * .5^(1-1) * (1-.5)^(1-1)
dbeta(.5, 1, 1)


dbeta(.5, 115, 85)
```


The Bayes factor measures how much your prior belief is altered by the evidence. It is the ratio of the likelihoods at some hypothesized value before and after observing the data. In this case, our confidence increased by a factor of...

```{r collapse=TRUE}
theta <- 0.5

alpha <- 1
beta <- 1
a <- 10
b <- 10

(prior_likelihood <- (1 / beta(alpha, beta)) * theta^(alpha-1) * (1-theta)^(beta-1))
(posterior_likelihood <- (1 / beta(alpha + a, beta + b)) * theta^(alpha-1+a) * (1-theta)^(beta-1+b))
(bayes_factor <- posterior_likelihood / prior_likelihood)

# 3.7 on alpha = beta = 1
# 1.91 on alpha = beta = 4
```

## A Gentler Introduction

This section is my notes from DataCamp course [Fundamentals of Bayesian Data Analysis in R](https://learn.datacamp.com/courses/fundamentals-of-bayesian-data-analysis-in-r). It is an intuitive approach to Bayesian inference.

Suppose you purchase 100 ad impressions on a web site and receive 13 clicks. How would you describe the click rate? The Frequentist approach would be to construct a 95% CI around the click proportion.

```{r}
(ad_prop_test <- prop.test(13, 100))
```

```{r fig.height=3, warning=FALSE, echo=FALSE}
ad_prop_test <- prop.test(13, 100)
click_rate_mu <- ad_prop_test$estimate
click_rate_se <- sqrt(click_rate_mu*(1-click_rate_mu)) / sqrt(100)
data.frame(pi = seq(0, .30, by = .01)) %>%
  mutate(
    likelihood = dnorm(pi, click_rate_mu, click_rate_se),
    # observed = map(pi, ~ c(., 1 - .) * 100),
    # expected = map(13/100, ~c(., 1 - .) * 100),
    # x2 = map2_dbl(observed, expected, ~sum((.x - .y)^2 / .y)),
    # likelihood = dchisq(x2, 1),
    # pcs = pchisq(x2, 1),
    ci_low = if_else(pi <= ad_prop_test$conf.int[1], likelihood, NA_real_),
    ci_high = if_else(pi >= ad_prop_test$conf.int[2], likelihood, NA_real_)
  ) %>%
  ggplot(aes(x = pi)) +
  geom_line(aes(y = likelihood)) +
  geom_area(aes(y = ci_low), fill = "firebrick", alpha = .4) +
  geom_area(aes(y = ci_high), fill = "firebrick", alpha = .4) +
  geom_vline(xintercept = click_rate_mu, linetype = 2) +
  theme_minimal() +
  scale_x_continuous(breaks = seq(0, .3, .05)) +
  theme(panel.grid.minor = element_blank()) +
  labs(title = "Frequentist proportion test for 13 clicks in 100 impressions", 
       subtitle = glue("p = {click_rate_mu}, 95%-CI (",
       "{ad_prop_test$conf.int[1] %>% scales::number(accuracy = .01)}, ",
       "{ad_prop_test$conf.int[2] %>% scales::number(accuracy = .01)})"),
       x = expression(theta))
```

How might you model this using Bayesian reasoning? One way is to run 1,000 experiments that sample 100 ad impression events from an `rbinom()` generative model using a uniform prior distribution of 0-30% click probability. The resulting 1,000 row data set of click probabilities and sampled click counts forms a joint probability distribution. This method of Bayesian analysis is called *rejection sampling* because you sample across the whole parameter space, then condition on the observed evidence.

```{r}
df_sim <- data.frame(click_prob = runif(1000, 0.0, 0.3))
df_sim$click_n <- rbinom(1000, 100, df_sim$click_prob)
```

```{r echo=FALSE}
df_sim %>% 
  mutate(is_13 = factor(click_n == 13, levels = c(TRUE, FALSE))) %>%
  ggplot(aes(x = click_prob, y = click_n, color = is_13)) +
  geom_point(alpha = 0.6, show.legend = FALSE) +
  geom_hline(yintercept = 13, color = "steelblue", linetype = 1, size = .5) +
  scale_color_manual(values = c("TRUE" = "steelblue", "FALSE" = "gray80")) +
  theme_minimal() +
  labs(title = "Joint probability of observed clicks and click probability",
       subtitle = "with conditioning on 13 observed clicks.",
       y = "clicks per 100 ads",
       x = expression(theta))
```

Condition the joint probability distribution on the 13 observed clicks to update your prior. The `quantile()` function returns the median and the .025 and .975 percentile values - the *credible interval*.

```{r}
# median and credible interval
(sim_ci <- df_sim %>% filter(click_n == 13) %>% pull(click_prob) %>% 
  quantile(c(.025, .5, .975)))
```

Your posterior click rate likelihood is `r scales::percent(sim_ci[2], accuracy = .1)` with 95 credible interval [`r scales::percent(sim_ci[1], accuracy = .1)`, `r scales::percent(sim_ci[3], accuracy = .1)`]. Here is the density plot of the `r df_sim %>% filter(click_n == 13) %>% nrow()` simulations that produced the 13 clicks. The median and 95% credible interval are marked.

```{r echo=FALSE}
df_sim %>% 
  filter(click_n == 13) %>% 
  ggplot(aes(x = click_prob)) +
  geom_density() +
  geom_vline(xintercept = sim_ci[2]) +
  geom_vline(xintercept = sim_ci[1], linetype = 2) +
  geom_vline(xintercept = sim_ci[3], linetype = 2) +
  scale_x_continuous(breaks = seq(0, .3, .05)) +
  theme_minimal() +
  labs(title = "Posterior click likelihood distribution", 
       subtitle = glue("p = {sim_ci[2] %>% scales::number(accuracy = .01)}, 95%-CI (",
       "{sim_ci[1] %>% scales::number(accuracy = .01)}, ",
       "{sim_ci[3] %>% scales::number(accuracy = .01)})"),
       x = expression(theta), y = "density (likelihood)")
```

That's pretty close to the frequentist result! Instead of running 1,000 experiments with randomly selected click probabilities and randomly selected click counts based on those probabilities, you could define a discrete set of candidate click probabilities, e.g. values between 0 and 0.3 incremented by .01, and calculate the click probability density for the 100 ad impressions. This method of Bayesian analysis is called *grid approximation*.

```{r}
df_bayes <- expand.grid(
  click_prob = seq(0, .3, by = .001), 
  click_n = 0:100
) %>%
  mutate(
    prior = dunif(click_prob, min = 0, max = 0.3),
    likelihood = dbinom(click_n, 100, click_prob),
    probability = likelihood * prior / sum(likelihood * prior)
  )
```

```{r echo=FALSE}
df_bayes %>% 
  mutate(is_13 = factor(click_n == 13, levels = c(TRUE, FALSE))) %>%
  filter(probability > .0001) %>%
  ggplot(aes(x = click_prob, y = click_n, color = is_13)) +
  geom_point(alpha = 0.6, show.legend = FALSE) +
  geom_hline(yintercept = 13, color = "steelblue", linetype = 1, size = .5) +
  scale_color_manual(values = c("TRUE" = "steelblue", "FALSE" = "gray80")) +
  # scale_color_gradient(low = "white", high = "steelblue", guide = "colorbar") +
  theme_minimal() +
  labs(title = "Joint probability of clicks and click probability.",
       subtitle = "with conditioning on 13 observed clicks.",
       y = "clicks per 100 ads",
       x = expression(theta))
```

Condition the joint probability distribution on the 13 observed clicks to update your prior. 

```{r}
df_bayes_13 <- df_bayes %>% filter(click_n == 13) %>%
  mutate(posterior = probability / sum(probability))
```

Instead of using the `quantile()` function on these values to measure the median and *credible interval*, resample the posterior probability to create a distribution.

```{r}
sampling_idx <- sample(
  1:nrow(df_bayes_13), 
  size = 10000, 
  replace = TRUE, 
  prob = df_bayes_13$posterior
)

sampling_vals <- df_bayes_13[sampling_idx, ]

(df_bayes_ci <- quantile(sampling_vals$click_prob, c(.025, .5, .975)))
```

```{r echo=FALSE, warning=FALSE}
df_bayes %>%
  filter(click_n == 13) %>%
  mutate(likelihood = probability / sum(probability),
         ci_low = if_else(click_prob < df_bayes_ci[1], likelihood, NA_real_),
         ci_high = if_else(click_prob > df_bayes_ci[3], likelihood, NA_real_), 
         ci = if_else(click_prob >= df_bayes_ci[1] & 
                        click_prob <= df_bayes_ci[3], "Y", "N")) %>%
  ggplot(aes(x = click_prob, y = likelihood)) +
  geom_line() +
  geom_area(aes(y = ci_low), fill = "firebrick", alpha = .4) +
  geom_area(aes(y = ci_high), fill = "firebrick", alpha = .4) +
  geom_vline(xintercept = df_bayes_ci[2], linetype = 2) +
  theme_minimal() +
  scale_x_continuous(breaks = seq(0, .3, .05)) +
  theme(panel.grid.minor = element_blank()) +
  labs(title = "Posterior click probability", 
       subtitle = glue("p = {df_bayes_ci[2] %>% scales::number(accuracy = .01)}, 95%-CI (",
       "{df_bayes_ci[1] %>% scales::number(accuracy = .01)}, ",
       "{df_bayes_ci[3] %>% scales::number(accuracy = .01)})"),
       x = expression(theta))
```

You can use a Bayesian model to estimate multiple parameters. Suppose you want to predict the water temperature in a lake on Jun 1 based on 5 years of prior water temperatures. 

```{r}
temp <- c(19, 23, 20, 17, 23)
```

You model the water temperature as a normal distribution, $\mathrm{N}(\mu, \sigma^2)$ with a prior distribution $\mu = \mathrm{N}(18, 5^2)$ and $\sigma = \mathrm{unif}(0, 10)$ based on past experience.

Using the grid approximation approach, construct a grid of candidate $\mu$ values from 8 to 30 degrees incremented by .5 degrees, and candidate $\sigma$ values from .1 to 10 incremented by .1 - a 4,500 row data frame. 

```{r}
mdl_grid <- expand_grid(mu = seq(8, 30, by = 0.5),
                        sigma = seq(.1, 10, by = 0.1))
```

For each combination of $\mu$ and $\sigma$, the *prior* probabilities are the densities from $\mu = \mathrm{N}(18, 5^2)$ and $\sigma = \mathrm{unif}(0, 10)$. The combined prior is their product. The *likelihoods* are the products of the probabilities of observing each `temp` given the candidate $\mu$ and $\sigma$ values.

```{r}
mdl_grid_2 <- mdl_grid %>%
  mutate(
    mu_prior = map_dbl(mu, ~dnorm(., mean = 18, sd = 5)),
    sigma_prior = map_dbl(sigma, ~dunif(., 0, 10)),
    prior = mu_prior * sigma_prior, # combined prior,
    likelihood = map2_dbl(mu, sigma, ~dnorm(temp, .x, .y) %>% prod()),
    posterior = likelihood * prior / sum(likelihood * prior)
  )
```

```{r echo=FALSE}
mdl_grid_2 %>%
  ggplot(aes(x = mu, y = sigma, fill = posterior)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "steelblue", guide = "colorbar") +
  theme_minimal() +
  labs(title = "Joint probability of mu and sigma.",
       x = expression(mu), y = expression(sigma))
```

Calculate a credible interval by drawing 10,000 samples from the grid with sampling probability equal to the calculated posterior probabilities. Use the `quantile()` function to estimate the median and .025 and .975 quantile values.

```{r fig.height=3}
sampling_idx <- sample(1:nrow(mdl_grid), size = 10000, replace = TRUE, prob = mdl_grid$posterior)
sampling_vals <- mdl_grid[sampling_idx, c("mu", "sigma")]
mu_ci <- quantile(sampling_vals$mu, c(.025, .5, .975))
sigma_ci <- quantile(sampling_vals$sigma, c(.025, .5, .975))
ci <- qnorm(c(.025, .5, .975), mean = mu_ci[2], sd = sigma_ci[2])

data.frame(temp = seq(0, 30, by = .1)) %>%
  mutate(prob = map_dbl(temp, ~dnorm(., mean = ci[2], sd = sigma_ci[2])),
         ci = if_else(temp >= ci[1] & temp <= ci[3], "Y", "N")) %>%
  ggplot(aes(x = temp, y = prob)) +
  geom_area(aes(y = if_else(ci == "N", prob, 0)), 
            fill = "firebrick", show.legend = FALSE) +
  geom_line() +
  geom_vline(xintercept = ci[2], linetype = 2) +
  theme_minimal() +
  scale_x_continuous(breaks = seq(0, 30, 5)) +
  theme(panel.grid.minor = element_blank()) +
  labs(title = "Posterior temperature probability", 
       subtitle = glue("mu = {ci[2] %>% scales::number(accuracy = .1)}, 95%-CI (",
       "{ci[1] %>% scales::number(accuracy = .1)}, ",
       "{ci[3] %>% scales::number(accuracy = .1)})"))
```

What is the probability the temperature is at least 18?

```{r}
pred_temp <- rnorm(1000, mean = sampling_vals$mu, sampling_vals$sigma)
scales::percent(sum(pred_temp >= 18) / length(pred_temp))
```

## Gamma Poisson

Given counts of sandwich sales by weekday, what are the average daily sales? 

```{r}
D <- c(50, 65, 72, 63, 70)
```

Count data have a Poisson distribution, $D_i|\lambda \sim Pois(\lambda)$, with PMF $f(D_i | \lambda) = e^{-\lambda}\frac{\lambda^{D_i}}{D_i!}$. Using Bayes' Theorem, we answer this question by estimating the posterior distribution of $\lambda$ given evidence $\textbf{D}$, $f(\lambda |\textbf{D})$ is equal to the joint likelihood of $\lambda$ and $\textbf{D}$ divided by the likelihood of $\textbf{D}$.

$$
f(\lambda |\textbf{D}) = \frac{f(\mathbf{D}|\lambda) f(\lambda)}{\int_\Lambda f(\mathbf{D}|\lambda) f(\lambda) d\lambda}
$$

$f(\textbf{D}|\lambda)$ is the sum-product of the Poisson distribution PMF.

$$
\begin{align}
f(\textbf{D}|\lambda) = f(D_i,\ldots, D_n | \lambda) &= \prod_i f(D_i | \lambda) \\
&= \prod_i e^{-\lambda}\frac{\lambda^{D_i}}{D_i!}
\end{align}
$$

What about the prior, $f(\lambda)$? We use the [gamma distribution](https://mpfoley73.github.io/probability/random-variables-and-distributions.html#gamma), $\lambda|a,b = \mathrm{Gamma}(a,b)$ to model positive values.

$$
f(\lambda) = f(\lambda | a,b) = \frac{b^a \lambda^{a-1} e^{-b\lambda}}{\Gamma(a)}
$$

where $\Gamma$ is the gamma function^[The gamma function is a generic function, just like sin, cos, etc., and is a kind of generalized factorial.]. Substituting into Bayes' Theorem and simplifying, you have this nightmare:

$$
f(\lambda |\textbf{D}) = \frac{\lambda^{a + \sum_i D_i-1}e^{-(b+n)\lambda}}{\int_0^\infty \lambda^{a + \sum_i D_i-1}e^{-(b+n)\lambda} d\lambda}
$$

However, there is good news. The integration in the denominator removes the dependence on $\lambda$, so $f(\lambda |\textbf{D}, a, b)$ is proportional to the numerator up to a constant.

$$
f(\lambda |\textbf{D}) \propto f(\textbf{D} | \lambda) f(\lambda)
$$

Since $f(\lambda |\textbf{D})$ is a PMF, you integrates (sums) to 1 and you can always figure out the constant later. What makes this good news is that this has the form of the PDF of the gamma distribution. 

$$
\begin{equation} 
\lambda | \textbf{D}, a, b \sim \mathrm{Gamma}(a + \sum_i D_i, b + n)
(\#eq:gamma-posterior)
\end{equation} 
$$

Equation \@ref(eq:gamma-posterior) is the posterior distribution of $\lambda$. We combined a gamma prior with the Poisson likelihood of evidence, $D$, to produce a gamma posterior. We call priors that produce posteriors of the same form, *conjugate priors* for the Poisson likelihood. Conjugate priors are popular because of their computational convenience.

Let's put this into action with the sandwich sales data. We need values to plug into Equation \@ref(eq:gamma-posterior). For the gamma distribution, $E(X) = a / b$ and $\mathrm{Var}(X) = a / b^2$. You might guess from intuition that mean daily sandwich sales are 70 +/- 5. Interpreting +/- 5 as a 95% CI and using the rule of thumb that a 95% CI is 2 SD, $\mathrm{Var} = (2.5)^2 = 6.25$. Solve for $a = 784$ and $b = 11.2$. We also have $\sum D = 320$ and $n = 5$.

$$
\lambda | \textbf{D}, a, b \sim \mathrm{Gamma}(784 + 320, 11.2 + 5) \sim \mathrm{Gamma}(1104, 16.2)
$$

The posterior $E(D) = a / b = 1104 / 16.2 = 68.1$ and $\mathrm{Var}(D) = a / b^2 = 1104 / 16.2^2 = 4.2$. Use the gamma distribution function to get the posterior 95% CI (_credible_ interval).

```{r collapse=TRUE}
# The prior distribution
qgamma(p = c(.025, .5, .975), 784, 11.2)

# The posterior distribution
qgamma(p = c(.025, .5, .975), 784 + 320, 11.2 + 5)
```

Whereas the prior expected mean daily sandwich sales was 70 (95% CI: 65, 75), the posterior is 68 (95% CI: 64, 72). Compare this to classical statistics: $E(D) = \bar{d} = 64$, $SE = \sqrt{\bar{d} / n} = 3.6$:

```{r collapse=TRUE}
qnorm(p = c(.025, .5, .975), 64, 3.6)
```

You might think that the reasonable Bayesian outcome was predicated on good $a$ and $b$ priors, but no. Suppose $a = .01$ and $b = .01$. The posterior is still reasonable.

```{r collapse=TRUE}
qgamma(p = c(.025, .5, .975), .01 + 320, .01 + 5)
```

```{r}
tibble(
  lambda = seq(0, 80, .1),
  `Gamma(.01, .01)` = dgamma(lambda, .01, .01),
  `Gamma(.01 + 320, .01 + 5)` = dgamma(lambda, .01 + 320, .01 + 5),
  `Gamma(784, 11.2)` = dgamma(lambda, 784, 11.2),
  `Gamma(784 + 320, 11.2 + 5)` = dgamma(lambda, 784 + 320, 11.2 + 5)
) %>%
  pivot_longer(-lambda) %>%
  mutate(name = fct_inorder(name)) %>%
  mutate(prior = if_else(str_detect(name, "\\+"), "posterior", "prior")) %>%
  ggplot(aes(x = lambda, y = value, color = name, linetype = prior)) +
  geom_line(linewidth = 1) +
  scale_color_manual(values = c(rep("seagreen", 2), rep("firebrick", 2))) +
  labs(color = NULL, linetype = NULL, y = "density")
```

You can see how the Bayesian estimate converges on the classical $\bar{d}$ with increasing sample size.

$$
E(\lambda|\textbf{D}, a, b) = \frac{a + \sum_i D_i}{b + n} = \frac{a + n \bar{d}}{b + n}
$$

Taking the limit, $\lim_{n \rightarrow \infty} E(\lambda|\textbf{D}, a, b) = \bar{d}$.

The *central* credible interval is the standard Bayesian credible interval. But when the posterior distribution is not perfectly symmetric, the shortest credible interval capturing x% of the distribution might different endpoints. Our example has a pretty symmetric distribution, but lets calculate the *highest density region (HDR)*.

```{r}
pp <- seq(0.01, .99, by = .0001)
x <- map_dbl(pp, ~qgamma(., 784 + 320, 11.2 + 5))
hdrcde::hdr(x, prob = 95)$hdr
```


<!--chapter:end:03-bayesian-statistics.Rmd-->

# One-Sample

```{r include=FALSE}
library(tidyverse)
library(broom)
library(gtsummary)
library(flextable)
```

Use *one-sample* tests to either *describe* a single variable's frequency or central tendency, or to *compare* the frequency or central tendency to a hypothesized distribution or value.

If the data generating process produces continuous outcomes (interval or ratio), and the outcomes are symmetrically distributed, the sample mean, $\bar{x}$, is a random variable centered at the population mean, $\mu$. You can then use a theoretical distribution (normal or student t) to estimate a 95% confidence interval (CI) around $\mu$, or compare $\bar{x}$ to an hypothesized population mean, $\mu_0$. If you (somehow) know the population variance, or the Central Limit Theorem (CLT) conditions hold, you can assume the random variable is normally distributed and use the *z*-test, otherwise assume the random variable has student *t* distribution and use the *t*-test.^[The *t*-test returns nearly the same result as the *z*-test when the CLT holds, so in practice no one bothers with the *z*-test except as an aid to teach the *t*-test.] If the data generating process produces continuous outcomes that are *not* symmetrically distributed, use a non-parametric test like the Wilcoxon median test.

If the data generating process produces discrete outcomes (counts), the sample count, $x$, is a random variable from a Poisson, binomial, normal, or multinomial distribution, or a random variable from a theoretical outcome. Whatever the source of the expected values, you use either the *chi-squared goodness-of-fit* test or *G* test to test whether the observed values fit the expected values from the distribution. In the special case of binary outcomes with small (*n* < 1,000), you can use Fisher's exact test instead. The discrete variable tests are discussed in [PSU STATS 504](https://online.stat.psu.edu/stat504).

* For counts over a fixed time or space, treat the count as a random variable from a Poisson distribution with expected value $\lambda$ and variance $\lambda$.

* For counts within a fixed total that are then classified into two levels (usually yes/no), then treat the count as a random variable from a binomial distribution with expected value $n\pi$ and variance $n\pi(1-\pi)$.

* For binomial distributions where $n\ge30$ and the frequency counts of both levels is $\ge$ 5, treat the *proportion* as a random variable from the normal distribution with expected valued $\pi$ and variance $\frac{\pi(1-\pi)}{n}$.

* For counts within a fixed total that are then classified into three or more levels, treat the count as a random variable from the multinomial distribution with expected value $n\pi_j$ and variance $n\pi_j(1-\pi_j)$.

## One-Sample Mean z Test

The *z* test is also called the normal approximation *z* test.  It only applies when the sampling distribution of the population mean is normally distributed with known variance, and there are no significant outliers. The sampling distribution is normally distributed when the underlying population is normally distributed, or when the sample size is large $(n >= 30)$, as follows from the central limit theorem. The *t* test returns similar results, plus it is valid when the variance is unknown, and that is pretty much always. For that reason, you probably will never use this test.

Under the normal approximation method, the measured mean $\bar{x}$ approximates the population mean $\mu$, and the sampling distribution has a normal distribution centered at $\mu$ with standard error $se_\mu = \frac{\sigma}{\sqrt{n}}$ where $\sigma$ is the standard deviation of the underlying population. Define a $(1 - \alpha)\%$ confidence interval as $\bar{x} \pm z_{(1 - \alpha) {/} 2} se_\mu$, or test $H_0: \mu = \mu_0$ with test statistic $Z = \frac{\bar{x} - \mu_0}{se_\mu}$.  

#### Example {-}

```{r include=FALSE}
n <- nrow(mtcars)
x_bar <- mean(mtcars$mpg)
s <- sd(mtcars$mpg)
mu_0 <- 18.0
sigma <- 6
```

The `mtcars` data set is a sample of *n* = `r n` cars. The mean fuel economy is $\bar{x} \pm s$ = `r x_bar %>% format(digits = 1, nsmall=1)` $\pm$ `r s %>% format(digits = 1, nsmall=1)` mpg. The prior measured overall fuel economy for vehicles was $\mu_0 \pm \sigma$ = `r mu_0 %>% format(digits = 1, nsmall=1)` $\pm$ `r sigma %>% format(digits = 1, nsmall=1)` mpg. Has fuel economy improved?

The sample size is $\ge$ 30, so the sampling distribution of the population mean is normally distributed. The population variance is known, so use the *z* test.  

```{r include=FALSE}
se <- sigma / sqrt(n)
Z <- (x_bar - mu_0) / se
p <- pnorm(q = Z, lower.tail = FALSE)
```

$H_0: \mu = 16.0$, and $H_a: \mu > 16.0$ - a right-tail test. The test statistic is $Z = \frac{\bar{x} - \mu_0}{se_\mu}=$ `r Z %>% format(digits = 1, nsmall = 2)` where $se_{\mu_0} = \frac{\mu_0}{\sqrt{n}} =$ `r se %>% format(digits = 1, nsmall = 2)`. $P(z > Z) =$ `r p %>% format(digits = 1, nsmall = 4)`, so reject $H_0$ at the $\alpha =$ 0.05 level of significance.

```{r echo=FALSE, warning=FALSE, fig.height=3.5, fig.width=6.5}
lrr = -Inf  # right-tailed test
urr = mu_0 + qnorm(p = .05, lower.tail = FALSE) * se
data.frame(mu = seq(10, 25, by = .1)) %>%
  mutate(Z = (mu - mu_0) / se,
         prob = dnorm(Z),
         rr = ifelse(mu < lrr | mu > urr, prob, NA_real_)) %>%
  ggplot() +
  geom_line(aes(x = mu, y = prob)) +
  geom_area(aes(x = mu, y = rr), fill = "firebrick", alpha = 0.6) +
  geom_vline(aes(xintercept = mu_0), size = 1) +
  geom_vline(aes(xintercept = x_bar), color = "goldenrod", size = 1, linetype = 2) +
  theme_minimal() +
  theme(legend.position="none",
        axis.text.y = element_blank()) +
  labs(title = bquote("Right-Tail z Test"),
       x = "mu",
       y = "Probability")
```

```{r include=FALSE}
z_crit <- qnorm(p = 1-.05/2, lower.tail = TRUE)
epsilon <- z_crit * se
ci_low <- x_bar - epsilon
ci_high <- x_bar + epsilon
```

The 95% confidence interval for $\mu$ is $\bar{x} \pm z_{(1 - \alpha){/}2} se_\mu$ where $z_{(1 - \alpha){/}2} =$ `r z_crit %>% round(2)`. $\mu =$ `r x_bar %>% format(digits = 1, nsmall = 2)` $\pm$ `r epsilon %>% format(digits = 1, nsmall = 2)` (95% CI `r ci_low %>% format(digits = 1, nsmall = 2)` to `r ci_high %>% format(digits = 1, nsmall = 2)`).

```{r echo=FALSE, warning=FALSE, fig.height=3.5, fig.width=6.5}
data.frame(mu = seq(15, 25, by = .1)) %>%
  mutate(Z = (mu - x_bar) / se,
         prob = dnorm(x = Z), 
         ci = if_else(mu >= ci_low & mu <= ci_high, prob, NA_real_)) %>%
  ggplot() +
  geom_line(aes(x = mu, y = prob), color = "goldenrod") +
  geom_area(aes(x = mu, y = ci), fill = "goldenrod", alpha = 0.3) +
  geom_vline(aes(xintercept = x_bar), color = "goldenrod", size = 1, linetype = 2) +
  theme_minimal() +
  theme(legend.position="none",
        axis.text.y = element_blank()) +
  labs(title = "95% CI",x = "mu",y = "Probability") 
```

## One-Sample Mean t Test

The one-sample *t* test applies when the sampling distribution of the population mean is normally distributed and there are no significant outliers. Unlike the *z* test, the population variance can be unknown. The sampling distribution is normally distributed when the underlying population is normally distributed, or when the sample size is large $(n >= 30)$, as follows from the central limit theorem. 

Under the *t* test method, the measured mean, $\bar{x}$, approximates the population mean, $\mu$. The sample standard deviation, $s$, estimates the unknown population standard deviation, $\sigma$. The resulting sampling distribution has a *t* distribution centered at $\mu$ with standard error $se_\bar{x} = \frac{s}{\sqrt{n}}$. Define a $(1 - \alpha)\%$ confidence interval as $\bar{x} \pm t_{(1 - \alpha){/}2} se_\bar{x}$ and/or test $H_0: \mu = \mu_0$ with test statistic $T = \frac{\bar{x} - \mu_0}{se_\bar{x}}$.

#### Example{-}

```{r include=FALSE}
dep <- foreign::read.spss("./input/one-sample-t-test.sav", to.data.frame = TRUE)

n <- nrow(dep)
mu_0 <- 4.0
x_bar <- mean(dep$dep_score)
s <- sd(dep$dep_score)
```

A researcher recruits a random sample of *n* = `r n` people to participate in a study about depression intervention. The researcher measures the participants' depression level prior to the study. The mean depression score (`r format(x_bar, digits = 2, nsmall = 2)` $\pm$ `r format(s, digits = 2, nsmall = 2)`) was lower than the population 'normal' depression score of `r format(mu_0, digits = 2, nsmall = 1)`. The null hypothesis is that the sample is representative of the overall population. Should you reject $H_0$?

```{r}
dep %>% gtsummary::tbl_summary(statistic = list(all_continuous() ~ "{mean} ({sd})"))
```

#### Conditions {-}

The one-sample *t* test applies when the variable is continuous and the observations are independent. Additionally, there are two conditions related to the data distribution. If either condition fails, try the suggested work-arounds or use the non-parametric [Wilcoxon 1-Sample Median Test for Numeric Var] instead.

1. **Outliers**. There should be no significant outliers. Outliers exert a large influence on the mean and standard deviation. Test with a box plot. If there are outliers, you might be able to drop them or transform the data.
2. **Normality**.  Values should be *nearly* normally distributed ("nearly" because the *t*-test is robust to the normality assumption). This condition is especially important with small sample sizes. Test with Q-Q plots or the Shapiro-Wilk test for normality. If the data is very non-normal, you might be able to transform the data.

##### Outliers {-}

Assess outliers with a box plot. Box plot whiskers extend up to 1.5\*IQR from the upper and lower hinges and outliers (beyond the whiskers) are are plotted individually. The boxplot shows no outliers.

```{r echo=FALSE, fig.height=2.5}
dep %>%
  ggplot(aes(x = "dep_score", y = dep_score)) +
  geom_boxplot(fill = "snow3", color = "snow4", alpha = 0.6, width = 0.5, 
               outlier.color = "goldenrod", outlier.size = 2) +
  theme_minimal() +
  labs(title = "Boxplot of Depression Score",
       y = "Score", x = NULL)
```

If the outliers might are data entry errors or measurement errors, fix them or discard them. If the outliers are genuine, you have a couple options before reverting to Wilcoxon.

* Transform the variable. Don't do this unless the variable is also non-normal. Transformation also has the downside of making interpretation more difficult.
* Leave it in if it doesn't affect the conclusion (compared to taking it out).

##### Normality {-}

Assume the population is normally distributed if *n* $\ge$ 30. Otherwise, asses a Q-Q plot, skewness and kurtosis values, or a histogram. If you still don't feel confident about normality, run a [Shapiro-Wilk Test].

The data set has *n* = 40 observations, so you can assume normality. Here is a QQ plot anyway. The QQ plot indicates normality.

```{r fig.height=2.5}
dep %>%
  ggplot(aes(sample = dep_score)) +
  stat_qq() +
  stat_qq_line(col = "goldenrod") +
  theme_minimal() +
  labs(title = "Normal Q-Q Plot")
```

Here is the Shapiro-Wilk normality test. It fails to reject the null hypothesis of a normally distributed population.

```{r}
shapiro.test(dep$dep_score)
```

If the data is not normally distributed, you still have a couple options before reverting to Wilcoxon.

* Transform the dependent variable.
* Carry on regardless - the one-sample *t*-test is fairly robust to deviations from normality.

#### Results {-}

Conduct the *t*-test. To get a 95% CI around the *difference* (instead of around the estimate), run the test using the difference, $\mu_0 - \bar{x}$, and leave `mu` at its default of 0.

```{r}
(dep_95ci <- t.test(x = mu_0 - dep$dep_score, alternative = "two.sided", conf.level = .95))
```

The difference is statistically different from 0 at the *p* = .05 level. The effect size, called Cohen's *d*, is defined as $d = |M_D| / s$, where $|M_D| = \bar{x} - \mu_0$, and $s$ is the sample standard deviation. $d <.2$ is considered trivial, $.2 \le d < .5$ small, and $.5 \le d < .8$ large.

```{r}
(d <- rstatix::cohens_d(dep, dep_score ~ 1, mu = 4) %>% pull(effsize) %>% abs())
```

Cohen's *d* is `r d %>% format(digits = 2, nsmall = 2)`, a small effect. 

Make a habit of constructing a plot, just to make sure your head is on straight.

```{r echo=FALSE, warning=FALSE, fig.height=3.5, fig.width=6.5}
se <- s / sqrt(n)
lrr = mu_0 + qt(p = .05, df = n - 1, lower.tail = TRUE) * se
urr = mu_0 + qt(p = .05, df = n - 1, lower.tail = FALSE) * se
data.frame(mu = seq(3.5, 4.5, by = .01)) %>%
  mutate(t = (mu - mu_0) / se,
         prob = dt(x = t, df = n - 1),
         lrr = if_else(mu < lrr, prob, NA_real_),
         urr = if_else(mu > urr, prob, NA_real_)) %>%
  ggplot() +
  geom_line(aes(x = mu, y = prob)) +
  geom_area(aes(x = mu, y = lrr), fill = "firebrick", alpha = 0.6) +
  geom_area(aes(x = mu, y = urr), fill = "firebrick", alpha = 0.6) +
  geom_vline(aes(xintercept = mu_0), size = 1) +
  geom_vline(aes(xintercept = x_bar), color = "goldenrod", size = 1, linetype = 2) +
  theme_minimal() +
  theme(legend.position="none",
        axis.text.y = element_blank()) +
  labs(title = bquote("Two-Tailed t Test"),
       x = "mu",
       y = "Probability")
```

Now you are ready to report the results.

> A one-sample *t*-test was run to determine whether depression score in recruited subjects was different from normal, as defined as a depression score of 4.0. Depression scores were normally distributed, as assessed by Shapiro-Wilk's test (*p* > .05) and there were no outliers in the data, as assessed by inspection of a boxplot. Data are mean $\pm$ standard deviation, unless otherwise stated. Mean depression score (`r format(x_bar, digits = 2, nsmall = 2)` $\pm$ `r format(s, digits = 2, nsmall = 2)`) was lower than the population "normal" depression score of `r format(mu_0, digits = 2, nsmall = 2)`, a statistically significant difference of `r format(dep_95ci$estimate, digits = 2, nsmall = 2)` (95% CI, `r format(dep_95ci$conf.int[1], digits = 1, nsmall = 2)` to `r format(dep_95ci$conf.int[2], digits = 1, nsmall = 2)`), t(`r dep_95ci$parameter`) = `r format(dep_95ci$statistic, digits = 1, nsmall = 2)`, *p* = `r format(dep_95ci$p.value, digits = 1, nsmall = 3)`, *d* = `r d %>% format(digits = 2, nsmall = 2)`.

#### Appendix: Deciding Sample Size {-}

Determine the sample size required for a maximum error $\epsilon$ in the estimate by solving the confidence interval equation, $\bar{x} \pm t_{(1 - \alpha){/}2} \frac{s}{\sqrt{n}}$ for $n=\frac{{t_{\alpha/2,n-1}^2se^2}}{{\epsilon^2}}$ . Unfortunately, $t_{\alpha/2,n-1}^2$ is dependent on $n$, so replace it with $z_{\alpha/2}^2$. What about $s^2$?  Estimate it from the literature, a pilot study, or using the empirical rule that 95% of the range falls within two standard deviations, $s=range / 4$.

For example, if the maximum tolerable error is* $\epsilon$ = 3, and $s$ is approximately 10, what sample size produces an $\alpha$ =0.05 confidence level?

```{r}
ceiling(qnorm(.975)^2 * 10^2 / 3^2)
```

## One-Sample Median Wilcoxon Test

The Wilcoxon one-sample median test (aka Wilcoxon signed rank test) is a non-parametric alternative to the *t*-test for cases when the the sampling distribution of the population mean is *not* normally distributed, but is at least symmetric.

Under the Wilcoxon test, the measured median, $\eta_x$, approximates the population median, $\eta$. The method calculates the difference between each value and the hypothesized median, $\eta_0$, ranks the difference magnitudes, then sums the ranks for the negative and the positive differences, $W+$ and $W-$. The test compares the smaller of the two sums to a table of critical values.

```{r include=FALSE}
wait <- c(3.8, 4.3, 3.5, 4.5, 7.2, 4.1)

eta <- median(wait)
eta_0 <- 4.0
```

Here is a case study. A store claims their checkout wait times are $\le$ 4 minutes. You challenge the claim by sampling 6 checkout experiences. The mean wait time was `r mean(wait) %>% scales::comma(accuracy = 0.1)`, but the data may violate normality.

```{r fig.height=2.5}
data.frame(wait = wait) %>%
  ggplot(aes(sample = wait)) +
  stat_qq() +
  stat_qq_line(col = "goldenrod") +
  theme_minimal() +
  labs(title = "Normal Q-Q Plot")
```

Shapiro-Wilk rejects the null hypothesis of a normally distributed population.

```{r}
shapiro.test(wait)
```

Use the Wilcoxon test instead.

```{r}
(wt <- wilcox.test(wait, mu = 4, alternative = "greater"))
```

> A Wilcoxon Signed-Ranks Test indicated that wait times were *not* statistically significantly higher than the 4-minute claim, *z* = `r wt$statistic`, *p* = `r scales::comma(wt$p.value, accuracy = 0.001)`. 

## Chi-Squared Goodness-of-Fit Test

Use the chi-squared goodness-of-fit test to test whether the observed frequency counts, $O_j$, of the $J$ levels of a categorical variable differ from the expected frequency counts, $E_j$. $H_0$ is $O_j = E_j$. You can use this test for dichotomous, nominal, or ordinal variables. There are only two conditions to use this test:

* the observations are independent, meaning either random assignment or random sampling without replacement from <10% of the population, and 
* the *expected* frequency in each group is >=5.

The Pearson goodness-of-fit test statistic is

$$X^2 = \sum \frac{(O_j - E_j)^2}{E_j}$$

where $O_j = p_j n$ and $E_j = \pi_j n$. The sampling distribution of $X^2$ approaches the $\chi_{J-1}^2$ as the sample size $n \rightarrow \infty$. The assumption that $X^2$ is distributed $\sim \chi^2$ is not quite correct, so you will see researchers subtract .5 from the differences to increase the p-value, the so-called [Yates Continuity Correction](https://en.wikipedia.org/wiki/Yates%27s_correction_for_continuity).

$$X^2 = \sum \frac{(O_j - E_j - 0.5)^2}{E_j}$$

$X^2 \rightarrow 0$ as the saturated model (the observed data represent the fit of the saturated model, the most complex model possible with the data) proportions approach the expected proportions, $p_j \rightarrow \pi_j$. The chi-squared test calculates the probability of the occurrence of $X^2$ at least as extreme given that it is a chi-squared random variable with degrees of freedom equal to the number of levels of the variable minus one, $J-1$. 

#### Example with Theoretical Values {-}

A researcher crosses tall cut-leaf tomatoes with dwarf potato-leaf tomatoes, then classifies the *n* = 1,611 offspring's phenotype. The four phenotypes should occur with relative frequencies 9:3:3:1. The observed frequencies constitute a one-way table.

If you only care about one level (or if the variable is binary) of if, conduct a one-proportion *Z*-test or an exact binomial test. Otherwise, conduct an exact multinomial test (recommended when *n* <= 1,000), Pearson's chi-squared goodness-of-fit test, or a *G*-test.

```{r echo=FALSE, fig.height=2.5, fig.width=6.5}
pheno_type <- c("tall cut-leaf", "tall potato-leaf", "dwarf cut-leaf", "dwarf potato-leaf")
pheno_obs <- c(956, 258, 293, 104)
pheno_pi <- c(9, 3, 3, 1) / (9 + 3 + 3 + 1)
pheno_exp <- sum(pheno_obs) * pheno_pi
names(pheno_obs) <- pheno_type
names(pheno_exp) <- pheno_type

data.frame(type = pheno_type,
           Observed = pheno_obs,
           Expected = pheno_exp) %>%
  pivot_longer(cols = c(Observed, Expected)) %>%
  ggplot(aes(x = fct_inorder(type), y = value, color = fct_rev(name), fill = fct_rev(name))) +
  geom_col(alpha = 0.6, width = 0.5, position = position_dodge2(padding = 0.1)) +
  scale_color_grey(start = 0.6, end = 0.8) +
  scale_fill_grey(start = 0.6, end = 0.8) +
  theme_minimal() +
  labs(title = "Phenotype Counts",
       subtitle = "Expected ratio of 9:3:3:1",
       x = NULL, y = NULL, color = NULL, fill = NULL)
```

#### Conditions {-}

This is a randomized experiment. The minimum expected frequency was `r floor(min(pheno_exp))`, so the chi-squared test of independence is valid.

Had the data violated the $\ge$ 5 condition, you could run an exact test (like the binomial, or in this case, the multinomial), or lump some factor levels together.

#### Results {-}

You can calculate $X^2$ by hand, and find the probability of a test statistic at least as extreme using the $\chi^2$ distribution with 4-1 = 3 degrees of freedom.

```{r collapse=TRUE}
(pheno_x2 <- sum((pheno_obs - pheno_exp)^2 / pheno_exp))
(pheno_p <- pchisq(q = pheno_x2, df = length(pheno_type) - 1, lower.tail = FALSE))
```

That is what `chisq.test()` does. The function applies the Yates continuity correction by default, so I had to specify `correct = FALSE` to exclude it. In this case, setting it to `TRUE` has almost no effect because the sample size is large. 

```{r}
(pheno_chisq_test <- chisq.test(pheno_obs, p = pheno_pi, correct = FALSE))
```

As always, plot the distribution.

```{r warning=FALSE, message=FALSE, fig.height=3, fig.width=5, echo=FALSE, fig.align="center"}
data.frame(chi2 = seq(from = 0, to = 20, by = .1)) %>%
  mutate(density = dchisq(chi2, df = length(pheno_type) - 1)) %>%
  mutate(q95 = if_else(chi2 > qchisq(p = .95, df = length(pheno_type) - 1), density, NA_real_)) %>%
  ggplot() +
  geom_area(aes(x = chi2, y = q95), fill = "firebrick", alpha = 0.6) +
  geom_line(aes(x = chi2, y = density)) +
  geom_vline(aes(xintercept = pheno_x2), color = "goldenrod", size = 1, linetype = 2) +
  labs(title = "Chi-Square Goodness-of-Fit Test",
       subtitle = expression(paste(X^2, " = 9.55 (p = .023)")),
       x = expression(chi^2), y = "Density") +
  theme_minimal() +
  theme(legend.position="none")
```

At this point you can report,

> Of the `r sum(pheno_obs) %>% scales::comma()` offspring produced from the cross-fertiliation, `r pheno_obs["tall cut-leaf"]` were *tall cut-leaf*, `r pheno_obs["tall potato-leaf"]` were *tall potato-leaf*, `r pheno_obs["dwarf cut-leaf"]` where *dwarf cut-leaf*, and `r pheno_obs["dwarf potato-leaf"]` were *dwarf potato-leaf*. A chi-square goodness-of-fit test was conducted to determine whether the offspring had the same proportion of phenotypes as the theoretical distribution. The minimum expected frequency was `r min(pheno_exp) %>% scales::comma()`. The chi-square goodness-of-fit test indicated that the number of tall cut-leaf, tall potato-leaf, dwarf cut-leaf, and dwarf potato-leaf offspring was statistically significantly different from the proportions expected in the theoretical distribution ($X^2$(`r length(pheno_type) - 1`) = `r scales::comma(pheno_chisq_test$statistic, accuracy = 0.001)`, *p* = `r scales::comma(pheno_chisq_test$p.value, accuracy = 0.001)`).

If you reject $H_0$, inspect the residuals to learn which differences contribute most to the rejection. Notice how $X^2$ is a sum of squared standardized cell differences, or "Pearson residuals", 

$$r_i = \frac{o_j - e_j}{\sqrt{e_j}}$$ 

Cells with the largest $|r|$ contribute the most to the total $X^2$. 

```{r}
pheno_chisq_test$residuals^2 / pheno_chisq_test$statistic
```

The two "tall" cells contributed over 95% of the $X^2$ test statistic, with the tall potato-leaf accounting for 67%. This aligns with what you'd expect from the bar plot.

#### Example with Theoretical Distribution {-}

You need to reduce the degrees of freedom (df) in the chi-squared goodness-of-fit test by 1 if you test whether the data conform to a particular distribution instead of a set of theoretical values. 

```{r}
j <- c(0:5)
o <- c(19, 26, 29, 13, 10, 3)
childr_n <- as.character(0:5)
```

Suppose you sample *n* = `r sum(o)` families and count the number of children. The count of children is a Poisson random variable, $J$, with maximum likelihood estimate $\hat{\lambda} = \sum{j_i O_i} / \sum{O_i}$. Test whether the observed values can be described as samples from a Poisson random variable. The probabilities for each possible count are 

$$f(j; \lambda) = \frac{e^{-\hat{\lambda}} \hat{\lambda}^j}{j!}.$$

```{r echo=FALSE, fig.height=2.5, fig.width=6.5}
lambda <- sum(j * o) / sum(o)
f <- exp(-lambda) * lambda^j / factorial(j)
e <- f * sum(o)

data.frame(children = childr_n, Observed = o, Expected = e) %>%
  pivot_longer(cols = c(Observed, Expected)) %>%
  ggplot(aes(x = fct_inorder(children), y = value, color = fct_rev(name), fill = fct_rev(name))) +
  geom_col(alpha = 0.6, width = 0.5, position = position_dodge2(padding = 0.1)) +
  scale_color_grey(start = 0.6, end = 0.8) +
  scale_fill_grey(start = 0.6, end = 0.8) +
  theme_minimal() +
  labs(title = "Children in Family Counts",
       subtitle = "Expected Poisson distribution",
       x = NULL, y = NULL, color = NULL, fill = NULL)
```

#### Conditions {-}

This is random sampling. The minimum expected frequency was `r floor(min(e))`, so the data violates the $\ge$ 5 rule. Lump the last two categories into "4-5".

```{r echo=FALSE, fig.height=2.5, fig.width=6.5}
j <- c(0:4)
o <- c(19, 26, 29, 13, 10 + 3)
childr_n <- c(as.character(0:3), "4-5")
lambda <- sum(j * o) / sum(o)
f <- exp(-lambda) * lambda^j / factorial(j)
e <- f * sum(o)

data.frame(children = childr_n, Observed = o, Expected = e) %>%
  pivot_longer(cols = c(Observed, Expected)) %>%
  ggplot(aes(x = fct_inorder(children), y = value, color = fct_rev(name), fill = fct_rev(name))) +
  geom_col(alpha = 0.6, width = 0.5, position = position_dodge2(padding = 0.1)) +
  scale_color_grey(start = 0.6, end = 0.8) +
  scale_fill_grey(start = 0.6, end = 0.8) +
  theme_minimal() +
  labs(title = "Children in Family Counts",
       subtitle = "Expected Poisson distribution",
       x = NULL, y = NULL, color = NULL, fill = NULL)
```

The minimum expected frequency was `r floor(min(e))`, so now the chi-squared test of independence is valid.

#### Results {-}

Compare the expected values to the observed values with the chi-squared goodness of fit test, but in this case $df = 5 - 1 - 1$ because the estimated parameter $\lambda$ reduces df by 1. You cannot set df in `chisq.test()`, so perform the test manually.

```{r collapse=TRUE}
(X2 <- sum((o - e)^2 / e))
(p.value <- pchisq(q = X2, df = length(j) - 1 - 1, lower.tail = FALSE))
```

```{r warning=FALSE, message=FALSE, fig.height=3, fig.width=5, echo=FALSE, fig.align="center"}
data.frame(chi2 = seq(from = 0, to = 20, by = .1)) %>%
  mutate(density = dchisq(chi2, df = length(j) - 1 - 1)) %>%
  mutate(
    q025 = if_else(chi2 < qchisq(p = .025, df = length(j) - 1 - 1), density, NA_real_),
    q975 = if_else(chi2 > qchisq(p = .975, df = length(j) - 1 - 1), density, NA_real_)) %>%
  ggplot() +
  geom_area(aes(x = chi2, y = q025), fill = "firebrick", alpha = 0.6) +
  geom_area(aes(x = chi2, y = q975), fill = "firebrick", alpha = 0.6) +
  geom_line(aes(x = chi2, y = density)) +
  geom_vline(aes(xintercept = X2), color = "goldenrod", size = 1, linetype = 2) +
  labs(title = "Chi-Square Goodness-of-Fit Test",
       subtitle = expression(paste(X^2, " = 7.09 (p = .931)")),
       x = expression(chi^2), y = "Density") +
  theme_minimal() +
  theme(legend.position="none")
```

At this point you can report,

> Of the `r sum(o) %>% scales::comma()` families sampled, `r o[1]` had no children, `r o[2]` had one child, `r o[3]` had two children, `r o[4]` had three children, and `r o[5]` had 4 or 5 children. A chi-square goodness-of-fit test was conducted to determine whether the observed family sizes follow a Poisson distribution. The minimum expected frequency was `r min(o) %>% scales::comma()`. The chi-square goodness-of-fit test indicated that the number of children was not statistically significantly different from the proportions expected in the Poisson distribution ($X^2$(`r length(o) - 1 - 1`) = `r scales::comma(X2, accuracy = 0.001)`, *p* = `r scales::comma(p.value, accuracy = 0.001)`).

## G-Test

The G-test is a likelihood-ratio statistical significance test increasingly used instead of chi-squared tests. The test statistic is defined 

$$G^2 = 2 \sum O_j \log \left[ \frac{O_j}{E_j} \right]$$

where the 2 multiplier asymptotically aligns with the chi-squared test formula. *G* is distributed $\sim \chi^2$, with the same number of degrees of freedom as in the corresponding chi-squared test. In fact, the chi-squared test statistic is a second order Taylor expansion of the natural logarithm around 1. 

Returning to the phenotype case study in the chi-squared goodness-of-fit test section, you can calculate the $G^2$ test statistic and probability by hand.

```{r collapse=TRUE}
(pheno_g2 <- 2 * sum(pheno_obs * log(pheno_obs / pheno_exp)))
(pchisq(q = pheno_g2, df = length(pheno_type) - 1, lower.tail = FALSE))
```

This is pretty close to the $X^2$ = `r pheno_chisq_test$statistic %>% scales::comma(accuracy = 0.001)`, *p* = `r pheno_chisq_test$p.value %>% scales::comma(accuracy = 0.001)` using the chi-squared goodness-of-fit test. The `DescTools::GTest()` function to conducts a G-test. 

```{r}
DescTools::GTest(pheno_obs, p = pheno_pi)
```

According to the function documentation, the G-test is not usually used for 2x2 tables.


```{r eval=FALSE}
EMT::multinomial.test(o, f, useChisq = TRUE)
```

```{r}
chisq.test(o, e)
```


## One-Sample Poisson Test

If $X$ is the number of successes in $n$ (many) trials when the probability of success $\lambda / n$ is small, then $X$ is a random variable with a Poisson distribution, and the probability of observing $X = x$ successes is 

$$f(x;\lambda) = \frac{e^{-\lambda} \lambda^x}{x!} \hspace{1cm} x \in (0, 1, ...), \hspace{2mm} \lambda > 0$$

with $E(X)=\lambda$ and $Var(X) = \lambda$ where $\lambda$ is estimated by the sample $\hat{\lambda}$,

$$\hat{\lambda} = \sum_{i=1}^N x_i / n.$$

Poisson sampling is used to model counts of events that occur randomly over a fixed period of time. You can use the Poisson distribution to perform an exact test on a Poisson random variable.

#### Example {-}

```{r include=FALSE}
dat_pois <- data.frame(
  goals = c(0, 1, 2, 3, 4, 5, 6, 7, 8),
  freq = c(23, 37, 20, 11, 2, 1, 0, 0, 1)
)
```

You are analyzing goal totals from a sample consisting of the `r sum(dat_pois$freq)` matches in the first round of the 2002 World Cup. The average match produced a mean/sd of `r weighted.mean(x = dat_pois$goals, w = dat_pois$freq) %>% scales::comma(accuracy = .01)` $\pm$ `r radiant.data::weighted.sd(x = dat_pois$goals, w = dat_pois$freq) %>% scales::comma(accuracy = .01)` goals, lower than the 1.5 historical average. Should you reject the null hypothesis that the sample is representative of typical values?

#### Conditions {-}

* The events must be independent of each other. In this case, the goal-count in one match has no effect on goal-counts in other matches.
* The expected value of each event must be the same (homogeneity). In this case, the expected goal-count of each match is the same regardless of which teams are playing. This assumption is often dubious, causing the distribution variance to be larger than the mean, a conditional called *over-dispersion*.

```{r include=FALSE}
o <- dat_pois$freq
j <- dat_pois$goals
lambda <- weighted.mean(j, o)
f <- exp(-lambda) * lambda^j / factorial(j)
e <- f * sum(o)
```

You might also check whether the data is consistent with a Poisson model. This is random sampling, but the data violates the $\ge$ 5 rule because the minimum expected frequency was `r floor(min(e))`. To comply with the minimum frequency rule, lump the last six categories into "3-8".

```{r, echo=FALSE, fig.height=2.5, fig.width=6.5}
o <- c(o[0:3], sum(o[4:9]))
e <- c(e[0:3], sum(e[4:9]))
j <- c(j[0:3], paste(j[4], j[9], sep = "-"))

data.frame(goals = j, Observed = o, Expected = e) %>%
  pivot_longer(cols = c(Observed, Expected)) %>%
  ggplot(aes(x = fct_inorder(as.character(goals)), y = value, color = fct_rev(name), fill = fct_rev(name))) +
  geom_col(alpha = 0.6, width = 0.5, position = position_dodge2(padding = 0.1)) +
  scale_color_grey(start = 0.6, end = 0.8) +
  scale_fill_grey(start = 0.6, end = 0.8) +
  theme_minimal() +
  theme(axis.title = element_text(size = 9)) +
  labs(title = "Counts of 2002 World Cup Round 1 Match Goals",
       subtitle = "Expected Poisson distribution",
       x = "Goals in Match", y = "Count of Matches", color = NULL, fill = NULL)
```

The minimum expected frequency was `r floor(min(e))`, so now the chi-squared test of independence is valid. Compare the expected values to the observed values with the chi-squared goodness of fit test, but in this case $df = 4 - 1 - 1$ because the estimated parameter $\lambda$ reduces the df by 1. You cannot set df in `chisq.test()`, so perform the test manually.

```{r collapse=TRUE}
(X2 <- sum((o - e)^2 / e))
(p.value <- pchisq(q = X2, df = length(j) - 1 - 1, lower.tail = FALSE))
```

```{r warning=FALSE, message=FALSE, fig.height=2.5, fig.width=5, echo=FALSE, fig.align="center"}
data.frame(chi2 = seq(from = 0, to = 10, by = .1)) %>%
  mutate(density = dchisq(chi2, df = length(j) - 1 - 1)) %>%
  mutate(
    q025 = if_else(chi2 < qchisq(p = .025, df = length(j) - 1 - 1), density, NA_real_),
    q975 = if_else(chi2 > qchisq(p = .975, df = length(j) - 1 - 1), density, NA_real_)) %>%
  ggplot() +
  geom_area(aes(x = chi2, y = q025), fill = "firebrick", alpha = 0.6) +
  geom_area(aes(x = chi2, y = q975), fill = "firebrick", alpha = 0.6) +
  geom_line(aes(x = chi2, y = density)) +
  geom_vline(aes(xintercept = X2), color = "goldenrod", size = 1, linetype = 2) +
  labs(title = "Chi-Square Goodness-of-Fit Test",
       subtitle = expression(paste(X^2, "(2) = 0.86, (p = .650)")),
       x = expression(chi^2), y = "Density") +
  theme_minimal() +
  theme(legend.position="none")
```

> Of the `r sum(o) %>% scales::comma()` World Cup matches, `r o[1]` had no goals, `r o[2]` had one goal, `r o[3]` had two goals, and `r o[4]` had 3-8 goals. A chi-square goodness-of-fit test was conducted to determine whether the observed goal counts follow a Poisson distribution. The minimum expected frequency was `r min(o) %>% scales::comma()`. The chi-square goodness-of-fit test indicated that the number of goals scored was not statistically significantly different from the frequencies expected from a Poisson distribution ($X^2$(`r length(o) - 1 - 1`) = `r scales::comma(X2, accuracy = 0.001)`, *p* = `r scales::comma(p.value, accuracy = 0.001)`).

#### Results {-}

The conditions for the exact Poisson test were met, so go ahead and run the test.

```{r}
(pois_val <- poisson.test(
  x = sum(dat_pois$goals * dat_pois$freq), 
  T = sum(dat_pois$freq), 
  r = 1.5)
)
```

Construct a plot showing the 95% CI around the hypothesized value. For a Poisson distribution, I built the distribution around the expected value, $n\lambda$, not the rate, $\lambda$.

```{r echo=FALSE, warning=FALSE, fig.height=3.5, fig.width=6.5}
lrr = qpois(.025, 1.5 * sum(dat_pois$freq), lower.tail = TRUE)
urr = qpois(.025, 1.5 * sum(dat_pois$freq), lower.tail = FALSE)
data.frame(tot_goals = seq(50, 200, by = 1)) %>%
  mutate(prob = dpois(x = tot_goals, lambda = 1.5 * sum(dat_pois$freq)),
         lrr = if_else(tot_goals < lrr, prob, NA_real_),
         urr = if_else(tot_goals > urr, prob, NA_real_)) %>%
  ggplot() +
  geom_line(aes(x = tot_goals, y = prob)) +
  geom_area(aes(x = tot_goals, y = lrr), fill = "firebrick", alpha = 0.6) +
  geom_area(aes(x = tot_goals, y = urr), fill = "firebrick", alpha = 0.6) +
  geom_vline(aes(xintercept = 1.5 * sum(dat_pois$freq)), size = 1) +
  geom_vline(aes(xintercept = pois_val$estimate * sum(dat_pois$freq)),
             color = "goldenrod", size = 1, linetype = 2) +
  theme_minimal() +
  theme(legend.position="none",
        axis.text.y = element_blank()) +
  labs(title = bquote("Two-Tailed Poisson Test"),
       x = expression(paste("n", lambda)),
       y = "Probability")
```

I think you could report these results like this.

> A one-sample exact Poisson test was run to determine whether the number of goals scored in the first round of the 2002 World Cup was different from past World Cups, 1.5. A chi-square goodness-of-fit test indicated that the number of goals was not statistically significantly different from the counts expected in the Poisson distribution ($X^2$(`r length(o) - 1 - 1`) = `r scales::comma(X2, accuracy = 0.001)`, *p* = `r scales::comma(p.value, accuracy = 0.001)`). Data are mean $\pm$ standard deviation, unless otherwise stated. Mean goals scored (`r format(lambda, digits = 2, nsmall = 2)` $\pm$ `r radiant.data::weighted.sd(x = dat_pois$goals, w = dat_pois$freq) %>% scales::comma(accuracy = .01)`) was lower than the historical mean of `r format(1.5, digits = 2, nsmall = 2)`, but was not statistically significantly different (95% CI, `r format(pois_val$conf.int[1], digits = 1, nsmall = 2)` to `r format(pois_val$conf.int[2], digits = 1, nsmall = 2)`), *p* = `r format(pois_val$p.value, digits = 1, nsmall = 3)`.

## Exact Binomial Test

The Clopper-Pearson exact binomial test is precise, but theoretically complicated in that it inverts two single-tailed binomial tests (*No theory here - I'll just rely on the software*). Use the exact binomial test if you have a small sample size or an extreme success/failure probability that invalidates the chi-square and *G* tests. The exact binomial also applies when you have a one-tail test. The exact binomial test has two conditions: 

* independence, and 
* at least $n\pi \ge 5$ successes *or* $n(1\pi)\ge 5$ failures. 

You can use this test for multinomial variables too, but the test only compares a single level's proportion to a hypothesized value.

#### Example {-}

A pharmaceutical company claims its drug reduces fever in >60% of cases. In a random sample of *n* = 40 cases the drug reduces fever in 20 cases. Do you reject the claim?

You are testing $P(x \le 20)$ in *n* = 40 trials when *p* = 60%, a one-tail test. The sample is a random assignment experiment with 20>5 successes and 20>5 failures, so it meets the conditions for the exact binomial test.

```{r}
binom.test(20, 40, p = 0.6, alternative = "greater")
```

The exact binomial test uses the "method of small *p*-values", in which the probability of observing a proportion $p$ as far or further from $\pi_0$ is the sum of all $P(X=p_i)$ where $p_i <= p$.

```{r}
map_dbl(dbinom(0:20, 40, 0.6), ~if_else(. <= 0.5, ., 0)) %>% sum()
```

That is what `pbinom()` does.

```{r}
pbinom(q = 20, size = 40, p = 0.6, lower.tail = TRUE)
```

A 95% confidence interval means 95% of confidence intervals constructed from a random sample of the population will contain the true population proportion. There are several methods to calculate a binomial confidence interval^[[Wikipedia](https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval).] `binom.test()` uses the Clopper-Pearson interval. This method calculates lower ($P_L$) and upper ($P_U$) limits that satisfy 

$$
\begin{eqnarray}
\sum_{x=n_1}^n \binom{n}{x} p_L^x(1 - p_L)^{n-x} &=& \alpha/2\\
\sum_{x=0}^{n_1} \binom{n}{x} p_U^x(1 - p_U)^{n-x} &=& \alpha/2
\end{eqnarray}
$$

where $n_i$ is the measured successes in $n$ trials. For a one-tail test, the confidence interval is calculated with the right side equaling 0 and $/alpha$ instead of $\alpha/2$. A right-tailed 95% confidence interval means 95% of confidence intervals will contain a lower limit that is less than the true population proportion. If you wanted to construct a confidence interval around the population proportion, use a two-sided test.

```{r}
binom.test(20, 40, p = 0.6, alternative = "two.sided")
```


If you just wanted to know whether 20 successes in 40 trials is compatible with a population proportion of 60%, then you could use the chi-squared goodness-of-fit test.

```{r}
chisq.test(x = c(20, 20), p = c(0.6, 0.4), correct = FALSE)
```

## One-Sample Proportion z Test

The *z*-test uses the sample proportion of group $j$, $p_j$, as an estimate of the population proportion $\pi_j$ to evaluate an hypothesized population proportion $\pi_{0j}$ and/or construct a $(1\alpha)\%$ confidence interval around $p_j$ to estimate $\pi_j$ within a margin of error $\epsilon$.

The *z*-test is intuitive to learn, but it only applies when the central limit theorem conditions hold:

* the sample is independently drawn, meaning random assignment (experiments), or random sampling without replacement from <10% of the population (observational studies),
* there are at least 5 successes and 5 failures,
* the sample size is >=30, and
* the expected probability of success is not extreme, between 0.2 and 0.8.

If these conditions hold, the sampling distribution of $\pi$ is normally distributed around $p$ with standard error $se_p = \frac{s_p}{\sqrt{n}} = \frac{\sqrt{p(1p)}}{\sqrt{n}}$. The measured values $p$ and $s_p$ approximate the population values $\pi$ and $\sigma_\pi$. You can define a $(1  \alpha)\%$ confidence interval as $p \pm z_{\alpha / 2}se_p$. Test the hypothesis of $\pi = \pi_0$ with test statistic $z = \frac{p  \pi_0}{se_{\pi_0}}$ where $se_{\pi_0} = \frac{s_{\pi_0}}{\sqrt{n}} = \frac{\sqrt{{\pi_0}(1{\pi_0})}}{\sqrt{n}}$.

#### Example {-}

A machine is supposed to randomly churn out prizes in 60% of boxes. In a random sample of *n* = 40 boxes there are prizes in 20 boxes. Is the machine flawed?

```{r}
prop.test(20, 40, 0.6, "two.sided", correct = FALSE)
```

The first thing you'll notice is that `prop.test()` performs a chi-squared goodness-of-fit test, not a one-proportion *Z*-test!

```{r}
chisq.test(c(20, 40-20), p = c(.6, .4), correct = FALSE)
```

It turns out $P(\chi^2 > X^2)$ equals $2 \cdot P(Z > z).$ Here is the manual calculation of the chi-squared test statistic $X^2$ and resulting *p*-value on 1 dof.

```{r}
pi_0 <- .6
p <- 20 / 40

observed <- c(p, 1-p) * 40
expected <- c(pi_0, 1-pi_0) * 40

X2 <- sum((observed - expected)^2 / expected)
pchisq(X2, 1, lower.tail = FALSE)
```

And here is the manual calculation of the *Z*-test statistic $z$ and resulting *p*-value.
```{r}
se <- sqrt(pi_0*(1-pi_0)) / sqrt(40)
z <- (p - pi_0) / se
pnorm(z, lower.tail = TRUE) * 2
```

The 95% CI presented by `prop.test()` is also not the $p \pm z_{\alpha / 2}se_p$ Wald interval; it is the *Wilson* interval!

```{r}
DescTools::BinomCI(20, 40, method = "wilson")
```

There are a lot of methods (*see* `?DescTools::BinomCI`), and *Wilson* is the one Agresti-Coull recommends. If you want *Wald*, use `DescTools::BinomCI()` with `method = "wald"`.

```{r}
DescTools::BinomCI(20, 40, method = "wald")
```

This matches the manual calculation below.

```{r}
z_crit = qnorm(1 - .05/2)
se <- sqrt(p*(1-p)) / sqrt(40)

(CI <- c(p - z_crit*se, p + z_crit*se))
```

`prop.test()` (and `chissq.test()`) reported a *p*-value of `r prop.test(20, 40, 0.6, "two.sided", correct = FALSE)["p.value"] %>% unlist()`, so you cannot reject the null hypothesis that $\pi = 0.6$. It's good practice to plot this out to make sure your head is on straight.

```{r warning=FALSE, message=FALSE, fig.height=3, fig.width=5, echo=FALSE, fig.align="center"}
p_value = prop.test(20, 40, 0.6, "two.sided", correct = FALSE)["p.value"] %>% unlist()
se_pi_0 <- sqrt(pi_0 * (1 - pi_0) / 40)
p_value <- pnorm(z, lower.tail = FALSE) * 2
data.frame(pi = seq(from = 0.2, to = 0.8, by = .001)) %>%
  mutate(density = dnorm(pi, pi_0, se_pi_0),
         q25 = ifelse(pi < qnorm(p = .025, pi_0, se_pi_0), density, as.numeric(NA)),
         q95 = ifelse(pi > qnorm(p = .975, pi_0, se_pi_0), density, as.numeric(NA))) %>%
  ggplot() +
  geom_area(aes(x = pi, y = q25), fill = "#EF7C8E") +
  geom_area(aes(x = pi, y = q95), fill = "#EF7C8E") +
  geom_line(aes(x = pi, y = density)) +
  geom_vline(aes(xintercept = p), color = "#0C6980", size = 1.0) +
  geom_vline(aes(xintercept = pi_0), color = "#000000", size = 0.5, linetype = 2) +
  labs(title = "One-Proportion Z-Test",
       subtitle = expression(paste(pi[0], " = 0.6, p = 0.5, p-value = 0.197")),
       x = expression(pi), y = "Density") +
  theme_minimal() +
  theme(legend.position="none")
```

Incidentally, if you have a margin of error requirement, you can back into the required sample size to achieve it. Just solve the margin of error equation $\epsilon  = z_{\alpha/2}^2 = \sqrt{\frac{\pi_0(1-\pi_0)}{n}}$ for $n = \frac{z_{\alpha/2}^2 \pi_0(1-\pi_0)}{\epsilon^2}.$

## 1 sample t Test for Categorical Var

This test applies when you do not know the population variance.

## Wilcoxon 1-Sample Median Test for Categorical Var

This test applies when the variable is not normally distributed.

## Multivariate Statistics

The t-tests and analysis of variance tests have multivariate analogs. Multivariate statistics apply when multiple variables are simultaneously analyzed. Hotellings's *T*^2 extends the independent samples *t*-test and MANOVA extends ANOVA to cases where there are two or more dependent variables (e.g., do math, science, and reading scores depend on students' anxiety level?). 

The mean of variable $j$ is the average of row vector $X_j$, $\bar{x}_j = \frac{1}{n} \sum_{i = 1}^n X_{ij}$. $\bar{x}_j$ estimates the population mean, $\mu_j = E(X_j)$. The collection of means is a column vector, $\bar{\mathbf{x}}$ estimating $\boldsymbol{\mu}$.

The variance of variable $j$ is the average squared difference from the mean for row vector $X_j$, $s_j^2 = \frac{1}{n-1} \sum_{i=1}^n (X_{ij} - \bar{x}_j)^2$. It estimates the population variance, $\sigma_j^2 = E(X_j - \mu_j)^2$. The collection of variances is a column vector, $\mathbf{s}^2$ estimating $\boldsymbol{\sigma}^2$. 

The covariance of variables $j$ and $k$ is the average product of differences from their respective means, $s_{jk} = \frac{1}{n-1} \sum_{i=1}^n (X_{ij} - \bar{x}_j) (X_{ik} - \bar{x}_k)$. It estimates the population covariance, $\sigma_{jk} = E\{ (X_{ij} - \mu_j) (X_{ik} - \mu_k)\}$. The generalization across the entire matrix is the _variance-covariance matrix_, $\textbf{S}$ which estimates $\boldsymbol{\Sigma}$.

$\bar{\mathbf{x}}$ is a function of random data, so it is also a random vector with a _sampling distribution_ mean and variance-covariance matrix. The variance of the sample mean is $V(\bar{\mathbf{x}}) = \frac{\textbf{S}}{n}$. It estimates the variance of the population mean,  $V(\bar{\mathbf{x}}) = \frac{\boldsymbol{\Sigma}}{n}$. If the samples are taken from a normal distribution or the sample size is large, the sampling distribution is approximately normal, $\bar{\textbf{x}} \sim N \left(\boldsymbol{\mu}, \frac{\boldsymbol{\Sigma}}{n} \right)$.

The joint estimate of confidence intervals (CIs) around a multivariate set of population means is complicated by how the individual variables are treated.

- **One at a time intervals**. The $1 - \alpha$ CI is $\bar{x}_j \pm t_{n-1}(\alpha / 2) \frac{s_j}{\sqrt{n}}$.

- **Bonferroni method**. A family of confidence intervals has a family-wide error rate of at least one CI not capturing its population mean equal to the sum of the individual error rates. The Bonferroni method divides $\alpha$ by the $p$ variables in the family, so the $1 - \alpha$ CI is $\bar{x}_j \pm t_{n-1}(\alpha / 2p) \frac{s_j}{\sqrt{n}}$.

- **Simultaneous confidence region**. This method considers the family of all possible linear combinations of the population means. The $1 - \alpha$ CI is $\bar{x}_j \pm \sqrt{\frac{p(n-1)}{n-p} F_{p, n-p}(\alpha)} \frac{s_j}{\sqrt{n}}$.

**Quick Example**. 

```{r collapse=TRUE}
# Suppose you have p = 3 variables with mean and sd as follows.
p <- 3
n <- c(25, 25, 25)
M <- c(.84390, 1.79268, .70440)
SD <- c(.11402, .28347, .10756)
SE <- SD / sqrt(n)

# One at a time margins of error
(prob <- 1 - .05/2)
(mult <- qt(prob, n-1))
(ME <- mult * SE)

# Bonferonni
(prob <- 1 - .05/(2*p))
(mult <- qt(prob, n-1))
(ME <- mult * SE)

# Simultaneous
(mult <- sqrt((p * (n-1) / (n-p)) * qf(.95, p, n-p)))
(ME <- mult * SE)
```





















<!--chapter:end:04-one-sample.Rmd-->

# Group Differences

```{r include=FALSE}
library(tidyverse)
library(gtsummary)
library(foreign)
library(scales)
library(janitor)
library(flextable)
library(broom)
library(glue)

theme_set(
  jtools::theme_apa() +
    theme(
      plot.caption = element_text(hjust = 0)
    )
)
```

This section presents a statistical tests of _comparison_. Which test to use depends on the structure of the data. Below is a guide to which test to use.

**Continuous (interval or ratio) and Ordinal Outcomes**

- The **Independent samples _t_-test** (\@ref(ttest)) is the main way to compare a continuous dependent variable between the two levels of a binomial independent categorical variable. Revert to the nonparametric **Wilcoxon Rank Sum Test** (\@ref(wilcoxonranksum)) if the _t_-test assumptions fail. A special case arises when samples are paired. Paired samples are more like one-sample tests where the dependent variable is the _difference_ between the pairs. Use the **Paired Samples _t_-test** (\@ref(pairedttest)) or the nonparametric **Wilcoxon signed-rank test** (\@ref(wilcoxonsignedrank)).
- If the independent categorical variable is multinomial, conduct an **ANOVA** (\@ref(onewayanova)) test or the nonparametric **Kruskal-Wallis test** (\@ref(kw)).

**Discrete (count) Outcomes**

- The **Chi-square test of homogeneity** (\@ref(chisqhomogeneity)) is the main way to compare a discrete dependent variable among the levels of a binomial or multinomial independent categorical variable. Revert to the nonparametric **Fisher's Exact Test** (\@ref(fisherexact)) if the sample size is small. Handle the special case of paired samples with the **Pairwise Prop Test** (\@ref(pairwiseproptest)) or the nonparametric **McNemar's test** (\@ref(mcnemar)).

## Independent Samples t-Test {#ttest}

If a population measure *X* is normally distributed with mean $\mu_X$ and variance $\sigma_X^2$, and a population measure *Y* is normally distributed with mean $\mu_Y$ and variance $\sigma_Y^2$, then their difference is normally distributed with mean $d = \mu_X - \mu_Y$ and variance $\sigma_{XY}^2 = \sigma_X^2 + \sigma_Y^2$. By the CLT, as the sample sizes grow, a non-normally distributed *X* and *Y* will approach normality, and so will their difference.

The **independent samples t-test** evaluates an hypothesized difference, $d_0$ (H0: $d = d_0$), from the difference in sample means $\hat{d} = \bar{x} - \bar{y}$, or constructs a (1 - $\alpha$)% confidence interval around $\hat{d}$ to estimate $d$ within a margin of error, $\epsilon$.

In principal, you can evaluate $\hat{d}$ with either a *z*-test or a *t*-test. Both require independent samples and approximately normal sampling distributions. Sampling distributions are normal if the underlying populations are normally distributed, or if the sample sizes are large ($n_X$ and $n_Y$ $\ge$ 30). However, the *z*-test additionally requires known sampling distribution variances, $\sigma^2_X$ and $\sigma^2_Y$. These variances are never known, so always use the *t*-test.

The *z*-test assumes $d$ is normally distributed around $\hat{d} = d$ with standard error $SE = \sqrt{\frac{\sigma_X^2}{n_X} + \frac{\sigma_Y^2}{n_Y}}.$ The test statistic for H0: $d = d_0$ is $Z = \frac{\hat{d} - d_0}{SE}$. The (1 - $\alpha$)% CI is $d = \hat{d} \pm z_{(1 - \alpha {/} 2)} SE$.  

The *t*-test assumes $d$ has a *t*-distribution around $\hat{d} = d$ with standard error $SE = \sqrt{\frac{s_X^2}{n_X} + \frac{s_Y^2}{n_Y}}.$ The test statistic for H0: $d = d_0$ is $T = \frac{\hat{d} - d_0}{SE}$. The (1 - $\alpha$)% CI iss $d = \hat{d} \pm t_{(1 - \alpha / 2), (n_X + n_Y - 2)} SE$.

There is a complication with the *t*-test *SE* and degrees of freedom. If the sample sizes are small and the standard deviations from each population are similar (the ratios of $s_X$ and $s_Y$ are <2), pool the variances, $s_p^2 = \frac{(n_X - 1) s_X^2 + (n_Y-1) s_Y^2}{n_X + n_Y-2}$, so that $SE = s_p \sqrt{\frac{1}{n_X} + \frac{1}{n_Y}}$ and the degrees of freedom (df) = $n_X + n_Y - 2$ (the **pooled variances t-test**). Otherwise, $SE = \sqrt{\frac{s_X^2}{n_X} + \frac{s_Y^2}{n_Y}}$, but you reduce df using the Welch-Satterthwaite correction, $df = \frac{\left(\frac{s_X^2}{n_X} + \frac{s_Y^2}{n_Y}\right)^2}{\frac{s_X^4}{n_X^2\left(N_X-1\right)} + \frac{s_Y^4}{n_Y^2\left(N_Y-1\right)}}$ (the **separate variance t-test**, or **Welch's t-test**).

## Wilcoxon Rank Sum Test {#wilcoxonranksum}

The **Wilcoxon rank sum test**^[The Mann-Whitney U test is also called the Mann-Whitney U test, Wilcoxon-Mann-Whitney test, and the two-sample Wilcoxon test] is a nonparametric alternative to the independent-samples *t*-test. Use the the test when the samples are not normally distributed or when the response variables are ordinal rather continuous. In the first case where the normality assumption fails, the test evaluates H0 that the two samples are from the same population distribution. In the second case where the response variables are ordinal, the test evaluates the difference in medians. 

The Wilcoxon Rank Sum test ranks the response values, then sums the ranks for the reference group, $W = \sum R_1$. The test statistic is $U = W - \frac{n_2(n_2 + 1)}{2}$ where $n_2$ is the number of observations in the test group. $U$ will equal 0 if there is complete separation between the groups, and $n_1 n_2$ if there is complete overlap. Reject H0 if $U$ is sufficiently small.

## Case Study 1 {-}

```{r include=FALSE}
ind_num <- list()

ind_num$t_dat <- read.spss("./input/independent-samples-t-test.sav", to.data.frame = TRUE)
ind_num$t_n <- with(ind_num$t_dat, by(engagement, gender, length))
ind_num$t_mean <- with(ind_num$t_dat, by(engagement, gender, mean))
ind_num$t_sd <- with(ind_num$t_dat, by(engagement, gender, sd))

ind_num$mw_dat <- read.spss("./input/mann-whitney-test.sav", to.data.frame = TRUE)
ind_num$mw_n <- with(ind_num$mw_dat, by(engagement, gender, length))
ind_num$mw_median <- with(ind_num$mw_dat, by(engagement, gender, median))
ind_num$mw_sd <- with(ind_num$mw_dat, by(engagement, gender, sd))
```

A company shows an advertisement to $n_M$ = `r ind_num$t_n["Male"]` males and $n_F$ = `r ind_num$t_n["Female"]` females, then measures their engagement with a survey. Do the groups' mean engagement scores differ?

[Laerd](https://statistics.laerd.com/) has two data sets for this example. One meets the conditions for a *t*-test, and the other fails the normality test, forcing you to use the Mann-Whitney U test.

```{r fig.height=3.5, fig.width=7.5, echo=FALSE}
bind_rows(
  `t-Test` = ind_num$t_dat, 
  `Mann-Whitney` = ind_num$mw_dat, 
  .id = "set"
) %>%
  ggplot(aes(x = engagement, fill = gender)) +
  # geom_density(alpha = 0.6) +
  geom_histogram(bins = 20, alpha = 0.6, position = "dodge") +
  facet_wrap(~fct_rev(set)) +
  labs(title = "Mean Engagement Scores Counts", fill = NULL) +
  theme_light() +
  scale_fill_manual(values = c("Male" = "slategray", "Female" = "snow3"))
```

```{r fig.height=3.5, fig.width=7.5, echo=FALSE}
bind_rows(
  `t-test` = ind_num$t_dat,
  `Mann-Whitney` = ind_num$mw_dat,
  .id = "set"
) %>%
  group_by(set, gender) %>%
  summarize(.groups = "drop",
            mean_engage = mean(engagement),
            n = n(),
            se_engage = sd(engagement) / sqrt(n),
            ci_low = mean_engage - qt(.975, n-1) * se_engage,
            ci_high = mean_engage + qt(.975, n-1) * se_engage) %>%
  ggplot(aes(x = gender)) +
  geom_col(aes(y = mean_engage), fill = "snow3", color = "snow4", width = 0.25) +
  geom_errorbar(aes(ymin = ci_low, ymax = ci_high), color = "snow4", width = .125) +
  theme_minimal() +
  facet_wrap(~fct_rev(set)) +
  labs(x = NULL, y = NULL,
       title = "Mean Engagement Score", 
       subtitle = "Two data sets with similar means.",
       caption = "Error bars show 95% CI.")
```

The *t*-test data set has the following summary statistics.

```{r}
(ind_num$t_gt <- ind_num$t_dat %>% 
  gtsummary::tbl_summary(
    by = c(gender), 
    statistic = list(all_continuous() ~ "{mean} ({sd})")
  ))
```

> There were `r ind_num$t_n["Male"]` male and `r ind_num$t_n["Female"]` female participants. Data are mean $\pm$ standard deviation, unless otherwise stated. The advertisement was more engaging to male viewers, `r gtsummary::inline_text(ind_num$t_gt, engagement, column = "Male")`, than female viewers, `r gtsummary::inline_text(ind_num$t_gt, engagement, column = "Female")`.

The Mann-Whitney data set has the following summary statistics.

```{r}
(ind_num$mw_gt <- ind_num$mw_dat %>% 
  gtsummary::tbl_summary(
    by = c(gender), 
    statistic = list(all_continuous() ~ "{mean} ({sd})")
  ))
```

> There were `r ind_num$mw_n["Male"]` male and `r ind_num$mw_n["Female"]` female participants. Data are mean $\pm$ standard deviation, unless otherwise stated. The advertisement was more engaging to male viewers, `r gtsummary::inline_text(ind_num$mw_gt, engagement, column = "Male")`, than female viewers, `r gtsummary::inline_text(ind_num$mw_gt, engagement, column = "Female")`.

#### Conditions {-}

The independent samples *t*-test and Mann-Whitney U test apply when 1) the response variable is continuous, 2) the independent variable is binomial, and 3) the observations are independent. The decision between the *t*-test and Mann-Whitney stems from two additional conditions related to the data distribution - if both conditions hold, use the *t*-test; otherwise use Mann-Whitney.

1. **Outliers**. There should be no outliers in either group. Outliers exert a large influence on the mean and standard deviation. Test with a box plot. If there are outliers, you might be able to drop them or transform the data.
2. **Normality**.  Values should be *nearly* normally distributed. The *t*-test is robust to normality, but this condition is important with small sample sizes. Test with Q-Q plots or the Shapiro-Wilk test for normality. If the data is very non-normal, you might be able to transform the data.

If the data passes the two conditions, use the *t*-test, but now you need to check a third condition related to the variances to determine which flavor of the *t*-test to use.

3. **Homogeneous Variances**. Use *pooled-variances* if the variances are homogeneous; otherwise use the *separate variances* method. Test with Levene's test of equality of variances.

If the data does not pass the first two conditions, use Mann-Whitney, but now you need to check a third condition here as well. The condition does not affect how to perform the test, but rather how to interpret the results.

3. **Distribution shape**. If the distributions have the same shape, interpret the Mann-Whitney result as a comparison of the *medians*; otherwise interpret the result as a comparison of the *mean ranks*.

##### Checking for Outliers {-}

Assess outliers with a box plot. Box plot whiskers extend up to 1.5\*IQR from the upper and lower hinges and outliers (beyond the whiskers) are are plotted individually.

```{r echo=FALSE, fig.height=2.5, fig.width=7.5}
bind_rows(
  `t-test` = ind_num$t_dat,
  `Mann-Whitney` = ind_num$mw_dat,
  .id = "set"
) %>%
  ggplot(aes(x = gender, y = engagement)) +
  geom_boxplot(fill = "snow3", color = "snow4", alpha = 0.6, width = 0.25, 
               outlier.color = "goldenrod", outlier.size = 2) +
  theme_minimal() +
  facet_wrap(~fct_rev(set)) +
  labs(title = "Boxplot of Engagement Score", y = "Score", x = NULL)
```

For the *t* test data set,

>There were no outliers in the data, as assessed by inspection of a boxplot.

and for the Mann-Whitney data set,

>There was one outlier in the data, as assessed by inspection of a boxplot.

If the outliers are data entry errors or measurement errors, fix or discard them. If the outliers are genuine, you have a couple options before reverting to the Mann-Whitney U test.

* Leave it in if it doesn't affect the conclusion (compared to taking it out).
* Transform the variable. Don't do this unless the variable is also non-normal. Transformation also has the downside of making interpretation more difficult.

##### Checking for Normality {-}

Assume the population is normally distributed if *n* $\ge$ 30. Otherwise, assess a Q-Q plot, skewness and kurtosis values, or a histogram. If you still don't feel confident about normality, run a Shapiro-Wilk test.

There are only $n_M$ = `r ind_num$t_n["Male"]` male and $n_F$ = `r ind_num$t_n["Female"]` female observations, so you need to test normality. The QQ plot indicates normality in the t-test data set, but not in the Mann-Whitney data set.

```{r fig.height=3.5, fig.width=7.5}
bind_rows(
  `t-test` = ind_num$t_dat,
  `Mann-Whitney` = ind_num$mw_dat,
  .id = "set"
) %>%
  ggplot(aes(sample = engagement, group = gender, color = fct_rev(gender))) +
  stat_qq() +
  stat_qq_line(col = "goldenrod") +
  theme_minimal() + theme(legend.position = "top") +
  facet_wrap(~fct_rev(set)) +
  labs(title = "Normal Q-Q Plot", color = NULL)
```

Run Shapiro-Wilk separately for the males and for the females. Since we are looking at two data sets in tandem, there are four tests below. For the t-test data set, 

```{r}
(ind_num$t_shapiro <- split(ind_num$t_dat, ind_num$t_dat$gender) %>% 
  map(~shapiro.test(.$engagement))
)
```

>Engagement scores for each level of gender were normally distributed, as assessed by Shapiro-Wilk's test (*p* > .05).

For the Mann-Whitney data set,

```{r}
(ind_num$mw_shapiro <- split(ind_num$mw_dat, ind_num$mw_dat$gender) %>% 
  map(~shapiro.test(.$engagement))
)
```

>Engagement scores for each level of gender were not normally distributed for the Female sample, as assessed by Shapiro-Wilk's test (*p* = `r ind_num$mw_shapiro$Female$p.value %>% scales::comma(accuracy = 0.001)`).

If the data is not normally distributed, you still have a couple options before reverting to the Mann-Whitney U test.

* Transform the dependent variable.
* Carry on regardless - the independent samples *t*-test is fairly robust to deviations from normality.

##### Checking for Homogenous Variances {-}

If the data passed the outliers and normality tests, you will use the *t*-test, so now you need to test the variances to see which version (*pooled-variances* method if variances are homogeneous; *separate variances* if variances are heterogeneous). A rule of thumb is that homogeneous variances have a ratio of standard deviations between 0.5 and 2.0:

```{r}
sd(ind_num$t_dat %>% filter(gender == "Male") %>% pull(engagement)) /
  sd(ind_num$t_dat %>% filter(gender == "Female") %>% pull(engagement))
```

You can also use the *F* test to compare the ratio of the sample variances $\hat{r} = s_X^2 / s_Y^2$ to an hypothesized ratio of population variances $r_0 = \sigma_X^2 / \sigma_Y^2 = 1.$

```{r}
var.test(ind_num$t_dat %>% filter(gender == "Female") %>% pull(engagement), 
         ind_num$t_dat %>% filter(gender == "Male") %>% pull(engagement))
```

Bartlett's test is another option.

```{r}
bartlett.test(ind_num$t_dat$engagement, ind_num$t_dat$gender)
```

Levene's test is a third option. Levene's is less sensitive to departures from normality than Bartlett.

```{r}
(ind_num$levene <- with(ind_num$t_dat, 
                        car::leveneTest(engagement, gender, center = "mean"))
)
```

> There was homogeneity of variances for engagement scores for males and females, as assessed by Levene's test for equality of variances (*p* = `r ind_num$levene %>% pluck("Pr(>F)", 1) %>% scales::number(accuracy = 0.001)`).

##### Checking for Similar Distributions {-}

If the data fail either the outliers or the normality test, use the Mann-Whitney test. The Mann-Whitney data set failed both, so the Mann-Whitney test applies. Now you need to test the distributions to determine how to interpret its results. If the distributions are similarly shaped, interpret the Mann-Whitney U test as inferences about differences in medians between the two groups. If the distributions are dissimilar, interpret the test as inferences about the distributions, lower/higher scores and/or mean ranks.

```{r fig.height = 3.5, fig.width=7.5, echo=FALSE}
ind_num$mw_dat %>%
  ggplot(aes(x = engagement, color = fct_rev(gender))) +
  geom_density() +
  theme_minimal() +
  theme(legend.position = "right") +
  labs(title = "Engagement Distribution", color = NULL)
```

> Distributions of the engagement scores for males and females were similar, as assessed by visual inspection.

#### Test {-}

Conduct the *t*-test or the Mann-Whitney U test. 

##### t-Test {-}

The the *t*-test data the variances were equal, so the pooled-variances version applies (`t.test(var.equal = TRUE)`).

```{r}
(ind_num$t_test <- t.test(engagement ~ gender, data = ind_num$t_dat, var.equal = TRUE))
```

> There was a statistically significant difference in mean engagement score between males and females, with males scoring higher than females, `r scales::number(ind_num$t_test$estimate[1] - ind_num$t_test$estimate[2] %>% as.numeric(), accuracy = 0.01)` (95% CI, `r ind_num$t_test$conf.int[1] %>% scales::number(accuracy = 0.01)` to `r ind_num$t_test$conf.int[2] %>% scales::number(accuracy = 0.01)`), t(`r ind_num$t_test$parameter`) = `r ind_num$t_test$statistic %>% scales::number(accuracy = 0.001)`, *p* = `r ind_num$t_test$p.value %>% scales::number(accuracy = 0.001)`.

The effect size, Cohen's *d*, is defined as $d = |M_D| / s$, where $|M_D| = \bar{x} - \bar{y}$, and $s$ is the pooled sample standard deviation, $s_p = \sqrt{\frac{(n_X - 1) s_X^2 + (n_Y-1) s_Y^2}{n_X + n_Y-2}}$. $d <.2$ is considered trivial, $.2 \le d < .5$ small, and $.5 \le d < .8$ large.

```{r message=FALSE}
(d <- effectsize::cohens_d(engagement ~ gender, data = ind_num$t_dat, pooled_sd = TRUE))
```

> There was a large difference in mean engagement score between males and females, Cohen's d = `r scales::number(d$Cohens_d, accuracy = 0.01)` 95% CI [`r scales::number(d$CI_low, accuracy = 0.01)`, `r scales::number(d$CI_high, accuracy = 0.01)`]

Before rejecting the null hypothesis, construct a plot as a sanity check.

```{r echo=FALSE, warning=FALSE, fig.height=3.5, fig.width=7.5}
d_bar <- ind_num$t_mean["Male"] - ind_num$t_mean["Female"]
d_0 <- 0
se <- effectsize::sd_pooled(engagement ~ gender, data = ind_num$t_dat) * 
  sqrt(1/ind_num$t_n["Male"] + 1/ind_num$t_n["Female"])
df <- sum(ind_num$t_n) - 2
lrr <- se * qt(.025, df)
urr <- se * qt(.975, df)
data.frame(d = seq(-1.0, 1.0, by = .01)) %>%
  mutate(t = (d - d_0) / se,
         prob = dt(x = t, df = df),
         lrr = if_else(d < lrr, prob, NA_real_),
         urr = if_else(d > urr, prob, NA_real_)) %>%
  ggplot() +
  geom_line(aes(x = d, y = prob)) +
  geom_area(aes(x = d, y = lrr), fill = "firebrick", alpha = 0.6) +
  geom_area(aes(x = d, y = urr), fill = "firebrick", alpha = 0.6) +
  geom_vline(aes(xintercept = d_0), size = 1) +
  geom_vline(aes(xintercept = d_bar), color = "goldenrod", size = 1, linetype = 2) +
  annotate("text", x = d_bar + .06, y = .05, 
           label = scales::number(d_bar, accuracy = .01), 
           size = 3, color = "goldenrod3") +
  theme_minimal() +
  theme(legend.position="none",
        axis.text.y = element_blank()) +
  labs(title = bquote("Two-Tailed t-test"),
       x = "Difference in Group Means (Male - Female)",
       y = "Probability",
       caption = "Shaded area is 95% CI.")
```

##### Wilcoxon Rank Sum test {-}

The reference level for the `gender` variable is males, so the Wilcoxon Rank Sum test statistic is the sum of male ranks minus $n_f(n_f + 1) / 2$ where $n_f$ is the number of females. You can calculate the test statistic by hand.

```{r}
(ind_num$mw_test_manual <- ind_num$mw_dat %>% 
  mutate(R = rank(engagement)) %>%
  group_by(gender) %>%
  summarize(.groups = "drop", n = n(), R = sum(R), meanR = sum(R)/n()) %>%
  pivot_wider(names_from = gender, values_from = c(n, R, meanR)) %>%
  mutate(U = R_Male - n_Female * (n_Female + 1) / 2))
```

Compare the test statistic to the Wilcoxon rank sum distribution with `pwilcox()`.

```{r}
pwilcox(
  q = ind_num$mw_test_manual[1, ]$U - 1, 
  m = ind_num$mw_test_manual[1, ]$n_Male, 
  n = ind_num$mw_test_manual[1, ]$n_Male, 
  lower.tail = FALSE
) * 2
```

There is a function for all this.

```{r}
(ind_num$mw_test <- wilcox.test(
  engagement ~ gender, 
  data = ind_num$mw_dat, 
  exact = TRUE, 
  correct = FALSE,
  conf.int = TRUE))
```

> Median engagement score was not statistically significantly different between males and females, *U* = `r ind_num$mw_test$statistic`, *p* = `r ind_num$mw_test$p.value %>% scales::number(accuracy = .001)`, using an exact sampling distribution for *U*.

Now you are ready to report the results. Here is how you would report the *t* test.

> Data are mean $\pm$ standard deviation, unless otherwise stated. There were `r ind_num$t_n[["Male"]]` male and `r ind_num$t_n[["Female"]]` female participants. An independent-samples t-test was run to determine if there were differences in engagement to an advertisement between males and females. There were no outliers in the data, as assessed by inspection of a boxplot. Engagement scores for each level of gender were normally distributed, as assessed by Shapiro-Wilk's test (*p* > .05), and there was homogeneity of variances, as assessed by Levene's test for equality of variances (*p* = `r ind_num$levene %>% pluck("Pr(>F)", 1) %>% scales::number(accuracy = 0.001)`). The advertisement was more engaging to male viewers (`r scales::number(ind_num$t_mean["Male"], accuracy = .01)` $\pm$ = `r scales::number(ind_num$t_sd["Male"], accuracy = .01)`) than female viewers (`r scales::number(ind_num$t_mean["Female"], accuracy = .01)` $\pm$ = `r scales::number(ind_num$t_sd["Female"], accuracy = .01)`), a statistically significant difference of `r scales::number(ind_num$t_test$estimate[1] - ind_num$t_test$estimate[2] %>% as.numeric(), accuracy = 0.01)` (95% CI, `r ind_num$t_test$conf.int[1] %>% scales::number(accuracy = 0.01)` to `r ind_num$t_test$conf.int[2] %>% scales::number(accuracy = 0.01)`), t(`r ind_num$t_test$parameter`) = `r ind_num$t_test$statistic %>% scales::number(accuracy = 0.001)`, *p* = `r ind_num$t_test$p.value %>% scales::number(accuracy = 0.001)`, *d* = `r d$Cohens_d %>% scales::number(accuracy = 0.01)`.

Here is how you would report the Mann-Whitney U-Test.

> A Mann-Whitney U test was run to determine if there were differences in engagement score between males and females. Distributions of the engagement scores for males and females were similar, as assessed by visual inspection. Median engagement score for males (`r ind_num$mw_median["Male"] %>% as.numeric() %>% scales::number(accuracy = .01)`) and females (`r ind_num$mw_median["Female"] %>% as.numeric() %>% scales::number(accuracy = .01)`) was not statistically significantly different, *U* = `r ind_num$mw_test$statistic`, *p* = `r ind_num$mw_test$p.value %>% scales::number(accuracy = .001)`, using an exact sampling distribution for *U*.

Had the distributions differed, you would report the Mann-Whitney like this:

> A Mann-Whitney U test was run to determine if there were differences in engagement score between males and females. Distributions of the engagement scores for males and females were not similar, as assessed by visual inspection. Engagement scores for males (mean rank = `r ind_num$mw_test_manual %>% pull(meanR_Male)`) and females (mean rank = `r ind_num$mw_test_manual %>% pull(meanR_Female)`) were not statistically significantly different, *U* = `r ind_num$mw_test$statistic`, *p* = `r ind_num$mw_test$p.value %>% scales::number(accuracy = .001)`, using an exact sampling distribution for *U*.

## Paired Samples t-Test {#pairedttest}

There are two common study designs that employ a paired samples t-test to compare two related groups. One relates the groups as two time points for the same subjects. The second relates the groups as two tests of the same subjects, e.g. comparing reaction time under two lighting conditions.

The *paired samples t-test* uses the mean of sampled paired differences $\bar{d}$ as an estimate of the mean of the population paired differences $\delta$ to evaluate an hypothesized mean $\delta_0$.  Test $H_0: \delta = \delta_0$ with test statistic $T = \frac{\bar{d} - \delta_0}{se}$, or define a $(1 - \alpha)\%$ confidence interval as $\delta = \bar{d} \pm t_{1 - \alpha / 2, n - 1} se$.  *The paired t-test is really just a one-sample mean t-test operating on variable that is defined as the difference between two variables*.

The paired samples *t* test applies when the sampling distribution of the mean of the population paired differences is normally distributed and there are no significant outliers.

## Wilcoxon Signed-Rank Test {#wilcoxonsignedrank}

The *Wilcoxon signed-rank test* is a nonparametric alternative to the paired-samples t-test for cases in which the paired differences fails the normality condition, but is at least symmetrically distributed.

The test statistic is the sum product of the difference signs (-1, +1) and the rank of the difference absolute values, $W = \sum_{i=1}^n sign (d_i) \cdot R_i$. The more differences that are of one sign, or of extreme magnitude, the larger $W$ is likely to be, and the more likely to reject $H_0$ of equality of medians.

### Sign Test {-}

The *sign test* is an alternative to the Wilcoxon signed-rank test for cases in which the paired differences fails the symmetrical distribution condition.

The test statistic is the count of pairs whose difference is positive, $W = cnt(d_i > 0)$. $W \sim b(n, 0.5)$, so the sign test is really just an exact binomial test (*exact sign test*), or for large *n*-size, the normal approximation to the binomial (*sign test*).

## Case Study 2 {-}

```{r include=FALSE}
drink <- list()

drink$t_dat <- read.spss("./input/paired-samples-t-test.sav", to.data.frame = TRUE) %>%
  mutate(diff = carb_protein - carb)
drink$t_n <- nrow(drink$t_dat)
drink$t_mean <- mean(drink$t_dat$carb_protein - drink$t_dat$carb)
drink$t_sd <- sd(drink$t_dat$carb_protein - drink$t_dat$carb)

drink$wilcoxon_dat <- read.spss("./input/wilcoxon-signed-rank-test.sav", 
                                to.data.frame = TRUE) %>%
  mutate(diff = carb_protein - carb)
drink$wilcoxon_n <- nrow(drink$wilcoxon_dat)
drink$wilcoxon_median <- list()
drink$wilcoxon_med$carb_protein <- median(drink$wilcoxon_dat$carb_protein)
drink$wilcoxon_med$carb <- median(drink$wilcoxon_dat$carb) 

drink$sign_dat <- read.spss("./input/sign-test.sav", to.data.frame = TRUE) %>%
  mutate(diff = carb_protein - carb)
drink$sign_n <- nrow(drink$sign_dat)
drink$sign_median <- list()
drink$sign_med$carb_protein <- median(drink$sign_dat$carb_protein)
drink$sign_med$carb <- median(drink$sign_dat$carb) 
```

$n$ = `r drink$t_n` athletes consume a carb-only or carb+protein drink prior to running as far as possible in 2 hours and a researcher records their distances under each condition. Do the distances differ from 0?

[Laerd](https://statistics.laerd.com/) has three data sets for this example. One meets the conditions for a t-test. The second fails the normality condition, but is symmetric and meets the conditions for the Wilcoxon test. The third fails the symmetry condition and requires the sign test.

##### t-test data set {-}

```{r}
(drink$t_gt <- drink$t_dat %>% 
  gtsummary::tbl_summary(statistic = list(all_continuous() ~ "{mean} ({sd})"))
)
```

> There were `r drink$t_n` participants. Data are mean $\pm$ standard deviation, unless otherwise stated. Participants ran further after consuming the carbohydrate-protein drink, `r gtsummary::inline_text(drink$t_gt, carb_protein)` km, than the carbohydrate-only drink, `r gtsummary::inline_text(drink$t_gt, carb)` km.

##### Wilcoxon data set {-}

Once you learn you need Wilcoxon or the sign-test, show the median and IQR summary statistics instead.

```{r}
(drink$wilcoxon_gt <- drink$wilcoxon_dat %>% 
  gtsummary::tbl_summary()
)
```

> There were `r drink$wilcoxon_n` participants. Data are medians and IQR unless otherwise stated. Participants ran further after consuming the carbohydrate-protein drink, `r gtsummary::inline_text(drink$wilcoxon_gt, carb_protein)` km, than the carbohydrate-only drink, `r gtsummary::inline_text(drink$wilcoxon_gt, carb)` km.

##### Sign data set {-}

```{r}
(drink$sign_gt <- drink$sign_dat %>% 
  gtsummary::tbl_summary()
)
```

> There were `r drink$sign_n` participants. Data are median and IQR unless otherwise stated. Participants ran further after consuming the carbohydrate-protein drink, `r gtsummary::inline_text(drink$sign_gt, carb_protein)` km, than the carbohydrate-only drink, `r gtsummary::inline_text(drink$sign_gt, carb)` km.

#### Conditions {-}

The paired samples *t* test applies when the variable is continuous and partitioned into dependent pairs, Additionally, there are two conditions related to the data distribution. If either condition fails, consider the suggested work-around or move to the nonparametric alternatives.

1. **Outliers**. There should be no outliers in the differences because they exert a large influence on the mean and standard deviation. Test with a box plot. If there are outliers, you might be able to drop them if they do not affect the conclusion, or you can transform the data.
2. **Normality**. Differences should be *nearly* normally distributed ("nearly" because the *t*-test is robust to the normality assumption). This condition is especially important with small sample sizes. Test with Q-Q plots or the Shapiro-Wilk test for normality. If the data is very non-normal, you might be able to transform the data.

##### Outliers {-}

Assess outliers with a box plot. Box plot whiskers extend up to 1.5\*IQR from the upper and lower hinges and outliers (beyond the whiskers) are are plotted individually.

```{r echo=FALSE, fig.height=2.5, fig.width=7.5}
bind_rows(
  `t-test` = drink$t_dat,
  `Wilcoxon` = drink$wilcoxon_dat,
  `Sign` = drink$sign_dat,
  .id = "set"
) %>%
  ggplot(aes(x = "difference", y = diff)) +
  geom_boxplot(fill = "snow3", color = "snow4", alpha = 0.6, width = 0.25, 
               outlier.color = "goldenrod", outlier.size = 2) +
  theme_minimal() +
  facet_wrap(~fct_inorder(set)) +
  labs(title = "Boxplot of Difference", y = "Km", x = NULL)
```

>There were no outliers in the data, as assessed by inspection of a boxplot.

Had there been outliers, you might report

> *X* outliers were detected. Inspection of their values did not reveal them to be extreme and they were kept in the analysis.

If the outliers are data entry errors or measurement errors, fix them or discard them. If the outliers are genuine, you can try leaving them in or transforming the data.

##### Normality {-}

Assume the population is normally distributed if *n* $\ge$ 30. These data sets have *n* = `r drink$t_n` observations, so you cannot assume normality. Asses a Q-Q plot, skewness and kurtosis values, histogram, or Shapiro-Wilk test.

```{r fig.height=3.5, fig.width=7.5, echo=FALSE}
bind_rows(
  `t-test` = drink$t_dat,
  `Wilcoxon` = drink$wilcoxon_dat,
  `Sign` = drink$sign_dat,
  .id = "set"
) %>%
  ggplot(aes(sample = diff)) +
  stat_qq() +
  stat_qq_line(col = "goldenrod") +
  theme_minimal() + theme(legend.position = "top") +
  facet_wrap(~fct_inorder(set)) +
  labs(title = "Normal Q-Q Plot", color = NULL)
```

For the t-test data set, 

```{r}
(drink$t_shapiro <- shapiro.test(drink$t_dat$diff))
```

> The differences between the distance ran in the carbohydrate-only and carbohydrate-protein trial were normally distributed, as assessed by Shapiro-Wilk's test (*p* = `r drink$t_shapiro$p.value %>% scales::number(accuracy = 0.001)`).

For the Wilcoxon data set,

```{r}
(drink$wilcoxon_shapiro <- shapiro.test(drink$wilcoxon_dat$diff))
```

> The differences between the distance ran in the carbohydrate-only and carbohydrate-protein trial were *not* normally distributed, as assessed by Shapiro-Wilk's test (*p* = `r drink$wilcoxon_shapiro$p.value %>% scales::number(accuracy = 0.001)`).

For the sign-test data set,

```{r}
(drink$sign_shapiro <- shapiro.test(drink$sign_dat$diff))
```

> The differences between the distance ran in the carbohydrate-only and carbohydrate-protein trial were *not* normally distributed, as assessed by Shapiro-Wilk's test (*p* = `r drink$sign_shapiro$p.value %>% scales::number(accuracy = 0.001)`).

If the data is normally distributed, use the t-test. If not, you try transforming the dependent variable, or carrying on regardless since the *t*-test is fairly robust to deviations from normality.

##### Symmetric Distribution {-}

If the data passed the outliers test, but failed the normality test, as the Wilcoxon and sign test data sets above did, you will use the Wilcoxon signed-rank test or sign test. Now you need to test the distribution to determine which test. If the distribution is symmetric, use Wilcoxon; otherwise use the sign test.

```{r fig.height = 3.5, fig.width=7.5, echo=FALSE}
bind_rows(
  `Wilcoxon` = drink$wilcoxon_dat,
  `Sign` = drink$sign_dat,
  .id = "set"
) %>%
  mutate(diff_sign = case_when(diff < 0 ~ "-", diff > 0 ~ "+", TRUE ~ "0")) %>%
  ggplot(aes(x = diff, fill = fct_rev(diff_sign))) +
  geom_histogram(bins = 20, color = "snow4") +
  scale_fill_manual(values = list("-" = "firebrick", "+" = "steelblue", "0" = "grey")) +
  theme_bw() +
  theme(legend.position = "top") +
  labs(title = "Paired Samples Differences Distribution", fill = NULL, x = NULL, y = "n") +
  facet_wrap(~fct_rev(set))
```

For the Wilcoxon data set,

> The distribution of the differences between the carbohydrate-protein drink and the carbohydrate-only was symmetric, as assessed by visual inspection.

For the sign data set,

> The distribution of the differences between the carbohydrate-protein drink and the carbohydrate-only was *not* asymmetric, as assessed by visual inspection.

#### Test {-}

##### t-test {-}

```{r}
(drink$t_t <- t.test(x = drink$t_dat$carb_protein, y = drink$t_dat$carb, paired = TRUE)
)
```

> The carbohydrate-protein drink elicited an increase of `r scales::number(drink$t_t$estimate %>% as.numeric(), accuracy = 0.001)` (95% CI, `r drink$t_t$conf.int[1] %>% scales::number(accuracy = 0.001)` to `r drink$t_t$conf.int[2] %>% scales::number(accuracy = 0.001)`) km in the distance run in two hours compared to a carbohydrate-only drink.

The effect size, called Cohen's *d*, is the number of standard deviations the measured mean difference is from the hypothesized difference, $(\bar{d}-d_0) / s$, where $s$ is the sample standard deviation. .2 is small, .5 is medium, and .8 is large. This one is large.

```{r}
(drink$t_d <- effectsize::cohens_d(drink$t_dat$diff))
```

You are about to reject the null hypothesis. Construct a plot as a sanity check on your reasoning.

```{r echo=FALSE, warning=FALSE, fig.height=2.5, fig.width=6.5}
d_0 <- 0
d_bar <- drink$t_mean
se <- drink$t_sd / sqrt(drink$t_n)
ci_lwr <- d_0 - qt(.975, df = drink$t_n - 1) * se
ci_upr <- d_0 + qt(.975, df = drink$t_n - 1) * se
ci_range <- ci_upr - ci_lwr

data.frame(d = seq(ci_lwr - ci_range/2, ci_upr + ci_range/2, length = 100)) %>%
  mutate(t = (d - d_0) / se,
         prob = dt(x = t, df = drink$t_n - 1),
         lrr = if_else(d < ci_lwr, prob, NA_real_),
         urr = if_else(d > ci_upr, prob, NA_real_)) %>%
  ggplot(aes(x = d)) +
  geom_line(aes(y = prob)) +
  geom_area(aes(y = lrr), fill = "firebrick", alpha = 0.6) +
  geom_area(aes(y = urr), fill = "firebrick", alpha = 0.6) +
  geom_vline(aes(xintercept = d_0), size = 1) +
  geom_vline(aes(xintercept = d_bar), color = "goldenrod", size = 1, linetype = 2) +
  annotate("label", x = d_0, y = 0.25,  label = "d[0]", parse = TRUE,
           fill = "grey90", size = 3) +
  annotate("label", x = d_bar, y = 0.15, label = "~bar(d)", parse = TRUE,
           fill = "goldenrod", size = 3) +
  theme_minimal() +
  theme(legend.position="none",         axis.text.y = element_blank()) +
  labs(title = "Two-Tailed t-Test", x = expression(paste(delta)), y = "Probability")
```

Report the results.

> A paired-samples t-test was used to determine the effect of a new formula of sports drink on running performance. Instead of the regular, carbohydrate-only drink, the new sports drink contains a new carbohydrate-protein mixture. Twenty participants were recruited to the study who each performed two trials in which they had to run as far as possible in two hours on a treadmill. In one of the trials they drank the carbohydrate-only drink and in the other trial they drank the carbohydrate-protein drink. The order of the trials was counterbalanced and the distance they ran in both trials was recorded.
>
>Two outliers were detected that were more than 1.5 box-lengths from the edge of the box in a boxplot. Inspection of their values did not reveal them to be extreme and they were kept in the analysis. The assumption of normality was not violated, as assessed by Shapiro-Wilk's test (*p* = `r drink$t_shapiro$p.value %>% scales::number(accuracy = 0.001)`). 
>
>Data are mean $\pm$ standard deviation, unless otherwise stated. Participants ran further after consuming the carbohydrate-protein drink, `r gtsummary::inline_text(drink$t_gt, carb_protein)` km, than the carbohydrate-only drink, `r gtsummary::inline_text(drink$t_gt, carb)` km, a statistically significant increase of `r scales::number(drink$t_t$estimate %>% as.numeric(), accuracy = 0.001)` (95% CI, `r drink$t_t$conf.int[1] %>% scales::number(accuracy = 0.001)` to `r drink$t_t$conf.int[2] %>% scales::number(accuracy = 0.001)`) km, *t*(`r drink$t_t$parameter`) = `r drink$t_t$statistic %>% scales::number(accuracy = 0.001)`, *p* = `r drink$t_t$p.value %>% scales::number(accuracy = .0001)`, *d* = `r drink$t_d$Cohens_d %>% scales::number(accuracy = .01)`.

##### Wilcoxon Signed-Rank Test {-}

From the distribution plot, you can see that most of the signs were positive, and the largest absolute difference values were among the positives, so expect a pretty large test statistic.

```{r}
(drink$wilcoxon_test <- wilcox.test(drink$wilcoxon_dat$carb_protein,
                                    drink$wilcoxon_dat$carb, 
                                    paired = TRUE))
```
> The carbohydrate-protein drink elicited a statistically significant median increase in distance run in two hours compared to the carbohydrate-only drink, *W* = `r drink$wilcoxon_test$statistic %>% number(accuracy = 1)`, *p* = `r drink$wilcoxon_test$p.value %>% scales::number(accuracy = .001)`.

Report the results.

> A Wilcoxon signed-rank test was conducted to determine the effect of a new formula of sports drink on running performance. Instead of the regular, carbohydrate-only drink, the new sports drink contains a new carbohydrate-protein mixture. Twenty participants were recruited to the study who each performed two trials in which they had to run as far as possible in two hours on a treadmill. In one of the trials they drank the carbohydrate-only drink and in the other trial they drank the carbohydrate-protein drink. The order of the trials was counterbalanced and the distance they ran in both trials was recorded.
>
>The difference scores were approximately symmetrically distributed, as assessed by a histogram with superimposed normal curve.
>
>Data are medians unless otherwise stated. Of the 20 participants recruited to the study, the carbohydrate-protein drink elicited an increase in the distance run in 17 participants compared to the carbohydrate-only drink, whereas two participants saw no improvement and one participant did not run as far with the carbohydrate-protein drink. There was a statistically significant median increase in distance run (`r median(drink$sign_dat$diff) %>% scales::number(accuracy = .0001)` km) when subjects imbibed the carbohydrate-protein drink (`r median(drink$sign_dat$carb_protein) %>% scales::number(accuracy = .001)` km) compared to the carbohydrate-only drink (`r median(drink$sign_dat$carb) %>% scales::number(accuracy = .001)` km), *W* = `r drink$wilcoxon_test$statistic %>% number(accuracy = 1)`, *p* = `r drink$wilcoxon_test$p.value %>% scales::number(accuracy = .0001)`.

##### Sign Signed-Rank Test {-}

Conduct the exact sign test since the n-size is not so large that we need the normal approximation to the binomial. Notice `n` is the count of *non-zero* differences.

```{r}
(drink$sign_test <- binom.test(sum(drink$sign_dat$diff > 0), 
                               n = sum(drink$sign_dat$diff != 0)))
```

> The carbohydrate-protein drink elicited a statistically significant median increase in distance run (`r median(drink$sign_dat$diff) %>% scales::number(accuracy = .001)` km) compared to the carbohydrate-only drink, *p* = `r drink$sign_test$p.value %>% scales::number(accuracy = .001)`.

Report the results.

> An exact sign test was conducted to determine the effect of a new formula of sports drink on running performance. Instead of the regular, carbohydrate-only drink, the new sports drink contains a new carbohydrate-protein mixture. Twenty participants were recruited to the study who each performed two trials in which they had to run as far as possible in two hours on a treadmill. In one of the trials they drank the carbohydrate-only drink and in the other trial they drank the carbohydrate-protein drink. The order of the trials was counterbalanced and the distance they ran in both trials was recorded.
>
> An exact sign test was used to determine whether there was a statistically significant median difference between the distance ran when participants drank a carbohydrate-protein drink compared to a carbohydrate-only drink. Data are medians unless otherwise stated. Of the `r drink$sign_n` participants recruited to the study, the carbohydrate-protein drink elicited an increase in the distance run in `r sum(drink$sign_dat$diff > 0)` participants compared to the carbohydrate-only drink, whereas `r sum(drink$sign_dat$diff < 0)` participants did not run as far and `r sum(drink$sign_dat$diff == 0)` participant saw no improvement with the carbohydrate-protein drink. There was a statistically significant median increase in distance run (`r median(drink$sign_dat$diff) %>% scales::number(accuracy = .0001)` km) when subjects imbibed the carbohydrate-protein drink (`r median(drink$sign_dat$carb_protein) %>% scales::number(accuracy = .001)` km) compared to the carbohydrate-only drink (`r median(drink$sign_dat$carb) %>% scales::number(accuracy = .001)` km), *p* = `r drink$sign_test$p.value %>% scales::number(accuracy = .0001)`.

## One-way ANOVA {#onewayanova}

Analysis of variance (ANOVA) is a method to compare the mean values of a continuous variable between groups of a categorical independent variable. ANOVA is typically used to analyze the response to a manipulation of the independent variable in a controlled experiment, but it can also be used to analyze the difference in the observed value among groups in a non-experimental setting.^[These notes are gleaned from [PSU STAT-502](https://online.stat.psu.edu/stat502_fa21/) "Analysis of Variance and Design of Experiments", and [Laerd Statistics](https://statistics.laerd.com/premium/spss/owa/one-way-anova-in-spss.php).]

**How it Works**

ANOVA decomposes the variability around the overall mean $Y_{ij} - \bar{Y}_{..}$ into two parts: the variability of the factor level means around the overall mean $\bar{Y}_{i.} - \bar{Y}_{..}$ (*between-group* variability) plus the variability of the factor level values around their means $Y_{ij} - \bar{Y}_{i.}$ (*within-group* variability). In the table below, the ratio of the treatment mean square and the mean squared error, $F = \frac{MSR}{MSE}$, follows an *F* distribution with $k-1$ numerator dof and $N-k$ denominator dof. The more observation variance captured by the treatments, the larger is the between-group variability relative to the within-group variability, and thus the larger is $F$, and the less likely that the null hypothesis, $H_0 = \mu_1 = \mu_2 = \cdots = \mu_k$ is true.

```{r echo=FALSE}
tribble(
  ~Source, ~SS, ~df, ~MS, ~F,
  "$SSR$", "$\\sum{n_i(\\bar{Y}_{i.} - \\bar{Y}_{..})^2}$", "$k - 1$", "${SSR}/{(k - 1)}$", "${MSR}/{MSE}$", 
  "$SSE$", "$\\sum(Y_{ij} - \\bar{Y}_{i.})^2$", "$N - k$", "${SSE}/{(N - k)}$", "",
  "$SST$", "$\\sum(Y_{ij} - \\bar{Y}_{..})^2$", "$N - 1$", "", ""
) %>% 
  knitr::kable(format = "html", caption = "ANOVA Table") %>%
  kableExtra::kable_styling(full_width = TRUE) %>%
  kableExtra::row_spec(row = 0, align = "c")
```

<br>

**Assumptions**

The ANOVA test applies when the independent variable is categorical, and the dependent variable is continuous and independent within groups. Independence means the observations are from a random sample, or from an experiment using random assignment. Each group's size should be less than 10% of its population size. The groups must also be independent of each other (non-paired, and non-repeated measures). Additionally, there are three assumptions related to the distribution of the dependent variable. If any assumption fails, either try the work-around or revert to the nonparametric Kruskal-Wallis test (Chapter \@ref(kw)).

1. **No outliers**. There should be no significant outliers in the groups. Outliers exert a large influence on the mean and variance. Test with a box plot or residuals vs predicted plot. Work-arounds are dropping the outliers or transforming the dependent variable.
2. **Normality**.  The dependent variable should be nearly normally distributed. ANOVA is robust to this condition, but it important with small sample sizes. Test with the Q-Q plots or the Shapiro-Wilk test for normality. Work-around is transforming the dependent variable.
3. **Equal Variances**. The group variances should be roughly equal. This condition is especially important with differing sample sizes. Test with a box plot, residuals vs predicted plot, rule of thumb (see case study in Chapter \@ref(groupdiffscs3)), or one of the formal [homogeneity of variance](http://www.cookbook-r.com/Statistical_analysis/Homogeneity_of_variance/) tests such as Bartlett and Levene (be careful here because the formal tests can be overly sensitive, esp. Bartlett). Work-around is the Games-Howell post hoc test instead of the Tukey post hoc test.

**Post Hoc Tests**

If the ANOVA procedure rejects the null hypothesis, use a post hoc procedure to determine which groups differ. The Tukey test is the most common. The test compares the differences in means to Tukey's $w$, $w = q_\alpha(p, df_{Err}) \cdot s_\bar{Y}$ where $q_\alpha(p, df_{Err})$ is a lookup table value, and $s_\bar{Y} = \sqrt{MSE/r}$ and $r$ is the number of comparisons. Any difference in group means greater than Tukey's $w$ is statistically significant. The Tukey test is only valid with equal sample sizes. Otherwise, the TukeyCramer method calculates the standard deviation for each pairwise comparison separately.

There are other post hoc tests. **Fishers Protected Least Significant Difference** (LSD) test is an older approach and less commonly used today. The **Bonferroni** and **Scheffe** methods are used for general tests of contrasts, including combinations of groups. The Bonferroni method is better when the number of contrasts is about the same as the number of factor levels. The Scheffe method is better for testing all possible contrasts. **Dunnetts mean comparison** method is appropriate for comparisons of treatment levels against a control. 

**ANOVA and OLS**

ANOVA is related to linear regression. The regression model intercept is the overall mean and the coefficient estimators indirectly indicate the group means. The analysis of variance table in a regression model shows how much of the overall variance is explained by those coefficient estimators. It's the same thing.

## KruskalWallis Test {#kw}

The **Kruskal-Wallis H test**^[The Kruskal-Wallis H test is also called the one-way ANOVA on ranks] measures the difference of a continuous or ordinal dependent variable between groups of a categorical independent variable. It is a rank-based nonparametric alternative to the one-way ANOVA test. Use Kruskal-Wallis if the dependent variable fails ANOVA's normality or homogeneity conditions, or if it is ordinal.

**How it Works**

The Kruskal-Wallis H test ranks the dependent variable irrespective of its group. The test statistic is a function of the averaged square of the rank sum per group:

$$
H = \left[ \frac{12}{n(n+1)} \sum_{j} \frac{T_j^2}{n_j} \right] - 3(n + 1)
$$

where $T_j$ is the sum of the ranks of group _j_. The test statistic approximately follows a $\chi^2$ distribution with _k_  1 degrees of freedom, where _k_ is the number of groups of the independent variable. The null hypothesis is that the rank means are equal. If you reject the null hypothesis, run a post hoc test to determine which groups differ.

**Assumptions**

Kruskal-Wallis has no assumptions per se, but the test interpretation depends on the distribution of the dependent variable. If its distribution has a similar shape across the groups of the categorical independent variable, then Kruskal-Wallis is a test of differences in their medians. Otherwise, Kruskal-Wallis is a test of differences in their distributions.

## Case Study 3 {.unnumbered #groupdiffscs3}

This case study uses the [data set](https://statistics.laerd.com/premium/spss/owa/one-way-anova-in-spss-6.php) from Laerd Statistics for ANOVA.

```{r}
cs3 <- list()

# Data sets are the same, so just use one.
# cs3$kw_dat <- read.spss("./input/kruskal-wallis-h-test.sav", to.data.frame = TRUE)
# cs3$anova_dat <- read.spss("./input/one-way-anova.sav", to.data.frame = TRUE)
cs3$dat <- read.spss("./input/kruskal-wallis-h-test.sav", to.data.frame = TRUE)
```

A study tests whether physically active individuals are better able to cope with workplace stress. The study categorizes $n$ = `r nrow(cs3$dat)` participants by physical activity level ("Sedentary", "Low", "Moderate", and "High") and measures their ability to cope with workplace-related stress (CWWS) as the average score of a series of Likert items on a questionnaire (higher scores indicating a greater CWWS ability). The means plot^[Trying [APA style guidelines](https://apastyle.apa.org/style-grammar-guidelines/tables-figures/figures).] and summary table are an initial look at the data.

```{r}
cs3$dat %>%
  group_by(group) %>%
  summarize(
    .groups = "drop",
    mean_coping_stress = mean(coping_stress),
    cl_025 = mean_coping_stress + qnorm(.025) * sd(coping_stress) / sqrt(n()),
    cl_975 = mean_coping_stress + qnorm(.975) * sd(coping_stress) / sqrt(n()),
    n = n()
  ) %>%
  ggplot(aes(x = group, y = mean_coping_stress)) +
  geom_point(shape = 21, fill = "gray80", color = "black", size = 3) +
  geom_errorbar(aes(ymin = cl_025, ymax = cl_975, width = 0.1)) +
  geom_text(aes(y = 2, label = glue("n = {n}")), size = 3) +
  labs(title = "Distribution of CWWS by Physical Activity Level Group",
       x = NULL, y = "Score",
       caption = "Means plot with 95% CI")
```

```{r}
(cs3$gt <- cs3$dat %>% 
  tbl_summary(
    by = group, 
    label = list(coping_stress = "CWWR"),
    type = coping_stress ~ "continuous2",
    statistic = coping_stress ~ c("{median} ({p25}, {p75})", "{mean}, {sd}")
  ) %>% 
  add_n())
```

<br>

CWWS score increased from the sedentary (`r inline_text(cs3$gt, variable = coping_stress, level = "Mean, SD", column = "Sedentary", pattern = "*M* = {mean}, _SD_ = {sd}")`), to low (`r inline_text(cs3$gt, variable = coping_stress, level = "Mean, SD", column = "Low", pattern = "*M* = {mean}, _SD_ = {sd}")`), to moderate (`r inline_text(cs3$gt, variable = coping_stress, level = "Mean, SD", column = "Moderate", pattern = "*M* = {mean}, _SD_ = {sd}")`) to high (`r inline_text(cs3$gt, variable = coping_stress, level = "Mean, SD", column = "High", pattern = "*M* = {mean}, _SD_ = {sd}")`) physical activity groups, in that order. 

### Assumptions

Recall that the one-way ANOVA test is valid under three assumptions. One, there are no significant outliers that influence the group mean. Two, the dependent variable is at least _approximately_ (ANOVA is robust to this assumption) normally distributed for each group if the sample size is small (for large sample sizes the Central Limit Theorem shows normality is unnecessary). Three, the dependent variable should have equal variances across groups. ANOVA is only sensitive to this condition if the group sample sizes are not similar.

Kruskal-Wallis has no assumptions per se, but the interpretation of its results depend on the distribution of the dependent variable. If the distributions are similar, then the test results tell you whether the medians differ. Otherwise, the test results tell you whether the distributions differ.

Use a boxplot to assess outliers for ANOVA and the data distribution (if you revert to Kruskal-Wallis). Values greater than 1.5 IQR from the hinges (values beyond the whiskers) are outliers. Outliers might occur from data entry errors or measurement errors, so investigate and fix or throw them out. If the outlier is a genuinely extreme, you still have a couple options before reverting to Kruskal-Wallis. You can transform the dependent variable, but don't do this unless the data is also non-normal. Transforming the variable also has the downside of making interpretation more difficult. You can also leave the outlier(s) in if it doesn't affect the conclusion. There are no outliers here. 

```{r}
cs3$dat %>%
  ggplot(aes(x = group, y = coping_stress)) +
  geom_boxplot(outlier.color = "goldenrod", outlier.size = 2) +
  labs(title = "Boxplot of CWWR vs Group",
       y = "Score", x = "Group")
```

There is no accepted practice for determining whether distributions are similar. The boxplot reveals a wider range of values for "Low" group, but this is close enough to conclude the distributions are similar. 

You can assume the populations are normally distributed if $n_j >= 30$. Otherwise, try the Q-Q plot, or skewness and kurtosis values, or histograms. If you still don't feel confident about normality, run the Shapiro-Wilk test of normality or the Kolmogorov-Smirnov test. Definitely do not use Shapiro-Wilk for $n_j >= 30$ because it is too sensitive. The Normal Q-Q plot below looks good for all groups except perhaps the "Low" group. The Shapiro-Wilk test confirms this, with all *p*-values over .05. 

```{r}
cs3$dat %>% 
  ggplot(aes(sample = coping_stress)) +
  stat_qq() +
  stat_qq_line(col = "goldenrod") +
  facet_wrap(facets = vars(group)) +
  labs(title = "Q-Q Plot", x = "Theoretical", y = "Sample")
```

```{r}
with(cs3$dat, by(coping_stress, group, shapiro.test)) %>% 
  map(tidy) %>%
  bind_rows(.id = "group")
```

Had the data failed the normality test, you could probably carry on anyway since the test is fairly robust to deviations from normality, particularly if the sample sizes are nearly equal. You can also try transforming the dependent variable. Transformations will generally only work when the distribution of scores in all groups are the same shape. Otherwise, revert to the Kruskal-Wallis H test. 

ANOVA's equality of sample variances condition is less critical when sample sizes are similar among the groups (as they are here). A rule of thumb is that no group's standard deviation should be more than double that of any other. In this case, "Moderate" and "Low" are more than double "Sedentary".

```{r echo=FALSE}
cs3$dat %>% 
  group_by(group) %>% 
  summarize(.groups = "drop", n = n(), sd = sd(coping_stress)) %>%
  arrange(sd) %>%
  mutate(multiple = sd / first(sd))
```

There are two common tests, Bartlett and Levene^[NIST has a good write-up for [Bartlett](https://www.itl.nist.gov/div898/handbook/eda/section3/eda357.htm) and [Levene](https://www.itl.nist.gov/div898/handbook/eda/section3/eda35a.htm)]. Levene is less sensitive to departures from normality. Neither test rejects the null hypothesis of equality of variance here.

```{r collapse=TRUE}
(cs3$levene <- car::leveneTest(coping_stress ~ group, data = cs3$dat, center = "mean"))
(cs3$bartlet <- bartlett.test(coping_stress ~ group, data = cs3$dat))
```

The residuals vs fitted values plot is included in the set of diagnostic plots that are produced in the base R `plot.lm()` function. 
```{r}
aov(coping_stress ~ group, data = cs3$dat) %>% plot(which = 1)
```

Heterogeneity of variances is a common problem in ANOVA. The Box-Cox procedure can help find a good transformation to remove heterogeneity. `MASS::boxcox()` calculates a profile of log-likelihoods for a power transformation of the dependent variable $Y^\lambda$.

|$\lambda$ | $Y^\lambda$ | Transformation |
|---|---|---|
|2 | $Y^2$ | Square |
|1 | $Y^1$ | (no transformation) |
|.5 | $Y^{.5}$ | Square Root |
|0 | $\ln(Y)$ | Log |
|-.5 | $Y^{-.5}$ | Inverse Square Root |
|-1 | $Y^{-1}$ | Inverse|

The Box-Cox procedure does not recommend any particular transformation of the data in this case.

```{r message=FALSE}
MASS::boxcox(aov(coping_stress ~ group, data = cs3$dat), plotit = TRUE)
```

Had the data failed the homogeneity assumption, you could use a modified version of ANOVA called Welch's ANOVA and the Games-Howell post hoc test, or you could revert to the nonparametric Kruskal-Wallis test.

### ANOVA

If the dependent variable conforms to the three ANOVA assumptions of no outliers, normality, and homogeneity, then you can run a one-way ANOVA with `aov()`. If the dependent variable only violates the homegeneity assumption, you can run Welch's ANOVA with `oneway.test(..., var.equal = FALSE)`

```{r collapse=TRUE}
cs3$aov <- aov(coping_stress ~ group, data = cs3$dat)
(cs3$anova <- anova(cs3$aov))
(cs3$welch <- oneway.test(coping_stress ~ group, data = cs3$dat, var.equal = FALSE))
```

The ability to cope with workplace-related stress (CWWS score) was statistically significantly different for different levels of physical activity group, _F_(`r paste(cs3$anova$Df, collapse = ", ")`) = `r comma(cs3$anova$"F value"[1], .1)`, _p_ = `r comma(cs3$anova$"Pr(>F)"[1], .0001)`.

```{r}
tibble(
  f_stat = seq(0, 10, .01), 
  d_val = df(f_stat, 3, 14.574),
  p_f = pf(f_stat, cs3$anova$Df[1], cs3$anova$Df[2], lower.tail = FALSE),
  region = if_else(p_f < .05, "reject", "accept")
) %>%
  ggplot(aes(x = f_stat, y = d_val)) +
  geom_area(aes(fill = region), show.legend = FALSE) +
  geom_line() +
  geom_vline(xintercept = cs3$anova$"F value"[1], linetype = 2, color = "firebrick") +
  scale_fill_manual(values = c(reject = "firebrick", accept = "white")) +
  labs(
    title = glue::glue("F({paste(cs3$anova$Df, collapse = ', ')}) = ",
                       "{comma(cs3$anova$'F value'[1], .1)}, p = ",
                       "{comma(cs3$anova$'Pr(>F)'[1], .0001)}"),
    x = "F", y = "P(F)"
  )
```

The _F_ test does not indicate which populations cause the rejection of $H_0$. Conduct a Tukey post hoc test if you have no specific hypothesis about two groups differing or want to see all group differences.^[There are other options for post-hoc tests not discussed here: Fisher's Least Significant Difference (LSD), Bonferroni, Scheffe, and Dunnett.] If you want to compare two groups or set of groups, then use a custom contrast. Tukey is valid for _balanced_ designs. If you have different sample sizes per group, use the Tukey-Kramer post hoc test. If the dependent variable failed the homogeneity of variances assumption, you would run the Games-Howell post hoc test instead.

```{r collapse=TRUE}
(cs3$tukey <- TukeyHSD(cs3$aov))
(cs3$games_howell <- rstatix::games_howell_test(cs3$dat, coping_stress ~ group))
```

Tukey post hoc analysis revealed that the increase from sedentary to moderate (`r comma(cs3$tukey$group["Moderate-Sedentary","diff"], .01)`, 95% CI (`r comma(cs3$tukey$group["Moderate-Sedentary","lwr"], .01)` to `r comma(cs3$tukey$group["Moderate-Sedentary","upr"], .01)`)) was statistically significant (_p_ = `r comma(cs3$tukey$group["Moderate-Sedentary","p adj"], .001)`), as well as the increase from sedentary to high (`r comma(cs3$tukey$group["High-Sedentary","diff"], .01)`, 95% CI (`r comma(cs3$tukey$group["High-Sedentary","lwr"], .01)` to `r comma(cs3$tukey$group["High-Sedentary","upr"], .01)`)) was statistically significant (_p_ = `r comma(cs3$tukey$group["High-Sedentary","p adj"], .001)`), but no other group differences were statistically significant.

```{r}
cs3$tukey %>% 
  tidy() %>%
  ggplot(aes(y = contrast, x = estimate)) +
  geom_point(shape = 3) +
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high), width = .2) +
  geom_vline(aes(xintercept = 0), linetype = 2) +
  labs(x = NULL, y = NULL, title = "95% family-wise confidence level")
```

Games-Howell post hoc analysis revealed that the increase from sedentary to moderate (`r cs3$games_howell %>% filter(group1 == "Sedentary", group2 == "Moderate") %>% pull(estimate) %>% comma(.01)`, 95% CI (`r cs3$games_howell %>% filter(group1 == "Sedentary", group2 == "Moderate") %>% pull(conf.low) %>% comma(.01)` to `r cs3$games_howell %>% filter(group1 == "Sedentary", group2 == "Moderate") %>% pull(conf.high) %>% comma(.01)`)) was statistically significant (*p* =`r cs3$games_howell %>% filter(group1 == "Sedentary", group2 == "Moderate") %>% pull(p.adj) %>% comma(.001)`), as well as the increase from sedentary to high (`r cs3$games_howell %>% filter(group1 == "Sedentary", group2 == "High") %>% pull(estimate) %>% comma(.01)`, 95% CI (`r cs3$games_howell %>% filter(group1 == "Sedentary", group2 == "High") %>% pull(conf.low) %>% comma(.01)` to `r cs3$games_howell %>% filter(group1 == "Sedentary", group2 == "High") %>% pull(conf.high) %>% comma(.01)`, *p* = `r cs3$games_howell %>% filter(group1 == "Sedentary", group2 == "High") %>% pull(p.adj) %>% comma(.001)`).

If you have specific hypotheses about the differences between the groups of your independent variable, e.g., whether the mean CWWS differs between the "Low" and "Sedentary" groups, $H_0: \sum_i^K{c_i u_i} = 0$ where $c_i = (1, -1, 0, 0)$ or "Sedentary" and average of all others, $c_i = (1, -1/3, -1/3, -1/3)$, set up a contrast using the **multcomp** package. 

```{r collapse=TRUE}
cs3$glht_1 <- multcomp::glht(cs3$aov, linfct = multcomp::mcp(group = c(-1, 1, 0, 0)))
summary(cs3$glht_1)
cs3$glht_2 <- multcomp::glht(cs3$aov, linfct = multcomp::mcp(group = c(-1, 1/3, 1/3, 1/3)))
summary(cs3$glht_2)
```

There are three groups, so you need to adjust the *p*-value and 95% CI for them.

```{r collapse=TRUE}
confint(cs3$glht_1, level = 1-.05/3)
confint(cs3$glht_2, level = 1-.05/3)
```

There was no statistically significant increase in CWWS score from the sedentary group (`r inline_text(cs3$gt, variable = coping_stress, level = "Mean, SD", column = "Sedentary")`) to the group performing a low level of physical activity (`r inline_text(cs3$gt, variable = coping_stress, level = "Mean, SD", column = "Low")`), a mean increase of `r comma(cs3$glht_1$coef["groupLow"], .1)` (95% CI, `r confint(cs3$glht_1, level = 1-.05/3) %>% tidy() %>% pull(conf.low) %>% pluck(1) %>% comma(.01)`, `r confint(cs3$glht_1, level = 1-.05/3) %>% tidy() %>% pull(conf.high) %>% pluck(1) %>% comma(.01)`), *p* = `r comma(summary(cs3$glht_1)$test$pvalues[1], .001)`.

CWWS score was statistically significantly higher in the non-sedentary groups (M = `r cs3$dat %>% filter(group != "Sedentary") %>% summarize(M = mean(coping_stress)) %>% pull(M) %>% comma(.1)`) compared to the sedentary group (`r inline_text(cs3$gt, variable = coping_stress, level = "Mean, SD", column = "Sedentary")`), a mean increase of `r comma(cs3$glht_2$coef["groupLow"], .1)` (95% CI, `r confint(cs3$glht_2, level = 1-.05/3)$confint[2] %>% comma(.01)`, `r confint(cs3$glht_2, level = 1-.05/3)$confint[3] %>% comma(.01)`), *p* = `r comma(summary(cs3$glht_2)$test$pvalues[1], .001)`.

You may also want to report the $\omega^2$ effect size, 

$$\omega^2 = \frac{SSR - df_R \cdot MSE}{MSE + SST}$$

where SSR is the between groups sum of squares, `r comma(cs3$anova$"Sum Sq"[1], .1)`, MSE is the within groups mean square, `r comma(cs3$anova$"Mean Sq"[2], .01)`, and SST is the total sum of squares, `r comma(sum(cs3$anova$"Sum Sq"), .1)`.

```{r}
(cs3$anova_stats <- sjstats::anova_stats(cs3$aov))
```
$\omega^2$ estimates the population effect size. It $\omega^2$ ranges from -1 to +1. Here, $\omega^2$ is `r cs3$anova_stats$omegasq[[1]]`.

```{r}
comma(cs3$anova$`Sum Sq`[1], .1)
```

Alternatively, the partial eta squared statistic, $\eta^2$, measures the effect size in the sample. Here $\eta^2$ is `r cs3$anova_stats$partial.etasq[[1]]`.

Now you can report your results.

> A one-way ANOVA was conducted to determine if the ability to cope with workplace-related stress (CWWS score) was different for groups with different physical activity levels. Participants were classified into four groups: sedentary (*n* = `r inline_text(cs3$gt, variable = coping_stress, level = "Mean, SD", column = "Sedentary", pattern = "{N_obs}")`), low (*n* = `r inline_text(cs3$gt, variable = coping_stress, level = "Mean, SD", column = "Low", pattern = "{N_obs}")`), moderate (*n* = `r inline_text(cs3$gt, variable = coping_stress, level = "Mean, SD", column = "Moderate", pattern = "{N_obs}")`) and high levels of physical activity (*n* = `r inline_text(cs3$gt, variable = coping_stress, level = "Mean, SD", column = "High", pattern = "{N_obs}")`). There were no outliers, as assessed by boxplot; data was normally distributed for each group, as assessed by Shapiro-Wilk test (*p* > .05); and there was homogeneity of variances, as assessed by Levene's test of homogeneity of variances (*p* = `r comma(cs3$levene$"Pr(>F)"[1], .001)`). CWWS score was statistically significantly different between different physical activity groups, _F_(`r paste(cs3$anova$Df, collapse = ", ")`) = `r comma(cs3$anova$"F value"[1], .1)`, _p_ = `r comma(cs3$anova$"Pr(>F)"[1], .0001)`, $\omega^2$ = `r cs3$anova_stats$omegasq[[1]]`. CWWS score increased from the sedentary (`r inline_text(cs3$gt, variable = coping_stress, level = "Mean, SD", column = "Sedentary", pattern = "*M* = {mean}, _SD_ = {sd}")`), to low (`r inline_text(cs3$gt, variable = coping_stress, level = "Mean, SD", column = "Low", pattern = "*M* = {mean}, _SD_ = {sd}")`), to moderate (`r inline_text(cs3$gt, variable = coping_stress, level = "Mean, SD", column = "Moderate", pattern = "*M* = {mean}, _SD_ = {sd}")`) to high (`r inline_text(cs3$gt, variable = coping_stress, level = "Mean, SD", column = "High", pattern = "*M* = {mean}, _SD_ = {sd}")`) physical activity groups, in that order. Tukey post hoc analysis revealed that the mean increase from sedentary to moderate (`r comma(cs3$tukey$group["Moderate-Sedentary","diff"], .01)`, 95% CI [`r comma(cs3$tukey$group["Moderate-Sedentary","lwr"], .01)` to `r comma(cs3$tukey$group["Moderate-Sedentary","upr"], .01)`]) was statistically significant (_p_ = `r comma(cs3$tukey$group["Moderate-Sedentary","p adj"], .001)`), as well as the increase from sedentary to high (`r comma(cs3$tukey$group["High-Sedentary","diff"], .01)`, 95% CI [`r comma(cs3$tukey$group["High-Sedentary","lwr"], .01)` to `r comma(cs3$tukey$group["High-Sedentary","upr"], .01)`], _p_ = `r comma(cs3$tukey$group["High-Sedentary","p adj"], .001)`), but no other group differences were statistically significant.

Had the dependent variable failed the homogeneity of variances assumption, you would report the results from Welch's ANOVA,

> The ability to cope with workplace-related stress (CWWS score) was statistically significantly different for different levels of physical activity group, Welch's *F*(`r cs3$welch$parameter[1]`, `r comma(cs3$welch$parameter[2], .1)`) = `r comma(cs3$welch$statistic, .1)`, *p* < .0005.

and the Games-Howell post hoc test,

> Games-Howell post hoc analysis revealed that the increase from sedentary to moderate (`r cs3$games_howell %>% filter(group1 == "Sedentary", group2 == "Moderate") %>% pull(estimate) %>% comma(.01)`, 95% CI [`r cs3$games_howell %>% filter(group1 == "Sedentary", group2 == "Moderate") %>% pull(conf.low) %>% comma(.01)` to `r cs3$games_howell %>% filter(group1 == "Sedentary", group2 == "Moderate") %>% pull(conf.high) %>% comma(.01)`]) was statistically significant (*p* =`r cs3$games_howell %>% filter(group1 == "Sedentary", group2 == "Moderate") %>% pull(p.adj) %>% comma(.001)`), as well as the increase from sedentary to high (`r cs3$games_howell %>% filter(group1 == "Sedentary", group2 == "High") %>% pull(estimate) %>% comma(.01)`, 95% CI [`r cs3$games_howell %>% filter(group1 == "Sedentary", group2 == "High") %>% pull(conf.low) %>% comma(.01)` to `r cs3$games_howell %>% filter(group1 == "Sedentary", group2 == "High") %>% pull(conf.high) %>% comma(.01)`], *p* = `r cs3$games_howell %>% filter(group1 == "Sedentary", group2 == "High") %>% pull(p.adj) %>% comma(.001)`).

**Power Analysis**

If you run an ANOVA and do not reject the null hypothesis, you may want to run a power analysis to make sure the power of the test was not very low. Power is the ability to reject the null when the null is really false. Power is affected by sample size, effect size,  variability of the experiment, and the significance of the type 1 error. You typically want power to be at 80%, meaning 80% of the time your test rejects the null when it should, and 20% of the time your test does not reject the null when it should. 

```{r}
power.anova.test(
  groups = cs3$aov$rank,
  n = cs3$aov$model %>% count(group) %>% pull(n) %>% min(),
  between.var = cs3$dat %>% group_by(group) %>% 
    summarize(M = mean(coping_stress)) %>% pull(M) %>% var(),
  within.var= cs3$anova$`Mean Sq`[2],
  sig.level = 0.05
)
```

### Kruskal-Wallis Test

Run a Kruskal-Wallis H test with `kruskal.test()`. 

```{r}
(cs3$kruskal <- kruskal.test(coping_stress ~ group, data = cs3$dat))
```

The dependent variable has similarly shaped distributions for all groups of the independent variable, so you can conclude the median CWWS scores were statistically significantly different between groups, $\chi^2$(`r cs3$kruskal$parameter`) = `r comma(cs3$kruskal$statistic, .1)`, *p* = `r comma(cs3$kruskal$p.value, .0001)`. Otherwise you would conclude the _distributions_ differ. You rejected the null hypothesis, so continue on with a post hoc test to determine which medians (similar distributions) or mean ranks (dissimilar distributions) differ with the Dunn procedure using a Bonferroni correction for multiple comparisons.

```{r}
(cs3$dunn <- FSA::dunnTest(coping_stress ~ group, data = cs3$dat, method = "bonferroni"))
```

*P.adj* equals *P.unadj* multiplied by the number of comparisons (`r nrow(cs3$dunn$res)`). You could report the adjusted _p_ or the unadjusted _p_ with a note that you accepted statistical significance at the _p_ < .05 / `r nrow(cs3$dunn$res)` = `r comma(.05 / nrow(cs3$dunn$res), .0001)` level.

Now you can report your results.

> A Kruskal-Wallis test was conducted to determine if there were differences in CWWS scores between groups that differed in their level of physical activity: the "sedentary" (_n_ = `r inline_text(cs3$gt, variable = coping_stress, column = "Sedentary", level = "Mean, SD", pattern = "{N_obs}")`), "low" (n = `r inline_text(cs3$gt, variable = coping_stress, column = "Low", level = "Mean, SD", pattern = "{N_obs}")`), "moderate" (n = `r inline_text(cs3$gt, variable = coping_stress, column = "Moderate", level = "Mean, SD", pattern = "{N_obs}")`) and "high" (n = `r inline_text(cs3$gt, variable = coping_stress, column = "High", level = "Mean, SD", pattern = "{N_obs}")`) physical activity level groups. Distributions of CWWS scores were similar for all groups, as assessed by visual inspection of a boxplot. Median CWWS scores were statistically significantly different between the different levels of physical activity group, $\chi^2$(`r cs3$kruskal$parameter`) = `r comma(cs3$kruskal$statistic, .1)`, *p* = `r comma(cs3$kruskal$p.value, .0001)`. Subsequently, pairwise comparisons were performed using Dunn's (1964) procedure with a Bonferroni correction for multiple comparisons. Adjusted *p*-values are presented. This post hoc analysis revealed statistically significant differences in CWWS scores between the sedentary (Mdn = `r inline_text(cs3$gt, variable = coping_stress, column = "Sedentary", level = "Median (IQR)", pattern = "{median}")`) and moderate (Mdn = `r inline_text(cs3$gt, variable = coping_stress, column = "Moderate", level = "Median (IQR)", pattern = "{median}")`) (*p* = `r cs3$dunn$res %>% filter(Comparison == "Moderate - Sedentary") %>% pull(P.adj) %>% comma(.0001)`) and sedentary and high (Mdn = `r inline_text(cs3$gt, variable = coping_stress, column = "High", level = "Median (IQR)", pattern = "{median}")`) (*p* = `r cs3$dunn$res %>% filter(Comparison == "High - Sedentary") %>% pull(P.adj) %>% comma(.0001)`) physical activity groups, but not between the low physical activity group (Mdn = `r inline_text(cs3$gt, variable = coping_stress, column = "Low", level = "Median (IQR)", pattern = "{median}")`) or any other group combination.

Had the distributions been different, you would report "CWWS scores" instead of "Median CWWS scores" and report the mean ranks instead of Mdn. Unfortunately, you cannot retrieve those ranks from the test object, so you would have to calculate them yourself.

## Chi-Square Test of Homogeneity {#chisqhomogeneity}

The chi-square test of homogeneity tests whether frequency counts of the *R* levels of a categorical variable are distributed identically across the *C* populations. It tests whether observed joint frequency counts $O_{ij}$ differ from expected frequency counts $E_{ij}$ under the *independence model* (the model of independent explanatory variables, $\pi_{ij} = \pi_{i+} \pi_{+j}$. $H_0$ is $O_{ij} = E_{ij}$. The chi-square homogeneity test can be extended to cases where $I$ and/or $J$ is greater than 2.

There are two possible test statistics for this test, Pearson $X^2 = \sum \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$, and deviance $G^2 = 2 \sum_{ij} O_{ij} \log \left( \frac{O_{ij}}{E_{ij}} \right)$.

**Side note: z-Test of Two Proportions**

The z-test and chi-square test produce the same statistical significance result because they are algebraically identical.

The *z*-test uses the difference in sample proportions $\hat{d} = p_1 - p_2$ as an estimate of the difference in population proportions $\delta = \pi_1 - \pi_2$ to evaluate an hypothesized difference in population proportions $d_0 = \pi_0 - \pi_1$ and/or construct a $(1\alpha)\%$ confidence interval around $\hat{d}$ to estimate $\delta$ within a margin of error $\epsilon$.

The *z*-test applies when the central limit theorem conditions hold so that the normal distribution approximates the binomial distribution.

* the sample is independently drawn, meaning random assignment (experiments) or random sampling without replacement from $n < 10\%$ of the population (observational studies),
* there are at least $n_i p_i >= 5$ successes and $n_i (1 - p_i) >= 5$ failures for each group $i$,
* the sample sizes are both $n_i >= 30$, and 
* the probability of success for each group is not extreme, $0.2 < \pi_i < 0.8$.

If these conditions hold, the sampling distribution of $\delta$ is normally distributed around $\hat{d}$ with standard error $se_\hat{d} = \sqrt{\frac{p_1(1 - p_1)}{n_1} + \frac{p_2(1  p_2)}{n_2}}$. The measured values $\hat{d}$ and $se_\hat{d}$ approximate the population values $\delta$ and $se_\delta$. Define a $(1  \alpha)\%$ confidence interval as $\hat{d} \pm z_{\alpha / 2}se_\hat{d}$ or test the hypothesis of $d = d_0$ with test statistic $z = \frac{\hat{d}  d_0}{se_{d_0}}$ where $se_{d_0} = \sqrt{p^*(1 - p^*) \left(\frac{1}{n_1} + \frac{1}{n_2}\right)}$ and $p^*$ is the overall success probability.


## Fisher's Exact Test {#fisherexact}

Fisher's exact test is an "exact test" in that the p-value is calculated exactly from the hypergeometric distribution rather than relying on the approximation that the test statistic distribution approaches $\chi^2$ as $n \rightarrow \infty$.

The test is applicable in situations where

* the row totals $n_{i+}$ and the column totals $n_+j$ are fixed by study design (rarely applies), and
* the expected values of >20% of cells (at least 1 cell in a 2x2 table) have expected cell counts >5, and no expected cell count is <1.

The p-value from the test is computed as if the margins of the table are fixed. This leads under a null hypothesis of independence to a hypergeometric distribution of the numbers in the cells of the table ([Wikipedia](https://en.wikipedia.org/wiki/Fisher%27s_exact_test)). Fisher's exact test is useful for small *n*-size samples where the chi-squared distribution assumption of the chi-squared and G-test tests fails. Fisher's exact test is overly conservative (p values too high) for large *n*-sizes.

The Hypergeometric density function is
$$f_X(k|N, K, n) = \frac{{{K}\choose{k}}{{N-K}\choose{n-k}}}{{N}\choose{n}}.$$

The density is the exact hypergeometric probability of observing this particular arrangement of the data, assuming the given marginal totals, on the null hypothesis that the conditional probabilities are equal.

## Case Study 4 {-}

```{r include=FALSE}
ind_discrete <- list()

ind_discrete$chisq_dat <- read.spss(
  "./input/test-of-two-proportions-individual-scores.sav",
  to.data.frame = TRUE
)

ind_discrete$fisher_dat <- ind_discrete$chisq_dat[seq(2, 100, 2),]
```

The case study below uses a data set from [Laerd](https://statistics.laerd.com/) and a second modified version. The first data set passes the chi-square test of homogeneity requirements. The second (in parentheses), fails the n-sizes test. 

A researcher recruits `r nrow(ind_discrete$chisq_dat)` (`r nrow(ind_discrete$fisher_dat)`) patients who have a "high" classification of cholesterol and who currently have a poor lifestyle. The researcher randomly assigns `r ind_discrete$chisq_dat %>% filter(intervention == "Drug") %>% nrow()` (`r ind_discrete$fisher_dat %>% filter(intervention == "Drug") %>% nrow()`) of them to a drug intervention and `r ind_discrete$chisq_dat %>% filter(intervention == "Lifestyle") %>% nrow()` (`r ind_discrete$fisher_dat %>% filter(intervention == "Lifestyle") %>% nrow()`) to a lifestyle intervention. After six months, a doctor reclassifies the patients as either still having a "high" classification of cholesterol or now having a "normal" classification of cholesterol.

```{r fig.height=3.5, fig.width=7.5, echo=FALSE}
bind_rows(
  `Chi-sq` = ind_discrete$chisq_dat,
  `Fisher` = ind_discrete$fisher_dat,
  .id = "set"
) %>%
  count(set, intervention, risk_level) %>%
  group_by(set, intervention) %>%
  summarize(.groups = "drop",
            p = sum(if_else(risk_level == "Normal", n, as.integer(0))) / sum(n),
            n = sum(n),
            se = sqrt(p * (1 - p) / n),
            ci_lwr = p - qt(.975, n - 1) * se,
            ci_upr = p + qt(.975, n - 1) * se) %>%
  ggplot(aes(x = intervention)) +
  geom_col(aes(y = p), fill = "snow3", color = "snow4", width = 0.25) +
  geom_errorbar(aes(ymin = ci_lwr, ymax = ci_upr), color = "snow4", width = .125) +
  theme_minimal() +
  scale_y_continuous(labels = scales::percent) +
  facet_wrap(~set) +
  labs(x = NULL, y = NULL,
       title = "Cholesterol Classification Improvement from High to Normal",
       subtitle = "Two data sets with different n-sizes.",
       caption = "Error bars show 95% CI.")
```

The chi-sq data set has the following summary statistics.

```{r echo=FALSE}
x <- ind_discrete$chisq_dat %>%
  tabyl(intervention, risk_level) %>%
  adorn_totals(where = c("row", "col")) %>%
  adorn_percentages() %>% 
  adorn_pct_formatting(digits = 0) %>% 
  adorn_ns(position = "front")
  
ind_discrete$chisq_test <- ind_discrete$chisq_dat %>%
  tabyl(intervention, risk_level) %>%
  chisq.test()
y <- ind_discrete$chisq_test$expected %>%
  adorn_totals(where = c("row", "col")) %>%
  adorn_percentages() %>% 
  adorn_pct_formatting(digits = 0) %>% 
  adorn_ns(position = "front")

# bind_rows(list(Observed = x, Expected = y), .id = "Data") %>%
#   flextable() %>% 
#   set_caption("Chisq Data Set: Observed vs Expected") %>% 
#   autofit() %>%
#   flextable::border(i = 3, border.bottom = officer::fp_border()) %>%
#   flextable::merge_v(j = 1)
```

The Fisher data set has the following summary statistics.

```{r echo=FALSE}
x <- ind_discrete$fisher_dat %>%
  tabyl(intervention, risk_level) %>%
  adorn_totals(where = c("row", "col")) %>%
  adorn_percentages() %>% 
  adorn_pct_formatting(digits = 0) %>% 
  adorn_ns(position = "front")
  
ind_discrete$fisher_chisq_test <- ind_discrete$fisher_dat %>%
  tabyl(intervention, risk_level) %>%
  chisq.test()
y <- ind_discrete$fisher_chisq_test$expected %>%
  adorn_totals(where = c("row", "col")) %>%
  adorn_percentages() %>% 
  adorn_pct_formatting(digits = 0) %>% 
  adorn_ns(position = "front")

# bind_rows(list(Observed = x, Expected = y), .id = "Data") %>%
#   flextable() %>% 
#   set_caption("Fisher Data Set: Observed vs Expected") %>% 
#   autofit() %>%
#   flextable::border(i = 3, border.bottom = officer::fp_border()) %>%
#   flextable::merge_v(j = 1)
```

#### Conditions {-}

##### n-Size {-}

The chi-square test of homogeneity applies with the CLT conditions hold.

* the sample is independently drawn,
* there are at least 5 successes (Normal) and failures (High) for each group $i$,
* the sample sizes for both groups are >=30, and 
* the probability of success for each group is not extreme, $0.2 < \pi_i < 0.8$.

The conditions hold for the chi-sq data set, but not for the Fisher data set.

#### Test {-}

##### Chi-Square {-}

```{r}
(ind_discrete$chisq_test <- ind_discrete$chisq_dat %>%
  tabyl(intervention, risk_level) %>%
  chisq.test(correct = FALSE))
```

> `r nrow(ind_discrete$chisq_dat)` patients with a high cholesterol classification were randomly assigned to either a drug or lifestyle intervention, `r ind_discrete$chisq_dat %>% filter(intervention == "Drug") %>% nrow()` in each intervention. The test of two proportions used was the chi-square test of homogeneity. At the conclusion of the drug intervention, `r ind_discrete$chisq_dat %>% filter(intervention == "Drug" & risk_level == "Normal") %>% nrow()` patients (`r tabyl(ind_discrete$chisq_dat, intervention, risk_level) %>% adorn_percentages() %>% filter(intervention == "Drug") %>% pull(Normal) %>% scales::percent()`) had improved their cholesterol classification from high to normal compared to `r ind_discrete$chisq_dat %>% filter(intervention == "Lifestyle" & risk_level == "Normal") %>% nrow()` patients (`r tabyl(ind_discrete$chisq_dat, intervention, risk_level) %>% adorn_percentages() %>% filter(intervention == "Lifestyle") %>% pull(Normal) %>% scales::percent()`) in the lifestyle intervention, a difference in proportions of `r (tabyl(ind_discrete$chisq_dat, intervention, risk_level) %>% adorn_percentages() %>% filter(intervention == "Drug") %>% pull(Normal) - tabyl(ind_discrete$chisq_dat, intervention, risk_level) %>% adorn_percentages() %>% filter(intervention == "Lifestyle") %>% pull(Normal)) %>% scales::number(accuracy = .01)`, *p* = `r ind_discrete$chisq_test$p.value %>% scales::number(accuracy = .0001)`.

##### Fisher {-}

```{r}
(ind_discrete$fisher_test <- ind_discrete$fisher_dat %>%
  tabyl(intervention, risk_level) %>%
  fisher.test())
```

> `r nrow(ind_discrete$fisher_dat)` patients with a high cholesterol classification were randomly assigned to either a drug or lifestyle intervention, `r ind_discrete$fisher_dat %>% filter(intervention == "Drug") %>% nrow()` in each intervention. At the conclusion of the drug intervention, `r ind_discrete$fisher_dat %>% filter(intervention == "Drug" & risk_level == "Normal") %>% nrow()` patients (`r tabyl(ind_discrete$fisher_dat, intervention, risk_level) %>% adorn_percentages() %>% filter(intervention == "Drug") %>% pull(Normal) %>% scales::percent()`) had improved their cholesterol classification from high to normal compared to `r ind_discrete$fisher_dat %>% filter(intervention == "Lifestyle" & risk_level == "Normal") %>% nrow()` patients (`r tabyl(ind_discrete$fisher_dat, intervention, risk_level) %>% adorn_percentages() %>% filter(intervention == "Lifestyle") %>% pull(Normal) %>% scales::percent()`) in the lifestyle intervention. Due to small sample sizes, Fisher's exact test was run. There was a non-statistically significant difference in proportions of `r (tabyl(ind_discrete$fisher_dat, intervention, risk_level) %>% adorn_percentages() %>% filter(intervention == "Drug") %>% pull(Normal) - tabyl(ind_discrete$fisher_dat, intervention, risk_level) %>% adorn_percentages() %>% filter(intervention == "Lifestyle") %>% pull(Normal)) %>% scales::number(accuracy = .01)`, *p* = `r ind_discrete$fisher_test$p.value %>% scales::number(accuracy = .0001)`.

## Pairwise Prop Test {#pairwiseproptest}
```{r}
library(tidyverse)
M <- 3573
F <- 4177
dat <- tribble(
  ~gender, ~src, ~Y, ~N,
  "Male", "Indeed", 1699, M-1699,
  "Male", "LinkedIn", 1755, M-1755,
  "Male", "Google", 1578, M-1578,
  "Female", "Indeed", 2554, F-2554,
  "Female", "LinkedIn", 1914, F-1914,
  "Female", "Google", 1694, F-1694
)
prop.test(x = dat$Y, n = dat$Y + dat$N)
pairwise.prop.test(x = dat$Y, n = dat$Y + dat$N)
```

## McNemar's Test {#mcnemar}

This test applies when you have paired samples.

Wilcoxon Paired-Sample applies when the variable distributions are non-normally distributed and samples are paired.

### MANOVA

Multi-factor ANOVA (MANOVA) is a method to compare mean responses by treatment factor level of two or more treatments applied in combination. The null hypotheses are $H_0: \mu_{1.} = \mu_{2.} = \dots = \mu_{a.}$ for the $a$ levels of factor 1, $H_0: \mu_{.1} = \mu_{.2} = \dots = \mu_{.b}$ for the $b$ levels of factor 2, etc. for all the factors in the experiment, and $H_0: $ no interaction for all the factor interactions.

There are two equivalent ways to state the MANOVA model:

$$Y_{ijk} = \mu_{ij} + \epsilon_{ijk}$$

In this notation $Y_{ijk}$ refers to the $k^{th}$ observation in the $j^{th}$ level of factor two and the $i^{th}$ level of factor 1.  Potentially there could be additional factors.  This model formulation decomposes the response into a cell mean and an error term.  The second makes the factor effect more explicit and is thus more common:

$$Y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha\beta)_{ij} +  \epsilon_{ijk}$$

### Multiple Variance Comparison F Test


### Example
*A study investigates the relationship between oxygen update and two explanatory variables: smoking, and type of stress test.  A sample of* $n = 27$ *persons, 9 non-smoking, 9 moderately-smoking, and 9 heavy-smoking are divided into three stress tests, bicycle, treadmill, and steps and their oxygen uptake was measured.  Is oxygen uptake related to smoking status and type of stress test?  Is there an interaction effect between smoking status and type of stress test?*
```{r message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(nortest)  # for Anderson-Darling test
library(stats)  # for anova

smoker <- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 
            2, 2, 2, 2, 2, 2, 2, 2, 2, 
            3, 3, 3, 3, 3, 3, 3, 3, 3)
stress <- c(1, 1, 1, 2, 2, 2, 3, 3, 3,
            1, 1, 1, 2, 2, 2, 3, 3, 3,
            1, 1, 1, 2, 2, 2, 3, 3, 3)
oxytime <- c(12.8, 13.5, 11.2, 16.2, 18.1, 17.8, 22.6, 19.3, 18.9,
             10.9, 11.1, 9.8, 15.5, 13.8, 16.2, 20.1, 21.0, 15.9,
             8.7, 9.2, 7.5, 14.7, 13.2, 8.1, 16.2, 16.1, 17.8)
oxy <- data.frame(oxytime, smoker, stress)
oxy$smoker <- ordered(oxy$smoker,
                      levels = c(1, 2, 3),
                      labels = c("non-smoker", "moderate", "heavy"))
oxy$stress <- factor(oxy$stress,
                     labels = c("bicycle", "treadmill", "steps"))

lm_oxy <- lm(oxytime~smoker+stress+smoker*stress, data = oxy)
anova(lm_oxy)
```



[SFU BIO710](http://online.sfsu.edu/efc/classes/biol710/manova/MANOVAnewest.pdf)


<!--chapter:end:05-group-differences.Rmd-->

# Association

```{r include=FALSE}
library(tidyverse)
library(gtsummary)
library(foreign)
library(scales)
library(janitor)
library(flextable)
```

Tests of association assess the strength of association between two variables. There are many variations on this theme.

[Pearson's correlation](#pearson) assesses the strength of a linear relationship between two continuous variables. It applies when the relationship is linear with no outliers and the variables are bi-variate normal. There are two less restrictive alternatives, [Spearman's rho](#spearman) and [Kendall's tau](#kendall), that assess the strength and direction of association.

A two-way frequency table is a frequency table for *two* categorical variables. You usually construct a two-way table to test whether the frequency counts in one categorical variable differ from the other categorical variable using the [chi-square test for independence or G-test](#chisq-independence), or [Fisher's exact test](#fisher-exact).  If there is a significant difference (i.e., the variables are related), then describe the relationship with an analysis of the residuals, calculations of measures of association (difference in proportions, **Phi** and **Cramer's V**, **Relative risks**, or **Odds Ratio**), and partition tests. 

If one of the variables is bivariate categorical, use **point-biserial correlation**, a special case of Pearson's correlation. **Pearson's partial correlation** controls for one or more variables - linear regression?

If both variables are ordinal, use **Goodman and Kruskal's gamma**. **Somers' d** is an alternative if you want to distinguish between a dependent and independent variable (instead of linear regression?). The **Mantel-Haenszel** test of trend is used to determine whether there is a linear trend (i.e., a linear relationship/association) between two ordinal variables that are represented in a contingency table. The **Cochran-Armitage** test of trend is used to determine whether there is a linear trend (i.e., a linear relationship/association) between an ordinal independent variable and a dichotomous dependent variable.

**Goodman and Kruskal's ** is a nonparametric measure of the strength of association between two nominal variables where a distinction is made between a dependent and independent variable

**Loglinear analysis** is used to understand (and model) associations between two or more categorical variables (i.e., nominal or ordinal variables). However, loglinear analysis is usually employed when dealing with three or more categorical variables, as opposed to two variables, where a chi-square test for association is usually conducted instead. 

There are three common correlation tests for categorical variables^[See https://www.statology.org/correlation-between-categorical-variables/]: Tetrachoric correlation for binary categorical variables; polychoric correlation for ordinal categorical variables; and Cramers V for nominal categorical variables.

## Pearson's Correlation {#pearson}

The Pearson product-moment correlation measures the strength and direction of a linear relationship between two continuous variables, *x* and *y*. The Pearson correlation coefficient, *r*, ranges from -1 (perfect negative linear relationship) to +1 (perfect positive linear relationship). A value of 0 indicates no relationship between two variables.

$$r_{x,y} = \frac{\text{Cov}(x,y)}{\text{SD}(x) \text{SD}(y)}$$

The statistic can be used as an estimate of the population correlation, $\rho$, in a test of statistical significance from 0 (H0: $\rho$ = 0).

$$\rho_{X,Y} = \frac{\text{Cov}(X,Y)}{\text{SD}(X) \text{SD}(Y)}$$

As rule of thumb, $|r|$ <.1 is _no correlation_, [.1, .3) is _small_, [.3, .5) is _moderate_, and [.5, 1.0] is _strong_. The test statistic follows a *t*-distribution with *n*  2 degrees of freedom. 

$$t = r_{x,y} \sqrt{\frac{n - 2}{1 - r^2_{x,y}}}$$

## Spearman's Rho {#spearman}

Spearman's ranked correlation (Spearman's rho) is a measure of the strength and direction of a monotonic relationship between two variables that are at least ordinal. Spearman's correlation is a non-parametric alternative to Pearson when one or more of its conditions are violated. Unlike Pearson, the relationship need not be linear (it only needs to be monotonic), and has no outliers or bivariate normality conditions.

Spearman's correlation is Pearson's correlation applied to the *ranks* of variables (for ordinal variables, their value already is a rank). However, there is also a second definition that gives the same result, at least when there are no ties in the ranks:

$$\rho = 1 - \frac{6 \sum_i d^2_i}{n(n^2 - 1)}$$

where $d_i$ is the difference in ranks of observation $i$.

## Kendal's Tau {#kendall}

Kendal's tau is a second alternative to Pearson and is identical to Spearman's rho with regard to assumptions. Kendal's tau only differs from Spearman's rho in *how* it measures the relationship. Whereas Spearman measures the correlation of the ranks, Kendal's tau is a function of concordant (C), discordant (D) and tied (Tx and Ty) pairs of observations. Concordant means both X and Y in one observation of a pair are larger than in the other. Discordant means X is larger in one observation than the other while Y is smaller. Tied mean either both observations have the same X or both have the same Y.

$$\mathrm{Kendall's} \space \tau_b = \frac{C - D}{\sqrt{(C + D + T_x) \times (C + D + T_Y)}}$$

## Case Study: Pearson, Spearman, Kendall

```{r include=FALSE}
cs <- list()

# Two continuous (c) vars
cs$dat1 <- read.spss("./input/pearson-correlation.sav", to.data.frame = TRUE)
# Two continuous (c) vars, but violating normality/outlier assumption.
cs$dat1b <- read.spss("./input/spearmans-correlation.sav", to.data.frame = TRUE)
# Two ordinal (o) vars
cs$dat2 <- read.spss("./input/kendalls-tau-b.sav", to.data.frame = TRUE)
```

This case study works with two data sets. `dat1` is composed of two continuous variables; `dat2` is composed of two ordinal variables.  

A researcher investigates the relationship between `cholesterol` concentration and time spent watching TV, `time_tv`, in *n* = `r cs$dat1 %>% nrow()` otherwise healthy 45 to 65 year old men (`dat1`). This data set will meet the conditions for Pearson, so we try all three tests on it.

A researcher investigates the relationship between the level of agreement with the statement "*Taxes are too high*" (`tax_too_high`, four level ordinal) and participant `income` level (three level ordinal) (`dat2`). The ordinal variables rule out Pearson, leaving Spearman and Kendall.

#### Conditions {-}

Pearson's correlation applies when X and Y are continuous (interval or ratio) paired variables with 1) a linear relationship that 2) has no significant outliers, and 3) are bivariate normal. Spearman's rho and Kendall's tau only require that X and Y be at least ordinal with 1) a monotonically increasing or decreasing relationship.

1. **Linearity** and **Monotonicity**. A visual inspection of a scatterplot should find a linear relationship (Pearson) or monotonic relationship (Spearman and Kendall).

Pearson's correlation additionall requires

2. **No Outliers**. Identify outliers with the scatterplot.
3. **Normality**. Bivariate normality is difficult to assess. Instead, check that each variable is individually normally distributed. Use the Shapiro-Wilk test.

##### Linearity / Monotonicity {-}

Assess linearity and monotonicity with a scatter plot. `dat1` is plotted on the left in Figure \@ref(fig:ccscatter). A second version that fails the linearity test is shown to the right. If the linear relationship assumption fails, consider transforming the variable instead of reverting to Spearman or Kendall.

```{r ccscatter, echo=FALSE, fig.height=3.5, fig.width=7.5, fig.cap="The left scatter plot is `dat1`. It meets Pearson's linearity condition. A second version at right illustrates what a failure might look like."}
bind_rows(
  `Linear` = cs$dat1,
  `Non-Linear` = cs$dat1b,
  .id = "set"
) %>%
  ggplot(aes(x = time_tv, y = cholesterol)) +
  geom_point(color = "snow4", alpha = 0.6) +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE, color = "goldenrod", linetype = 2) +
  theme_minimal() +
  facet_wrap(vars(set)) +
  labs(
    title = "Scatter Plot of Cholesterol Concentration by Time Watching TV", 
    subtitle = "Two data sets, only one meeting all conditions for parametric test.",
    x = "Time (mins/d)",
    y = "Cholesterol (mmol/L)"
  )
```

The ordinal variable data set `dat2` is plotted in Figure \@ref(fig:ooscatter).

```{r ooscatter, echo=FALSE, fig.height=3.5, fig.width=5.5, fig.cap="`dat2` meets the mononicity assumption for Spearman's rho and Kendall's tau."}
cs$dat2 %>%
  count(income, tax_too_high) %>%
  ggplot(aes(x = income, y = tax_too_high, size = n)) +
  geom_point(color = "snow4", alpha = 0.6) +
  theme_minimal() +
  labs(
    title = "Scatter Plot of Tax Attitude by Income Level", 
    x = "Income Level",
    y = "Taxes too high?"
  )
```

##### No Outliers {-}

Pearson's correlation requires no outliers. Both plots in Figure \@ref(fig:ccscatter) are free of outliers. If there were outliers, check whether they are data entry errors or measurement errors and fix or discard them. If the outliers are genuine, leave them in if they do not affect the conclusion. You can also try tranforming the variable. Failing all that, revert to the Spearman's rho or Kendall's tau.

##### Bivariate Normality {-}

Bivariate normality is difficult to assess. If two variables are bivariate normal, they will each be individually normal as well. That's the best you can hope to check for. Use the Shapiro-Wilk test.

```{r collapse=TRUE}
shapiro.test(cs$dat1$time_tv)
shapiro.test(cs$dat1$cholesterol)
```

If a variable is not normally distributed, you can transform it, carry on regardless since the Pearson correlation is fairly robust to deviations from normality, or revert to Spearman and Kendall.

#### Test {-}

Calculate Pearson's correlation, Spearman's rho, or Kendall's tau. `dat` meets the assumptions for Pearson's correlation, but try Spearman's rho and Kendall's tau too, just to see how close they come to Pearson. `dat2` only meets the assumptions for Spearman and Kendall.

##### Pearson's Correlation {-}

`dat1` met the conditions for Pearson's correlation.

```{r}
(cs$cc_pearson <- 
  cor.test(cs$dat1$cholesterol, cs$dat1$time_tv, method = "pearson")
)
```

*r* = `r cs$cc_pearson$estimate %>% scales::number(accuracy = .01)` falls in the range of a "moderate" linear relationship. $r^2$ = `r scales::percent(cs$cc_pearson$estimate^2, accuracy = 1)` is the coefficient of determination. Interpret it as the percent of variability in one variable that is explained by the other. If you are not testing a hypothesis (HO: $\rho \ne 0$), you can report just report *r*. Otherwise, include the *p*-value. Report your results like this:

>A Pearson's product-moment correlation was run to assess the relationship between cholesterol concentration and daily time spent watching TV in males aged 45 to 65 years. One hundred participants were recruited.
>
>Preliminary analyses showed the relationship to be linear with both variables normally distributed, as assessed by Shapiro-Wilk's test (p > .05), and there were no outliers.
>
>There was a statistically significant, moderate positive correlation between daily time spent watching TV and cholesterol concentration, *r*(`r cs$cc_pearson$parameter`) = `r cs$cc_pearson$estimate %>% scales::number(accuracy = .01)`, p < .0005, with time spent watching TV explaining `r scales::percent(cs$cc_pearson$estimate^2, accuracy = 1)` of the variation in cholesterol concentration.

You wouldn't use Spearman's rho or Kendall's tau here since the more precise Pearson's correlation is available. But just out of curiosity, here are the correlations using those two measures.

```{r collapse=TRUE}
cor(cs$dat1$cholesterol, cs$dat1$time_tv, method = "kendall")
cor(cs$dat1$cholesterol, cs$dat1$time_tv, method = "spearman")
```

Both Kendall and Spearman produced more conservative estimates of the strength of the relationship - Kendall especially so.

You probably wouldn't use linear regression here either because it describes the linear relationship between a *response* variable and changes to an independent *explanatory* variable. Even though we are reluctant to interpret a regression model in terms of causality, that is what is implied the formulation `y ~ x` and independence assumption in of X. Nevertheless, correlation and regression are related. The slope term in a simple linear regression of the normalized values equals the Pearson correlation.

```{r}
lm(
  y ~ x, 
  data = cs$dat1 %>% mutate(y = scale(cholesterol), x = scale(time_tv))
)
```

##### Spearman's Rho {-}

Data set `dat2` did not meet the conditions for Pearson's correlation, so use Spearman's rho and/or Kendall's tau.

Start with Spearman's rho. Recall that Spearman's rho is just the Pearson correlation applied to the ranks. Recall also that the Pearson's correlation is just the covariance divided by the product of the standard deviations. You can quickly calculate it by hand.

```{r}
cov(rank(cs$dat2$income), rank(cs$dat2$tax_too_high)) /
  (sd(rank(cs$dat2$income)) * sd(rank(cs$dat2$tax_too_high)))
```

Use the function though. I don't get why `cor.test` requires `x` and `y` be numeric.

```{r}
(cs$spearman <- 
  cor.test(
    as.numeric(cs$dat2$tax_too_high), 
    as.numeric(cs$dat2$income), 
    method = "spearman")
)
```

Interpret the statistic using the same rule of thumb as for Pearson's correlation. A rho over .5 is a strong correlation.

>A Spearman's rank-order correlation was run to assess the relationship between income level and views towards income taxes in 24 participants. Preliminary analysis showed the relationship to be monotonic, as assessed by visual inspection of a scatterplot. There was a statistically significant, strong positive correlation between income level and views towards income taxes, $r_s$ = `r cs$spearman$estimate %>% scales::number(accuracy = .001)`, *p* = `r cs$spearman$p.value %>% scales::number(accuracy = .001)`.

##### Kendall's Tau {-}

Recall that Kendall's tau is a function of concordant (C), discordant (D) and tied (Tx and Ty) pairs of observations. The manual calculation is a little more involved. Here is the function instead.

```{r}
(cs$kendall <- 
  cor.test(
    as.numeric(cs$dat2$tax_too_high), 
    as.numeric(cs$dat2$income), 
    method = "kendall")
)
```

As with the case study on cholesterol and television, Kendall's tau was more conservative than Spearman's rho. $\tau_b$ = `r cs$kendall$estimate %>% scales::number(accuracy = .001)` is still in the "strong" range, though just barely.

>A Kendall's tau-b correlation was run to assess the relationship between income level and views towards income taxes amongst 24 participants. Preliminary analysis showed the relationship to be monotonic, as assessed by visual inspection of a scatterplot. There was a statistically significant, strong positive correlation between income level and the view that taxes were too high, $\tau_b$ = `r cs$kendall$estimate %>% scales::number(accuracy = .001)`, *p* = `r cs$kendall$p.value %>% scales::number(accuracy = .001)`.

## Chi-Square Test of Independence {#chisq-independence}

The **chi-square test of independence** tests whether two categorical variables are associated, or are instead *independent*^[The test is sometimes call the chi-square test for *association*.]. It tests whether the observed joint frequency counts $O_{ij}$ differ from expected frequency counts $E_{ij}$ under the *independence model* (the model of independent explanatory variables, $\pi_{ij} = \pi_{i+} \pi_{+j}$. The null hypothesis is $O_{ij} = E_{ij}$. The test assumes the two variables are independent^[Independence is usually true by assumption and/or construction. It can be violated by, for example, a sample that includes spouses where one spouse's status (e.g., preference for a vacation destination) may be related to the other spouse's status.] and that all cell counts are at least 5.

Choose from two test statistics, Pearson $X^2$ (and the continuity adjusted $X^2$), and deviance *G*. As $n \rightarrow \infty$ their sampling distributions approach $\chi^2(df)$ with degrees of freedom (df) equal to the saturated model df $I \times J - 1$ minus the independence model df $(I - 1) + (J - 1)$, which you can algebraically solve for $df = (I - 1)(J - 1)$.

The Pearson goodness-of-fit statistic is

$$X^2 = \sum \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$$

where $O_{ij}$ is the observed count, and $E_{ij}$ is the product of the row and column marginal probabilities. The deviance statistic is 

$$G = 2 \sum_{ij} O_{ij} \log \left( \frac{O_{ij}}{E_{ij}} \right)$$
 
$X^2$ and *G* increase with the disagreement between the saturated model proportions $p_{ij}$ and the independence model proportions $\pi_{ij}$.

## Fisher's Exact Test {#fisher-exact}

Whereas the chi-squared and *G*-test rely on the approximation that the test statistic distribution approaches $\chi^2$ as $n \rightarrow \infty$, Fisher's exact test is an "exact test" in that the *p*-value is calculated exactly from the [hypergeometric distribution](https://bookdown.org/mpfoley1973/probability/hypergeometric.html). Therefore Fisher's test *should* only apply only to 2 x 2 tables. For some reason, it doesn't.^[It doesn't apply to just 2 x 2, so I need to figure out why.]

The test assumes the row totals $n_{i+}$ and the column totals $n_{+j}$ are fixed by study design, and the expected values of at least 20% of cells in the table have expected cell count >5, and no expected cell count is 0.

The famous example of the Fisher exact test is the "Lady tea testing" example. A lady claims she can guess whether the milk was poured into the cup before or after the tea. The experiment consists of 8 cups, 4 with milk poured first, 4 with milk poured second. The lady guesses correctly in 6 of the 8 cups.

```{r}
(tea <- matrix(c(3, 1, 1, 3), nrow = 2, 
       dimnames = list(Guess = c("Milk", "Tea"),
                       Truth = c("Milk", "Tea"))))
```

This is a hypergeometric distribution question because you want to know the probability of 3 or 4 successes in a sample of 4. If $X = k$ is the count of successful events in a sample of size $n$ *without replacement* from a population of size $N$ containing $K$ successes, then $X$ is a random variable with a hypergeometric distribution

$$f_X(k|N, K, n) = \frac{{{K}\choose{k}}{{N-K}\choose{n-k}}}{{N}\choose{n}}.$$

The formula follows from the frequency table of the possible outcomes. ${K}\choose{k}$ is the number of ways to get *k* successes in *K* draws. ${N-K}\choose{n-k}$ is the number of ways to fail to get *k* successes in *K* draws. And the denominator ${N}\choose{n}$ is the total of all ways to succeed and fail.

```{r}
tibble::tribble(
  ~` `, ~Sampled, ~`Not Sampled`,  ~Total,
  "success", "k", "K-k", "K",
  "non-success", "n-k", "(N-K)-(n-k)", "N-K",
  "Total", "n", "N-n", "N"
) %>%
  flextable::flextable() %>%
  flextable::autofit()
```

Function `choose()` returns the binomial coefficient ${{n}\choose{k}} = \frac{n!}{k!(n-k)!}$. The probability of choosing correctly at least 3 times out of 4 is the number of combinations of *k* = 3 plus the number with *k* = 4 divided by the number of combinations of *any* outcome.

```{r}
k <- 3; n <- 4; K <- 4; N <- 8

choose(K, k) * choose(N-K, n-k) / choose(N, n) +
choose(K, k+1) * choose(N-K, n-(k+1)) / choose(N, n)
```

`phyper()` does this.^[k-1 because it returns the probability of >k and we want >=k).]

```{r}
(pi <- phyper(q = k-1, m = K, n = N-K, k = n, lower.tail = FALSE))
```

The *p*-value from Fisher's exact test is calculated this way.

```{r}
fisher.test(tea, alternative = "greater")
```

The odds ratio at the bottom is the odds of success divided by the odds of non-success. The sample odds ratio is (3/1) / (1/3) = `r (3/1) / (1/3)`, but the documentation for `fisher.test()` explains that this it calculates the *conditional maximum likelihood estimate*.^[I don't really know what that means. [Brownlee](https://machinelearningmastery.com/what-is-maximum-likelihood-estimation-in-machine-learning/) might help.]

## Case Study: Chi-Square, Fisher

```{r include=FALSE}
# Gender vs Competition
# https://statistics.laerd.com/premium/spss/or2x2/odds-ratio-2x2-in-spss.php
cs$or2x2 <- read.spss("./input/odds-ratio-2x2-individual-scores.sav", to.data.frame = TRUE)

# Same data, already summarized! Do no use this.
# https://statistics.laerd.com/premium/spss/cstfa/chi-square-test-for-association-in-spss.php
cs$cstfa <- read.spss("./input/chi-square-test-for-association-frequencies.sav", to.data.frame = TRUE)

# Gender vs Competition again, but this time with small cell counts
# https://statistics.laerd.com/premium/spss/fet2x2/fishers-exact-test-in-spss.php
cs$fet2x2 <- read.spss("./input/fishers-exact-test-2x2-individual-scores.sav", to.data.frame = TRUE)
```

A researcher investigates whether males and females enrolled in an Exercise Science degree course differ in the type of exercise then engage, competitive or non-competitive. They survey 25 males and 25 females. 

```{r}
cs$or2x2 %>% 
  gtsummary::tbl_cross(
    percent = "row",
    label = list(comp ~ "Competitive")
  ) %>%
  gtsummary::add_p()
```

#### Conditions {-}

Use the Chi-Square test of independence for nominal, independent variables. The test requires all cell counts to be greater than 5. If your data does not meet this condition, consider collapsing some levels. If you collapse all the way to a 2 x 2 cross-tabulation and still do not have at least 5 counts per cell, use Fisher's exact test.

Use Fisher's exact test for *dichotomous* nominal, independent variables. The test is valid for *cross-sectional* samples, but not for prospective or retrospective samples.

#### Test {-}

##### Chi-Square {-}

To see what's going on, here is chi-square test done by hand. The expected values are the joint probabilities from the independence model (e.g., $E(\mathrm{male}, {\mathrm{yes}}) = \pi_{\mathrm{male}} \times \pi_{\mathrm{yes}} \times n$).

```{r collapse=TRUE}
exer_table <- cs$or2x2 %>% table()
expected <- marginSums(exer_table, 1) %*% t(marginSums(exer_table, 2)) / sum(exer_table)
(X2 <- sum((exer_table - expected)^2 / expected))
(df <- (2 - 1) * (2 - 1))
pchisq(X2, df, lower.tail = FALSE)
```

And the *G* test.

```{r collapse=TRUE}
(G <- 2 * sum(exer_table * log(exer_table / expected)))
pchisq(G, df, lower.tail = FALSE)
```

The `chisq.test()` function applies the Yates continuity correction by default to correct for situations with small cell counts.  The Yates continuity correction  subtracts 0.5 from the $O_{ij} - E_{ij}$ differences.  Set `correct = FALSE` to suppress Yates. The Yates continuity correction only applies to 2 x 2 tables.

```{r}
(cs$or2x2_chisq.test <- chisq.test(exer_table, correct = FALSE))
```

Calculate the *G* test with `DescTools::GTest()`.

```{r}
(cs$or2x2_g.test <- DescTools::GTest(exer_table, correct = "none"))
```

As a side note, if the cell size >5 condition is violated, you can use Monte Carlo simulation.

```{r}
chisq.test(exer_table, correct = FALSE, simulate.p.value = TRUE)
```

##### Fisher {-}

The documentation for `fisher.test()` explains that the *p*-value is based on the the first element of the contingency table "*with non-centrality parameter given by the odds ratio.*" I don't understand the odds ratio business and cannot figure out how it is calculated. It says in the documentation for the `estimate` value that "*Note that the conditional Maximum Likelihood Estimate (MLE) rather than the unconditional MLE (the sample odds ratio) is used.*", so that may hold the answer.

At least the *p*-value I can calculate by hand.

```{r}
phyper(q = exer_table[1, 1] - 1,  # k minus 1
       m = sum(exer_table[1, ]),  # K
       n = sum(exer_table[2, ]),  # N - K
       k = sum(exer_table[, 1]),  # n
       lower.tail = FALSE) * 2
```

```{r}
(cs$or2x2_fisher.test <- fisher.test(exer_table))
```

##### Post-Test: Phi, Cramer's V {-}

The problem with the chi-square test for association is that it does not measure the *strength* of any association. Two measures of associate are often calculated after the chi-squared or Fisher tests.

Cramer's V is derived from the chi-square statistic. It restricts the statistic to a range of 0 to 1. ^[[This tutorial](https://methods.sagepub.com/base/download/DatasetStudentGuide/cramers-v-lfsp-2015-r) says both variables should have more than two levels, but doesn't explain why.] Values under .1 are considered poor evidence of association; >.2 are "moderately strong" evidence.

$$V = \sqrt{\frac{\chi^2 / n}{\mathrm{min}(I, J) - 1}}$$

```{r collapse=TRUE}
# by hand
sqrt(cs$or2x2_chisq.test$statistic / sum(exer_table) / (min(2, 2) - 1))

# from package
(cs$or2x2_v <- rcompanion::cramerV(exer_table))
```

The Phi Coefficient is defined

$$\Phi = \sqrt{\frac{AD-BC}{(A+B)(C+D)(A+C)(B+D)}}$$

where A, B, C, and D are the four values of the 2 x 2 contingency table. 

```{r}
matrix(c("A", "C", "B", "D"), nrow = 2)
```
Similar to a Pearson Correlation Coefficient, a Phi Coefficient takes on values between -1 and 1.

```{r collapse=TRUE}
# by hand
det(matrix(exer_table, nrow = 2)) / 
  sqrt(prod(c(marginSums(exer_table, 1), marginSums(exer_table, 2))))

# from package
(cs$or2x2_phi <- psych::phi(exer_table))
```
##### Reporting {-}

>A chi-square test for association was conducted between gender and preference for performing competitive sport. All expected cell frequencies were greater than five. There was a statistically significant association between gender and preference for performing competitive sport, ($X^2$(1) = `r number(cs$or2x2_chisq.test$statistic, .001)`, *p* = `r number(cs$or2x2_chisq.test$p.value, .001)`. There was a moderately strong association between gender and preference for performing competitive sport, *V* = `r number(cs$or2x2_v, .001)`.

> A Fisher's Exact test was conducted between gender and preference for performing competitive sport. There was a statistically significant association between gender and preference for performing competitive sport, *p* = `r number(cs$or2x2_fisher.test$p.value, .001)`.

## Kruskal's Lambda
## Relative Risk
## Odds Ratio
Calculate relative risks for *dichotomous* nominal, independent variables when one variable is independent, the other dependent. Relative risks are valid for *prospective* and *retrospective cohort designs*, *randomized controlled trials (RCTs)*, and some types of *cross-sectional studies*, but not with case-control studies.

Calculate odds ratios as an alternative to relative risk. It makes the same assumptions. Unlike relative risk, it is valid for *all* study designs.


```{r}
cs$or2
# Smoking vs Lung Cancer
# https://statistics.laerd.com/premium/spss/rr2x2/relative-risk-2x2-in-spss.php
cs$rr2x2 <- read.spss(
  "./input/relative-risk-2x2-individual-scores.sav", 
  to.data.frame = TRUE
)

```

## Partial Correlation

The _partial correlation_ of $\textbf{Y}_j$ and $\textbf{Y}_k$ is their correlation _conditioned on sampling from a sub-population of_ $X$. Express the sub-population as $\textbf{Z} = \begin{pmatrix}\textbf{X}_1 \\ \textbf{X}_2 \end{pmatrix}$ with partitioned mean vector $\boldsymbol{\bar{x}} = \begin{pmatrix}\boldsymbol{\bar{x}}_1 \\ \boldsymbol{\bar{x}}_2 \end{pmatrix}$. The variance-covariance matrix is complicated by the covariances between $\textbf{X}_1$ and $\textbf{X}_1$, so the partition is a _matrix of matrices_, $\boldsymbol{S} = \begin{pmatrix} \boldsymbol{S}_{11} & \boldsymbol{S}_{12} \\ \boldsymbol{S}_{21} & \boldsymbol{S}_{22} \end{pmatrix}$. $\boldsymbol{S}_{11}$ and $\boldsymbol{S}_{22}$ are the variance-covariance matrices of $\textbf{X}_1$ and $\textbf{X}_2$, and $\boldsymbol{S}_{12}$ and $\boldsymbol{S}_{21}$ are the (identical) variance-covariance matrices between the variables in $\textbf{X}_1$ and the variables in $\textbf{X}_2$.

Partitioning means you can express the mean, ($\textbf{M}$, and variance-covariance matrix, $\textbf{CV}$ of $Y$ _conditioned_ on $\textbf{X} = \textbf{x}_2$.

$$
\begin{equation}
\textbf{M} = \boldsymbol{\bar{x}}_1 + \boldsymbol{S}_{12} \boldsymbol{S}_{22}^{-1} (\textbf{x}_2 - \boldsymbol{\bar{x}}_2)
\end{equation}
$$

$$
\begin{equation}
\textbf{CV} = \boldsymbol{S}_{11} - \boldsymbol{S}_{12} \boldsymbol{S}_{22}^{-1} \boldsymbol{S}_{21}
\end{equation}
(\#eq:s06-cond-cov)
$$

To see what this means, consider the bi-variate case. Suppose height and weight are distributed $\mu = \begin{pmatrix} 175 \\ 71 \end{pmatrix}$ with variance-covariance matrix $\boldsymbol{\Sigma} = \begin{pmatrix} 550 & 40 \\ 40 & 8 \end{pmatrix}$. The mean of height, $Y$ given weight $\textbf{X} = \textbf{x}_2$ is $\textbf{M}_{\textbf{Y.x}} = 175 + \frac{40}{8} (x_2 - 71)$. The variance is $\textbf{v}_{\textbf{Y.x}} = 550 = \frac{40^2}{8}$ (a constant).

The conditional mean of $\textbf{Y}$ given $\textbf{X} = \textbf{x}$ is $\mu_{\textbf{Y.x}} = E(\textbf{Y|X=x})$. If the distribution is multivariate normal, then the conditional mean equals its overall mean plus an adjustment based how $\textbf{Y}$ varies with $\textbf{X}$ as a fraction of the variation of $\textbf{X}$ multiplied by the distance of $\textbf{X}$ from its mean.

$$
\begin{equation}
\mu_{\textbf{Y.x}} = E(\textbf{Y|X=x}) = \mu_\textbf{Y} + \boldsymbol{\Sigma}_\textbf{YX} \boldsymbol{\Sigma}_\textbf{X}^{-1} (\textbf{x} - \boldsymbol{\mu}_\textbf{X})
\end{equation}
(\#eq:s06-conditional-mean)
$$

Note how if $\textbf{x} = \boldsymbol{\mu}_\textbf{X}$ the conditional mean equals the overall mean. If the $\textbf{X}$'s are generally positively correlated with the $\textbf{Y}$'s, then $\textbf{x} > \boldsymbol{\mu}_\textbf{X}$ results in a positive adjustment, and vice-versa.

https://online.stat.psu.edu/stat505/lesson/6/6.1

The conditional variance is $\sigma_{\textbf{Y.x}}^2 =E\{ (\textbf{Y} - \boldsymbol{\mu}_{\textbf{Y.x}})^2 | \textbf{X = x} \}$. The conditional covariance between $Y_i$ and $Y_j$ is $\sigma_{i,j.\textbf{x}} =E\{ (Y_i - \mu_{Y_i.x})(Y_j - \mu_{Y_j.x}) | \textbf{X = x} \}$. The variances and covariances can be collected into a variance-covariance matrix.

$$
\boldsymbol{\Sigma}_{\textbf{Y}.\textbf{x}} = \begin{pmatrix} 
\sigma_{Y_{1}.\textbf{X}} & \sigma_{12.\textbf{X}} & \cdots & \sigma_{1p.\textbf{X}} \\
\sigma_{21.\textbf{X}} & \sigma_{Y_{2}.\textbf{X}} & \cdots & \sigma_{2p.\textbf{X}} \\
\vdots & \vdots & \ddots & \vdots \\
\sigma_{p1.\textbf{X}} & \sigma_{p2.\textbf{X}} & \cdots & \sigma_{Y_{p}.\textbf{X}} 
\end{pmatrix}
$$

Divide the covariance of two variables by the product of their standard deviations to get the _unconditional correlation_, $r_{jk} = \frac{s_{jk}}{s_j s_k}$. $r_{jk}$ estimates the population correlation, $\rho_{jk} = \frac{\sigma_{jk}}{\sigma_j \sigma_k}$. The _partial correlation_ is $\rho_{jk.\textbf{X}} = \frac{\sigma_{jk.\textbf{X}}}{\sigma_{Y_j.\textbf{X}} \sigma_{Y_k.\textbf{X}}}$

Test the null hypothesis that $H_0: \rho_{jk} = 0$ with a *t*-test where

$$
t = r_{jk}\sqrt{\frac{n-2}{1-r_{jk}^2}}
$$

is distributed with $n - 2$ degrees of freedom. The 95% CI is complicated by the [-1, 1] bounding of the data which creates skew. The 95% CI is formed from the _Fisher transformation_ of the data,

$$
z_{jk} = \frac{1}{2} \log \frac{1 + r_{jk}}{1 - r_{jk}}
$$

$z_{jk}$ is distributed normally with variance $\frac{1}{n - 3}$.

**Quick Example**. 

```{r collapse=TRUE}
# 37x5 data set from [PSU STAT 505](https://online.stat.psu.edu/stat505/lesson/4/4.7).
wechsler <- readr::read_fwf(
  file = "./input/wechsler.txt", 
  col_positions = readr::fwf_widths(
    c(2, 3, 3, 3, 3),
    col_names = c("ID", "Information", "Similarities", "Arithmetic", "PictureCompletion")
  ),
  show_col_types = FALSE
) 

# Correlation between Information and Similarities
cor(wechsler$Information, wechsler$Similarities)

# t.test with 95% CI.
cor.test(wechsler$Information, wechsler$Similarities)
```


The sum of the diagonal of any square matrix is called the _trace_. The trace of the covariance matrix is a single number that expresses the total dispersion of the data set.

$$\mathrm{Trace}(S) = \sum_p S_{pp}$$

The trace has the shortcoming of not taking the variable correlations into account. A data set can have a large trace, but really high correlations. Instead, the _generalized variance_ expresses total variation with the _determinant_.

$$|S| = \sum_{j = 1}^p (-1)^{j+1} b_{1j} |B_{1j}|$$

Build your intuition with the bivariate case. If variables $Y$ and $X$ have a multivariate normal distribution, the conditional distribution of $Y|X = x$ is normal with mean $\mu_Y + \frac{\sigma_{YX}}{\sigma_{XX}}(x - \mu_X)$ and variance $\sigma_{YY} - \frac{\sigma_{YX}^2}{\sigma_{XX}}$. For example, suppose $Y$ = height (in) and $X$ = weight (lbs) are distributed $\mu = \begin{pmatrix}175 \\ 71 \end{pmatrix}$ with variance-covariance matrix $\boldsymbol{\Sigma} = \begin{pmatrix} 550 & 40 \\ 40 & 8 \end{pmatrix}$. Conditioning on $X = x$, $\mu_{Y.x} = 175 + \frac{40}{8}(x - 71)$ and $\sigma_{Y.x} = 550 - \frac{40^2}{8}$.

Extend this to the multivariate case. Let $\textbf{Y}$ be a set of outcomes (e.g., blood pressure and cholesterol) and $\textbf{X}$ be a set of conditioning variables (e.g., age, height, and weight). The multivariate conditional mean, $\mu_{\textbf{Y}.\textbf{x}}$, is 

$$
\begin{equation}
E(\textbf{Y} | \textbf{X} = \textbf{x}) = \mu_{\textbf{Y}} + \boldsymbol{\Sigma}_{\textbf{YX}} \boldsymbol{\Sigma}_\textbf{X}^{-1}(\textbf{x} - \boldsymbol{\mu}_\textbf{X})
(\#eq:multivariate-conditional-mean)
\end{equation}
$$

The multivariate conditional variance-covariance matrix of $\textbf{Y}$, $\Sigma_{\textbf{Y}.\textbf{x}}$, is

$$
\begin{equation}
E(\textbf{Y} | \textbf{X} = \textbf{x}) = \mu_{\textbf{Y}} + \boldsymbol{\Sigma}_{\textbf{YX}} \boldsymbol{\Sigma}_\textbf{X}^{-1}(\textbf{x} - \boldsymbol{\mu}_\textbf{X})
(\#eq:multivariate-conditional-mean)
\end{equation}
$$

 the mean of a set of outcomes, $\textbf{Y}$ (e.g., blood pressure and cholesterol) conditioned on a set of attributes, $\textbf{X}$ (e.g., age, height, and weight). Denote it as . Denote the conditional variance as $\sigma_{\textbf{Y}.x}^2 = \text{var}(\textbf{Y}|\textbf{X}=\textbf{x})$. It is equal to conditional squared difference from the mean, $E\{(\textbf{Y} - \boldsymbol{\mu}_{\textbf{Y}.\textbf{x}})^2|\textbf{X} = \textbf{x}\}$. Any two variables, $Y_i$ and $Y_j$ have a conditional covariance $\sigma_{i,j.\textbf{x}} = \text{cov}(Y_i,Y_j|\textbf{X} = \textbf{x})$. It is equal to $E\{ (Y_i - \mu_{Y_{i.x}}) (Y_j - \mu_{Y_{j.x}}) | \textbf{X} = \textbf{x}\}$. The set of all covariances is a matrix.

<!--chapter:end:06-association.Rmd-->

# Matrix Algebra

```{r include=FALSE}
library(tidyverse)
library(glue)
library(scales)

theme_set(
  theme_light()
)
```

This section is a primer on basic matrix algebra and its application to multivariate statistics.^[Notes are primarily from [PSU STAT 505: Applied Multivariate Statistical Analysis](https://online.stat.psu.edu/stat505).] 

**An initial remark on these notes**: Matrix algebra treats data *transposed* to how it is actually stored. An $n$-row $\times$ $p$-column data set is represented in matrix $X$ as $n$ _columns_ and $p$ rows. Think of $X$ as a column vector of variables with each variable represented by a row vector of observations.

$$
X_i = \begin{pmatrix}X_{i1} \\ X_{i2} \\ \vdots \\ X_{ip} \end{pmatrix}
$$

So $X_{ij}$ refers to index $i$ of row vector $j$, opposite of how the data frame is organized. Observation $i$ is represented as $X_i$, but it is a _column_ vector of the matrix.

## Central Tendancy, Dispersion, and Association

The mean of variable $j$ is the average of row vector $X_j$, $\bar{x}_j = \frac{1}{n} \sum_{i = 1}^n X_{ij}$. $\bar{x}_j$ estimates the population mean, $\mu_j = E(X_j)$. The collection of means are a column vector.

$$\boldsymbol{\mu} = \begin{pmatrix} \mu_1 \\ \mu_2 \\ \cdots \\ \mu_p  \end{pmatrix}$$

The variance of variable $j$ is the average squared difference from the mean for row vector $X_j$, $s_j^2 = \frac{1}{n-1} \sum_{i=1}^n (X_{ij} - \bar{x}_j)^2$. It estimates the population variance, $\sigma_j^2 = E(X_j - \mu_j)^2$. Again, the collection is represented as a column vector,

$$\boldsymbol{\sigma}^2 = \begin{pmatrix} \sigma_1^2 \\ \sigma_2^2 \\ \cdots \\ \sigma_p^2  \end{pmatrix}$$

The covariance of variables $j$ and $k$ is the average product of differences from their respective means, $s_{jk} = \frac{1}{n-1} \sum_{i=1}^n (X_{ij} - \bar{x}_j) (X_{ik} - \bar{x}_k)$. It estimates the population covariance, $\sigma_{jk} = E\{ (X_{ij} - \mu_j) (X_{ik} - \mu_k)\}$. Notice how the covariance is positive if when one variable is larger than its mean, so is the other. A zero covariance implies the value of one variable tells you nothing about the other. It can be shown that the covariance is equivalently expressed as 

$$
s_{jk} = \frac{1}{n-1} \left[ \sum_{i=1}^n X_{ij}X_{ik} - \frac{\sum_{i = 1}^n X_{ij} \sum_{i = 1}^n X_{ik}}{n} \right]
$$

This is how it is actually calculated (see example below). The first term is dot product $X_j \cdot X_k$. The second term is the product of the averages. The generalization across the entire matrix is the _variance-covariance matrix_.

$$
\begin{align}
S &= \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{x}) (X_i - \bar{x})' \\
&= \frac{1}{n-1} \left[ \sum_{i=1}^n X_i X_i^{'} - \frac{\sum_{i = 1}^n X_i \sum_{i = 1}^n X_i}{n} \right]
\end{align}
$$

Divide the covariance of two variables by the product of their standard deviations to get the _correlation_.

$$
\rho_{jk} = \frac{\sigma_{jk}}{\sigma_j \sigma_k}
$$

The sum of the diagonal of any square matrix is called the _trace_. The trace of the covariance matrix is a single number that expresses the total dispersion of the data set.

$$\mathrm{Trace}(S) = \sum_p S_{pp}$$

The trace has the shortcoming of not taking the variable correlations into account. A data set can have a large trace, but really high correlations. Instead, the _generalized variance_ expresses total variation with the _determinant_.

$$|S| = \sum_{j = 1}^p (-1)^{j+1} b_{1j} |B_{1j}|$$

#### Example {-}

File nutrient.txt is from [PSU STAT 505](https://online.stat.psu.edu/stat505/lesson/1/1.4).

```{r}
nutrient <- readr::read_fwf(
  file = "./input/nutrient.txt", 
  col_positions = readr::fwf_widths(
    c(3, 8, 8, 8, 8, 8),
    col_names = c("ID", "Calcium", "Iron", "Protien", "Vitamin A", "Vitamin C")
  ),
  show_col_types = FALSE
) 
```

This is an `r nrow(nutrient)` $\times$ `r ncol(nutrient) - 1` data set.

```{r}
(nutrient_smry <- nutrient %>% pivot_longer(cols = -ID) %>%
  summarize(M = mean(value), SD = sd(value), .by = name))
```

Calculate the variance-covariance matrix with matrix algebra. Notice the transpose step, `t()`, to convert the $i \times j$ data set into the $j \times i$ matrix .

```{r}
X <- nutrient %>% select(-ID) %>% as.matrix() %>% t()

varcov_mtrx <- 1 / (nrow(nutrient) - 1) * 
    (X %*% t(X) - rowSums(X) %*% t(rowSums(X)) / nrow(nutrient))

round(varcov_mtrx, 0)
```

Divide by the product of the standard deviations to get the correlation matrix.

```{r}
sd_prd <- nutrient_smry$SD %*% t(nutrient_smry$SD)

corr_mtrx <- varcov_mtrx / sd_prd

round(corr_mtrx, 3)
```

That's what `cor()` does.

```{r}
nutrient %>% select(-ID) %>% cor() %>% round(digits = 3)
```

The _coefficient of determination_ is the square of the correlation coefficient. Interpret cell (1, 2) below as `r percent((corr_mtrx^2)[1, 2], .1)` of the variation in iron is explained by calcium intake, or vice-versa.

```{r}
r_sqr_mtrx <- corr_mtrx^2

round(r_sqr_mtrx, 3)
```

The total variation of the nutrient data set is the trace of the covariance-variance matrix.

```{r collapse=TRUE}
psych::tr(varcov_mtrx)

# Or just the sum of the variances
sum((nutrient_smry$SD)^2)
```

The generalized variance is the determinant.

```{r}
det(varcov_mtrx)
```

## Linear Transformations

Linear combinations can be expressed as the multiplication of a matrix by the transpose of a column vector, $\textbf{Y} = \textbf{c}^{'} \textbf{X}$. A trivial example can be taken from the `nutrient.txt` example data from the prior section. Vitamin A is measured in micrograms and vitamin C is measured in milligrams. The total intake in milligrams is measured by the transformation, $\textbf{Y} = .001 X_4 + X_5$.

The mean of linear combination is the linear combination of the means, $\bar{\textbf{y}} = \textbf{c}'\bar{x}$. $\bar{\textbf{y}}$ estimates the population mean, $\textbf{c}'\mathbf{\mu} = E(\textbf{Y})$.

$\text{Var}(Y) = \textbf{c}' \mathbf{\Sigma} \textbf{c}$ where $\mathbf{\Sigma}$ is the variance-covariance matrix.

Suppose you have two linear transformations, $\textbf{Y}_1 = \textbf{c}^{'} \textbf{X}$ and $\textbf{Y}_1 = \textbf{d}^{'} \textbf{X}$. Their covariance, $\sigma_{Y_1 Y_2}$, is $\text{Cov}(Y_1, Y_2) = \textbf{c}' \mathbf{\Sigma} \textbf{d}$. Their correlation is their covariance divided by the individual standard deviations, $\rho = \frac{\sigma_{Y_1 Y_2}}{\sigma_{Y_1}\sigma_{Y_2}}$.

#### Example {-}

Using file nutrient.txt from the prior section, if $Y = .001 X_4 + X5$, then the mean of $Y$ is 79.8.

```{r collapse=TRUE}
C <- c(0, 0, 0, .001, 1)
x_bar <- nutrient_smry$M

# Mean
t(C) %*% x_bar

# Variance
(VarY1 <- t(C) %*% varcov_mtrx %*% C)

# Covariance between Y1 = cX and Y2 = dX
d <- c(1, 1, 0, 0, 0)
(CovY1Y2 <- t(C) %*% varcov_mtrx %*% d)

# Correlation
VarY2 <- t(d) %*% varcov_mtrx %*% d
(CorY1Y2 <- CovY1Y2 / sqrt(VarY1 * VarY2))
```

## Multivariate Normal Distribution

The univariate normal distribution, $X \sim N(\mu, \sigma^2)$, is a function of the variable's mean and variance, $\phi(x) = \frac{1}{\sqrt{2 \pi \sigma^2}}\exp\{-\frac{1}{2\sigma^2} (x - \mu)^2\}$. The multivariate normal distribution is similar except that the mean is the mean vector and the variance is the variance-covariance matrix, $\textbf{X} \sim N(\mu, \Sigma)$. Notice the determinant $|\Sigma|$ and [matrix inverse](https://www.mathsisfun.com/algebra/matrix-inverse-row-operations-gauss-jordan.html) in the equation.

$$
\begin{equation}
\phi(\textbf{X}) = \left(\frac{1}{2 \pi}\right)^{p/2}|\Sigma|^{-1/2}\exp\{-\frac{1}{2} (\textbf{x} - \mathbf{\mu})'\Sigma^{-1}(\textbf{x} - \mathbf{\mu})\}
\end{equation}
(\#eq:multivariate-normal)
$$

If $p$ is 2, then you have a bi-variate normal distribution. The exponentiated term $(\textbf{x} - \mathbf{\mu})'\Sigma^{-1}(\textbf{x} - \mathbf{\mu})$ is called the squared _Mahalanobis distance_ between $x$ and $\mu$.

A linear transformation is distributed $\textbf{Y} \sim N(\textbf{c}'\mu, \textbf{c}'\Sigma \textbf{c})$. It's useful to note that each variable in the multivariate normal distribution is normal, as are subsets of variables, linear combinations, and conditional distributions.

For an intuitive understanding of the material, consider the bivariate case.

$$
\begin{pmatrix}X_1 \\ X_2 \end{pmatrix} \sim N \left[ \begin{pmatrix} \mu_1 \\ \mu_2 \end{pmatrix} \begin{pmatrix} \sigma_1^2 & \rho \sigma_1 \sigma_2 \\ \rho \sigma_1 \sigma_2 & \sigma_2^2 \end{pmatrix}\right]
$$

If $\rho$ is 0, then the bivariate normal density function is symmetric in all dimension. As $\rho \rightarrow 1$, the curve gets increasing skinny along the diagonal.

```{r}
x <- seq(-4, 4, .1)
y <- seq(-4, 4, .1)

z_values <- function(x, y, r = .8) {
  exp(-(x^2-2*r*x*y+y^2)/2/(1-r^2))/2/pi/sqrt(1-r^2)
}
```

<!-- The colons (::::, :::) create a style html within document -->
:::: {style="display: grid; grid-template-columns: 1fr 1fr; grid-column-gap: 10px;"}
::: {}
```{r}
# correlation is .3
z <- outer(x, y, z_values, .3)
persp(x, y, z)
```
:::
::: {}
```{r}
# correlation is .9
z <- outer(x, y, z_values, .9)
persp(x, y, z)
```
:::
::::

The squared Mahalanobis distance, $d^2 = (\textbf{x} - \mathbf{\mu})'\Sigma^{-1}(\textbf{x} - \mathbf{\mu})$, is the equation for a hyper-ellipse centered at $\mu$. In two dimensions, it looks like this:

```{r}
bvn_mtrx <- MASS::mvrnorm(
  n = 100,
  mu = c(10, 5),
  Sigma = matrix(c(10, 5, 2, 9), ncol = 2)
)
colnames(bvn_mtrx) <- c("x1", "x2")

d2 <- mahalanobis(bvn_mtrx, colMeans(bvn_mtrx), cov(bvn_mtrx))

bvn <- as_tibble(bvn_mtrx) %>% bind_cols(d2 = d2)

bvn %>% ggplot(aes(x = x1, y = x2, color = d2)) + 
  geom_point() +
  scale_color_continuous(type = "viridis")
```

$d^2$ has a chi-square distribution with $p$ degrees of freedom. The distribution can be used to evaluate whether a point is an outlier or whether the data is multivariate normal. A Q-Q plot shows the ordered Mahalanobis distances versus the quantiles for a sample of size $n$ from a chi-squared distribution with $p$ degrees of freedom.

```{r}
qqplot(dchisq(bvn$d2, 2), bvn$d2)
```

Describe the shape of the ellipse mathematically with eigenvalues and eigenvectors of the variance-covariance matrix.^[Eigenvalues and eigenvectors show up in confidence ellipses, PCA, and factor analysis.] A $p \times p$ matrix $\textbf{A}$ has $p$ _eigenvalues_, $[\lambda_1, .., \lambda_p]$, that solve the expression 

$$
\begin{equation}
|\textbf{A} - \lambda \textbf{I}| = 0.
(\#eq:eigenvalue)
\end{equation}
$$

Calculate $\lambda$ by taking the determinant and solving the resulting $p$-ordered polynomial. The result is $p$ solutions, not necessarily all unique. Plug the eigenvectors into the following equation and solve for the $p$ _eigenvectors_, $\textbf{e}$. The eigenvector solutions are generally not unique, so to obtain a unique solution, require that $\textbf{e}_j'\textbf{e}_j = 1$.

$$
\begin{equation}
(\textbf{A} - \lambda_j \textbf{I}) \textbf{e}_j = \textbf{0}
(\#eq:eigenvector)
\end{equation}
$$

The eigenvalues and eigenvectors define the shape and orientation of the $(1 - \alpha)\%$ _prediction ellipse_. The ellipse is centered on the means with axes pointing in the directions of the eigenvectors. The distance from the origin to the ellipse boundary is

$$
\begin{equation}
l_j = \sqrt{\lambda_j \chi_{p, \alpha}^2}
(\#eq:prediction-ellipse-dist)
\end{equation}
$$

```{r warning=FALSE}
dat <- tibble(X = runif(40, 5, 20))
dat$Y <- dat$X + rnorm(40, 0, 3)

mu_X <- mean(dat$X)
mu_Y <- mean(dat$Y)

dat %>%
  ggplot(aes(x = X, y = Y)) + 
  geom_point() + 
  stat_ellipse(type = "norm") +
  geom_segment(aes(x = mu_X, y = 0, xend = mu_X, yend = mu_Y), linetype = 2) +
  geom_segment(aes(x = 0, y = mu_Y, xend = mu_X, yend = mu_Y), linetype = 2) +
  annotate("text", x = 0, y = mu_Y*1.1, label = expression(paste(mu[Y])), parse = TRUE, hjust = 0) +
  annotate("text", x = mu_X*1.05, y = 0, label = expression(paste(mu[X])), parse = TRUE, hjust = 0) +
  geom_segment(x = mu_X, y = mu_Y, xend = 8.5, yend = 16, 
               arrow = arrow(length = unit(0.03, "npc")), color = "goldenrod") +
  geom_segment(x = mu_X, y = mu_Y, xend = 22.5, yend = 24.5, 
               arrow = arrow(length = unit(0.03, "npc")), color = "goldenrod") +
  annotate("text", x = 8, y = 14.5, label = expression(paste(lambda[Y])), 
           parse = TRUE, hjust = 0, color = "darkgoldenrod") +
  annotate("text", x = 20, y = 23.5, label = expression(paste(lambda[X])), 
           parse = TRUE, hjust = 0, color = "darkgoldenrod") +
  tune::coord_obs_pred()
```

The elliptical shape is due to the correlation in the data. In the two-dimensional diagram above, the ellipse would be a perfect circle if the covariances were zero. The eigenvalues would equal the variances, $\lambda = \sigma^2$, and the eigenvectors would be parallel to the coordinate axis, $\textbf{e} = \begin{pmatrix}1 & 0 \\ 0 & 1 \end{pmatrix}$. The ellipse flattens with increasing correlation.

#### Example {-}

File wechsler.txt is a 37x4 data set from [PSU STAT 505](https://online.stat.psu.edu/stat505/lesson/4/4.7) with variance-covariance matrix

```{r}
wechsler <- readr::read_fwf(
  file = "./input/wechsler.txt", 
  col_positions = readr::fwf_widths(
    c(2, 3, 3, 3, 3),
    col_names = c("ID", "Information", "Similarities", "Arithmetic", "PictureCompletion")
  ),
  show_col_types = FALSE
) 

(weschler_cov <- cov(wechsler[, -1]))
```

and the eigenvalues and eigenvectors

```{r}
(weschler_eigen <- eigen(weschler_cov))
```

Now consider the 95% prediction ellipse formed by the multivariate normal distribution whose variance-covariance matrix. The half-lengths of the ellipse axes are $l_j = \sqrt{\lambda_j \chi_{p, \alpha}^2}$ where $\chi_{4, .05}^2$ is `r comma(qchisq(.95, 4), .01)`. 

```{r}
(weschler_half_len <- (weschler_eigen$values * qchisq(.95, 4))^.5)
```

The eigenvectors are the directions of the axes. The first vector, (`r paste(comma(weschler_eigen$vectors[, 1], .001), collapse = ", ")`) has large values for the first three variables (Information, Similarities, and Arithmetic) and a small value for the fourth (PictureCompletion), so the vector points toward the first three. The second axis has a half-length that is about half the size of the first. It's directed mostly toward the third variable (Arithmetic) and decreasing for the second variable (Similarities). Overall, the ellipse has one long axis and three shorter axes.


```{r warning=FALSE, eval=FALSE}
mu_Information <- mean(wechsler$Information)

mu_Similarities <- mean(wechsler$Similarities)

ggplot(wechsler, aes(x = Information, y = Similarities)) + 
  geom_point() + 
  stat_ellipse(type = "norm") +
  geom_segment(aes(x = mu_Information, y = 0, xend = mu_Information, yend = mu_Similarities), linetype = 2) +
  geom_segment(aes(x = 0, y = mu_Similarities, xend = mu_Information, yend = mu_Similarities), linetype = 2) +
  annotate("text", x = 0, y = mu_Similarities*1.1, label = expression(paste(mu[2])), parse = TRUE, hjust = 0) +
  annotate("text", x = mu_Information*1.05, y = 0, label = expression(paste(mu[1])), parse = TRUE, hjust = 0) +
  geom_segment(x = mu_Information, y = mu_Similarities, xend = 10, yend=12.5, arrow = arrow(length = unit(0.03, "npc"))) +
  geom_segment(x = mu_Information, y = mu_Similarities, xend = 21, yend=17.5, arrow = arrow(length = unit(0.03, "npc"))) +
  annotate("text", x = 11, y = 12, label = expression(paste(lambda[2])), parse = TRUE, hjust = 0) +
  annotate("text", x = 19, y = 17, label = expression(paste(lambda[1])), parse = TRUE, hjust = 0) +
  tune::coord_obs_pred()


```



https://online.stat.psu.edu/stat505/lesson/4

<!--chapter:end:07-matrix-algebra.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:08-references.Rmd-->

