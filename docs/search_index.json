[["bayesian-statistics.html", "Chapter 3 Bayesian Statistics", " Chapter 3 Bayesian Statistics Bayesian inference estimates the probability, \\(\\theta\\), that an hypothesis is true. It differs from classical (aka, frequentist) inference in its insistence that all uncertainties be described by probabilities. Bayesian inference updates the prior probability distribution in light of new information. Bayesian inference differs from classical inference in three respects. Both methods assume a data generating mechanism expressed as a likelihood. However, classical inference assumes the nature of the mechanism is infinitely repeatable while Bayesian inference treats each event as unique. classical estimates a single population parameter (usually the mean), while Bayesian inference estimates a parameter distribution. Finally, the machinery of classical inference is maximizing likelihood and for Bayesian inference it is updating the prior distribution. Bayesian inference builds on Bayes’ Theorem, so let’s start there. "],["bayes-theorem.html", "3.1 Bayes’ Theorem", " 3.1 Bayes’ Theorem Bayes’ Theorem, is the inverse conditional probability, the probability of the condition given the observed outcome. It reorganizes the relationship between joint probability and conditional probability, \\(P(\\theta D) = P(\\theta|D)P(D) = P(D|\\theta)P(\\theta)\\), into \\[P(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)}.\\] In common English, the probability that \\(\\theta\\) is after observing \\(D\\) is equal to the probability of observing \\(D\\) when \\(\\theta\\) is true divided by the probability of observing \\(D\\) under any circumstance. \\(P(\\theta)\\) is the strength of your belief in \\(\\theta\\) prior to considering the data \\(D\\). \\(P(D|\\theta)\\) is the likelihood of observing \\(D\\) from a generative model with parameter \\(\\theta\\). Note that the likelihood is a probability density, and is not quite the same as probability. For a continuous variable, likelihoods can sum to greater than 1. E.g., dbinom(seq(1, 100, 1), 100, .5) sums to 1, but dnorm(seq(0,50,.001), 10, 10) sums to 841. \\(P(D)\\) is the likelihood of observing \\(D\\) from any prior. It is the marginal distribution, or prior predictive distribution of \\(D\\). The likelihood divided by the marginal distribution is the proportional adjustment made to the prior in light of the data. \\(P(\\theta|D)\\) is the strength of your belief in \\(\\theta\\) posterior to considering \\(D\\). Bayes’ Theorem is useful for evaluating medical tests. A test’s sensitivity is its probability of yielding a positive result \\(D\\) when condition \\(\\theta\\) exists. \\(P(D|\\theta)\\) is a test sensitivity, \\(\\mathrm{sens}\\). \\(P(\\theta)\\) is the probability of \\(\\theta\\) prior to the test (e.g., the general rate in society), \\(\\mathrm{prior}\\). The numerator of Bayes’ Theorem, the joint probability \\(P(D \\theta) = P(D|\\theta)P(\\theta)\\), is \\(\\mathrm{sens\\times prior}\\). A test’s specificity is the probability of observing negative test result \\(\\hat{D}\\) when the condition does not exist, \\(\\hat{\\theta}\\). The specificity is the compliment of a false positive test result, \\(P(\\hat{D} | \\hat{\\theta}) = 1 - P(D | \\hat{\\theta})\\). The denominator of Bayes’ Theorem is the overall probability of a positive test result, \\(P(D) = P(D|\\theta)P(\\theta) + P(D|\\hat\\theta)P(\\hat\\theta)\\) or in terms of sensitivity and specificity, \\(P(D) = \\mathrm{(sens \\times prior) + (1 - spec)(1 - prior)}\\). Example. Suppose E. Coli is typically present in 4.5% of samples, and an E. Coli screen has a sensitivity of 0.95 and a specificity of 0.99. Given a positive test result, what is the probability that E. Coli is actually present? \\[P(\\theta|D) = \\frac{.95\\cdot .045}{.95\\cdot .045 + (1 - .99)(1 - .045)} = \\frac{.04275}{.04275 + .00955} = \\frac{.04275}{.05230} = 81.7\\%.\\] The elements of Bayes’ Theorem come directly from the contingency table. The first row is the positive test result. The probability of E. Coli is the joint probability of E. coli and a positive test divided by the probability of a positive test E. Coli Safe Total + Test .95 * .045 = 0.04275 .01 * .955 = 0.00955 0.05230 - Test .05 * .045 = 0.00225 .99 * .955 = 0.94545 0.94770 Total 0.04500 0.95500 1.00000 "],["bayesian-inference.html", "3.2 Bayesian Inference", " 3.2 Bayesian Inference Bayesian inference extends the logic of Bayes’ Theorem by replacing the prior probability estimate that \\(\\theta\\) is true with a prior probability distribution that \\(\\theta\\) is true. Rather than saying, “I am x% certain \\(\\theta\\) is true,” you say “I believe the probability that \\(\\theta\\) is true is somewhere in a range that has maximum likelihood at x%”. \\[ f(\\theta | D) = \\frac{f(D|\\theta) f(\\theta)}{\\int_\\Theta f(D|\\theta) f(\\theta) d\\theta} \\] This formula expresses the posterior distribution of \\(\\theta\\) as a function of the prior distribution and new information. Let \\(\\Pi(\\theta)\\) be the prior probability function. \\(\\Pi(\\theta)\\) has a PMF or PDF \\(P(\\theta) = f(\\theta)\\), and a set of conditional distributions, \\(\\{f_\\theta(D) = f(D|\\theta), \\theta \\in \\Omega\\}\\), called the generative model. \\(f_\\theta(D)\\) is the likelihood of observing \\(D\\) given \\(\\theta\\). The numerator is the product of the likelihood and the PMF/PDF, \\(f_\\theta(D)P(\\theta)\\), the joint distribution of \\((D, \\theta)\\). The denominator is the marginal distribution, or prior predictive distribution, of \\(D\\): \\(m(D) = \\int_\\Omega f_\\theta(D)P(\\theta) d\\theta\\). For discrete cases, replace the integral with a sum, \\(m(D) = \\sum\\nolimits_\\Omega f_\\theta(D) P(\\theta)\\). The posterior probability distribution of \\(\\theta\\), conditioned on the observance of \\(D\\), \\(\\Pi(\\cdot|D)\\), is the joint density, \\(f_\\theta(D) P(\\theta),\\) divided by the the marginal density, \\(m(D)\\). \\[P(\\theta | D) = \\frac{f_\\theta(D) P(\\theta)}{m(D)}\\] The numerator makes the posterior proportional to the prior. The denominator is a normalizing constant that scales the likelihood into a proper density function (whose values sum to 1). It is easier to see how observed evidence shifts the probabilities of the priors into their posterior probabilities by working with discrete priors first. From there it is straight-forward to grasp the more abstract case of continuous prior and posterior distributions. "],["discrete-cases.html", "3.3 Discrete Cases", " 3.3 Discrete Cases Suppose you have a string of numbers \\([1,1,1,1,0,0,1,1,1,0]\\) (7 ones and 3 zeros) produced by a Bernoulli random number generator. What parameter \\(p\\) was used in the Bernoulli function?1 D &lt;- c(1, 1, 1, 1, 0, 0, 1, 1, 1, 0) Your best guess is \\(p = 0.7\\), but how confident are you? Posit eleven competing priors, \\(\\theta = [.0, .1, \\ldots, 1]\\) with equal prior probabilities, \\(P(\\theta) = [1/11, \\ldots]\\). theta &lt;- seq(0, 1, by = 0.1) prior &lt;- rep(1/11, 11) Using a Bernoulli generative model, the likelihood of observing 7 ones and 3 zeros are \\(P(D|\\theta) = \\theta^7 + (1-\\theta)^3.\\) likelihood &lt;- theta^7 * (1 - theta)^3 data.frame(theta, likelihood) %&gt;% ggplot() + geom_segment(aes(x = theta, xend = theta, y = 0, yend = likelihood), linetype = 2, color = &quot;steelblue&quot;) + geom_point(aes(x = theta, y = likelihood), color = &quot;steelblue&quot;, size = 3) + scale_x_continuous(breaks = theta) + theme_minimal() + theme(panel.grid.minor = element_blank()) + labs(title = expression(paste(&quot;Maximum likelihood of observing D is at &quot;, theta, &quot; = 0.7.&quot;)), x = expression(theta), y = expression(f[theta](D))) The posterior probability is the likelihood divided by the marginal probability of observing \\(D\\) multiplied by the prior, \\(P(\\theta|D) = \\frac{P(D|\\theta)}{P(D)}\\cdot P(\\theta).\\) In this case, the marginal probability is straight-forward to calculate: it is the sum-product of the priors and their associated likelihoods. posterior &lt;- likelihood / sum(likelihood * prior) * prior What would the posterior look like if we started with an educated guess on \\(P(\\theta)\\) that more heavily weights \\(\\theta = 0.7\\)? prior &lt;- c(.05, .05, .05, .05, .05, .10, .15, .20, .15, .10, .05) posterior &lt;- likelihood / sum(likelihood * prior) * prior What if we now employ a larger data set? To see, generate a sample of 100 Bernoulli(.7) observations. D100 &lt;- Rlab::rbern(100, p = 0.7) %&gt;% as.numeric() likelihood &lt;- theta^sum(D100) * (1 - theta)^(length(D100)-sum(D100)) posterior &lt;- likelihood / sum(likelihood * prior) * prior What would it look like if it also had more competing hypotheses, \\(\\theta \\in (0, .01, .02, \\ldots, 1)\\). theta &lt;- seq(0, 1, by = .01) prior &lt;- rep(1/100, 101) likelihood &lt;- theta^sum(D100) * (1 - theta)^(length(D100)-sum(D100)) posterior &lt;- likelihood / sum(likelihood * prior) * prior Example borrowed from chris’ sandbox↩︎ "],["continuous-cases.html", "3.4 Continuous Cases", " 3.4 Continuous Cases Continuing the example of inferring the parameter \\(p\\) used in the Bernoulli process, what if we considered all values between 0 and 1?2 When prior beliefs are best described in continuous distributions, express them using the beta, gamma, or normal distribution so that the posterior distributions are conjugates of the prior distributions with new parameter values. Otherwise, the marginal distribution is difficult to calculate. In this case, use the beta distribution, described by shape parameters, \\(\\alpha\\) and \\(\\beta\\). \\[P(\\theta|D,\\alpha,\\beta) = \\frac{f_\\theta(D) P(\\theta|\\alpha,\\beta)}{\\int_0^1 f_\\theta(D)P(\\theta|\\alpha, \\beta)d\\theta}\\] As with the discrete case, the numerator is the likelihood of observing \\(D\\) if \\(\\theta\\) is true multiplied by the prior probability, but now the prior is a Beta(\\(\\alpha\\), \\(\\beta\\)) distribution. The denominator, sometimes called the evidence, is the marginal probability of \\(D\\). The likelihood of observing \\(D\\) = \\(a\\) successes and \\(b\\) non-successes given a success probability of \\(p\\) = \\(\\theta\\) is \\[f_\\theta(D) = \\theta^a(1-\\theta)^b\\] The prior distribution is the probability density function of the beta distribution \\[P(\\theta|\\alpha,\\beta) = \\frac{1}{\\mathrm{B}(\\alpha, \\beta)}\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\\] where \\(\\mathrm{B}(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\\) is the beta function. The marginal distribution is \\[\\int_0^1 f_\\theta(D)P(\\theta|\\alpha, \\beta)d\\theta = \\frac{\\mathrm{B}(\\alpha + a, \\beta + b)}{\\mathrm{B}(\\alpha, \\beta)}\\] Putting this all together, the posterior distribution is \\[P(\\theta|D, \\alpha, \\beta) = \\frac{1}{\\mathrm{B}(\\alpha + a, \\beta + b)} \\theta^{\\alpha-1+a}(1-\\theta)^{\\beta-1+b}\\] The posterior equals the prior with shape parameters incremented by the observed counts, \\(a\\) and \\(b.\\) plot_bayes &lt;- function(alpha, beta, a, b) { prior_ev &lt;- (alpha / (alpha + beta)) %&gt;% round(2) posterior_ev &lt;- ((alpha + a) / (alpha + beta + a + b)) %&gt;% round(2) dat &lt;- data.frame(theta = seq(0, 1, by = .01)) %&gt;% mutate(prior = (1 / beta(alpha, beta)) * theta^(alpha-1) * (1-theta)^(beta-1), prior_ci = theta &gt; qbeta(.025, alpha, beta) &amp; theta &lt; qbeta(.975, alpha, beta), likelihood = theta^a * (1-theta)^b, posterior = (1 / beta(alpha + a, beta + b)) * theta^(alpha-1+a) * (1-theta)^(beta-1+b), posterior_ci = theta &gt; qbeta(.025, alpha + a, beta + b) &amp; theta &lt; qbeta(.975, alpha + a, beta + b)) p_prior &lt;- dat %&gt;% ggplot(aes(x = theta, y = prior)) + geom_line(color = &quot;steelblue&quot;) + geom_area(aes(alpha = prior_ci), fill = &quot;steelblue&quot;) + geom_vline(xintercept = prior_ev, color = &quot;steelblue&quot;) + scale_alpha_manual(values = c(.1, .5)) + theme_minimal() + theme(legend.position = &quot;none&quot;) + labs(x = NULL) p_likelihood &lt;- dat %&gt;% ggplot(aes(x = theta, y = likelihood)) + geom_line(color = &quot;steelblue&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) + labs(x = NULL) p_posterior &lt;- dat %&gt;% ggplot(aes(x = theta, y = posterior)) + geom_line(color = &quot;steelblue&quot;) + geom_area(aes(alpha = posterior_ci), fill = &quot;steelblue&quot;) + geom_vline(xintercept = posterior_ev, color = &quot;steelblue&quot;) + scale_alpha_manual(values = c(.1, .5)) + theme_minimal() + theme(legend.position = &quot;none&quot;) + labs(x = expression(theta)) out &lt;- p_prior / p_likelihood / p_posterior + plot_annotation( title = glue(&quot;Beta({alpha}, {beta}) prior with observed evidence a = {a} &quot;, &quot;and b = {b}&quot;), subtitle = &quot;with shaded 95% credible interval.&quot;, caption = glue(&quot;Prior expected value = {prior_ev}; Posterior expected &quot;, &quot;value = {posterior_ev}&quot;)) out } Suppose you claim complete ignorance and take a uniform Beta(1, 1) prior. Recall that you observed a = 7 ones and b = 3 zeros. The posterior expected value is still pretty close! plot_bayes(alpha = 10, beta = 10, a = 7, b = 3) Suppose you had prior reason to believe p = 0.7. You would model that as \\(\\alpha\\) = 7, \\(\\beta\\) = 3. The prior probability distribution would be \\(P(\\theta|\\alpha = 7,\\beta = 3) = \\frac{1}{\\mathrm{B}(7, 3)}\\theta^{7-1}(1-\\theta)^{3-1}\\). Then after observing a = 7 ones and b = 3 zeros, the posterior probability distribution would be \\(P(\\theta|\\alpha = 7+7,\\beta = 3+3) = \\frac{1}{\\mathrm{B}(7+7, 3+3)}\\theta^{7+7-1}(1-\\theta)^{3+3-1}\\). plot_bayes(alpha = 7, beta = 3, a = 7, b = 3) Chris’s Sandbox again.↩︎ "],["bayes-factors.html", "3.5 Bayes Factors", " 3.5 Bayes Factors The Bayes Factor (BF) is a measure of the relative evidence of one model over another. Take another look at Bayes’ formula: \\[P(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)}.\\] Suppose you want to compare how two models explain and observed data outcome, \\(D\\). Model \\(M_1:f_1(D|\\theta_1)\\) says the observed data \\(D\\) was produced by a generative model with pdf \\(f_1\\) parameterized by \\(\\theta_2\\). Model \\(M_2:f_2(D|\\theta_2)\\) says it was produced by a generative model with pdf \\(f_2\\) parameterized by \\(\\theta_2\\). In each model you specify a prior probability distribution for the parameter If you take the ratio of the posterior probabilities, the posterior odds, the \\(P(D)\\) terms cancel and you have \\[\\frac{P(\\theta_1|D)}{P(\\theta_2|D)} = \\frac{P(D|\\theta_1)}{P(D|\\theta_2)} \\cdot \\frac{P(\\theta_1)}{P(\\theta_2)}\\] The posterior odds equals the ratio of the likelihoods multiplied by the prior odds. That likelihood ratio is the Bayes Factor (BF). Rearranging, BF is the odds ratio of the posterior and prior odds. \\[BF = \\frac{P(D|\\theta_1)}{P(D|\\theta_2)} = \\mathrm{\\frac{Posterior Odds}{Prior Odds}}\\] Return to the example of observing \\(D\\) = 7 ones and 3 zeros. You can compare an hypothesized \\(\\theta\\) of .5 to a completely agnostic model where \\(\\theta\\) is uniform over [0, 1]. The likelihood of observing \\(D\\) when \\(\\theta\\) = .5 is \\(P(D|\\theta_1) = 5^7(1-.5)^3\\) = 0.117. The likelihood of observing \\(D\\) where \\(\\theta\\) is uniform on [0, 1] is \\(P(D|\\theta_2) = \\int_0^1 \\binom{10}{3}q^7(1-q)^3dq\\) .5^1 * .5^1 ## [1] 0.25 dbinom(1, 1, .5) ## [1] 0.5 dbinom(11, 11, .5) ## [1] 0.0004882812 beta(11, 11) ## [1] 2.577402e-07 with a uniform Beta(1, 1) prior (i.e., complete agnosticism). The Bayes factor at \\(\\theta\\) = .7 quantifies how much the odds of H0: \\(\\theta\\) = .7 over H1: \\(\\hat{\\theta}\\) = .7. prior &lt;- function(theta, alpha, beta) { (1 / beta(alpha, beta)) * theta^(alpha-1) * (1-theta)^(beta-1) } posterior &lt;- function(theta, alpha, beta, a, b) { (1 / beta(alpha + a, beta + b)) * theta^(alpha-1+a) * (1-theta)^(beta-1+b) } prior(.5, 115, 85) ## [1] 1.164377 posterior(.5, 1, 1, 10, 10) ## [1] 3.700138 posterior(.5, 1, 1, 10, 10) / prior(.5, 1, 1) ## [1] 3.700138 1 / beta(115, 85) ## [1] 4.677704e+59 # Posterior Distribution 1/beta(1+10, 1+10) * .5^(1-1+10) * (1-.5)^(1-1+10) ## [1] 3.700138 dbeta(.5, 11, 11) ## [1] 3.700138 # Prior Beta Distributions 1/beta(1, 1) * .5^(1-1) * (1-.5)^(1-1) ## [1] 1 dbeta(.5, 1, 1) ## [1] 1 dbeta(.5, 115, 85) ## [1] 1.164377 The Bayes factor measures how much your prior belief is altered by the evidence. It is the ratio of the likelihoods at some hypothesized value before and after observing the data. In this case, our confidence increased by a factor of… theta &lt;- 0.5 alpha &lt;- 1 beta &lt;- 1 a &lt;- 10 b &lt;- 10 (prior_likelihood &lt;- (1 / beta(alpha, beta)) * theta^(alpha-1) * (1-theta)^(beta-1)) ## [1] 1 (posterior_likelihood &lt;- (1 / beta(alpha + a, beta + b)) * theta^(alpha-1+a) * (1-theta)^(beta-1+b)) ## [1] 3.700138 (bayes_factor &lt;- posterior_likelihood / prior_likelihood) ## [1] 3.700138 # 3.7 on alpha = beta = 1 # 1.91 on alpha = beta = 4 "],["a-gentler-introduction.html", "3.6 A Gentler Introduction", " 3.6 A Gentler Introduction This section is my notes from DataCamp course Fundamentals of Bayesian Data Analysis in R. It is an intuitive approach to Bayesian inference. Suppose you purchase 100 ad impressions on a web site and receive 13 clicks. How would you describe the click rate? The Frequentist approach would be to construct a 95% CI around the click proportion. (ad_prop_test &lt;- prop.test(13, 100)) ## ## 1-sample proportions test with continuity correction ## ## data: 13 out of 100, null probability 0.5 ## X-squared = 53.29, df = 1, p-value = 2.878e-13 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.07376794 0.21560134 ## sample estimates: ## p ## 0.13 How might you model this using Bayesian reasoning? One way is to run 1,000 experiments that sample 100 ad impression events from an rbinom() generative model using a uniform prior distribution of 0-30% click probability. The resulting 1,000 row data set of click probabilities and sampled click counts forms a joint probability distribution. This method of Bayesian analysis is called rejection sampling because you sample across the whole parameter space, then condition on the observed evidence. df_sim &lt;- data.frame(click_prob = runif(1000, 0.0, 0.3)) df_sim$click_n &lt;- rbinom(1000, 100, df_sim$click_prob) Condition the joint probability distribution on the 13 observed clicks to update your prior. The quantile() function returns the median and the .025 and .975 percentile values - the credible interval. # median and credible interval (sim_ci &lt;- df_sim %&gt;% filter(click_n == 13) %&gt;% pull(click_prob) %&gt;% quantile(c(.025, .5, .975))) ## 2.5% 50% 97.5% ## 0.05435893 0.11514869 0.19132473 Your posterior click rate likelihood is 11.5% with 95 credible interval [5.4%, 19.1%]. Here is the density plot of the 26 simulations that produced the 13 clicks. The median and 95% credible interval are marked. That’s pretty close to the frequentist result! Instead of running 1,000 experiments with randomly selected click probabilities and randomly selected click counts based on those probabilities, you could define a discrete set of candidate click probabilities, e.g. values between 0 and 0.3 incremented by .01, and calculate the click probability density for the 100 ad impressions. This method of Bayesian analysis is called grid approximation. df_bayes &lt;- expand.grid( click_prob = seq(0, .3, by = .001), click_n = 0:100 ) %&gt;% mutate( prior = dunif(click_prob, min = 0, max = 0.3), likelihood = dbinom(click_n, 100, click_prob), probability = likelihood * prior / sum(likelihood * prior) ) Condition the joint probability distribution on the 13 observed clicks to update your prior. df_bayes_13 &lt;- df_bayes %&gt;% filter(click_n == 13) %&gt;% mutate(posterior = probability / sum(probability)) Instead of using the quantile() function on these values to measure the median and credible interval, resample the posterior probability to create a distribution. sampling_idx &lt;- sample( 1:nrow(df_bayes_13), size = 10000, replace = TRUE, prob = df_bayes_13$posterior ) sampling_vals &lt;- df_bayes_13[sampling_idx, ] (df_bayes_ci &lt;- quantile(sampling_vals$click_prob, c(.025, .5, .975))) ## 2.5% 50% 97.5% ## 0.077 0.135 0.210 You can use a Bayesian model to estimate multiple parameters. Suppose you want to predict the water temperature in a lake on Jun 1 based on 5 years of prior water temperatures. temp &lt;- c(19, 23, 20, 17, 23) You model the water temperature as a normal distribution, \\(\\mathrm{N}(\\mu, \\sigma^2)\\) with a prior distribution \\(\\mu = \\mathrm{N}(18, 5^2)\\) and \\(\\sigma = \\mathrm{unif}(0, 10)\\) based on past experience. Using the grid approximation approach, construct a grid of candidate \\(\\mu\\) values from 8 to 30 degrees incremented by .5 degrees, and candidate \\(\\sigma\\) values from .1 to 10 incremented by .1 - a 4,500 row data frame. mdl_grid &lt;- expand_grid(mu = seq(8, 30, by = 0.5), sigma = seq(.1, 10, by = 0.1)) For each combination of \\(\\mu\\) and \\(\\sigma\\), the prior probabilities are the densities from \\(\\mu = \\mathrm{N}(18, 5^2)\\) and \\(\\sigma = \\mathrm{unif}(0, 10)\\). The combined prior is their product. The likelihoods are the products of the probabilities of observing each temp given the candidate \\(\\mu\\) and \\(\\sigma\\) values. mdl_grid_2 &lt;- mdl_grid %&gt;% mutate( mu_prior = map_dbl(mu, ~dnorm(., mean = 18, sd = 5)), sigma_prior = map_dbl(sigma, ~dunif(., 0, 10)), prior = mu_prior * sigma_prior, # combined prior, likelihood = map2_dbl(mu, sigma, ~dnorm(temp, .x, .y) %&gt;% prod()), posterior = likelihood * prior / sum(likelihood * prior) ) Calculate a credible interval by drawing 10,000 samples from the grid with sampling probability equal to the calculated posterior probabilities. Use the quantile() function to estimate the median and .025 and .975 quantile values. sampling_idx &lt;- sample(1:nrow(mdl_grid), size = 10000, replace = TRUE, prob = mdl_grid$posterior) ## Warning: Unknown or uninitialised column: `posterior`. sampling_vals &lt;- mdl_grid[sampling_idx, c(&quot;mu&quot;, &quot;sigma&quot;)] mu_ci &lt;- quantile(sampling_vals$mu, c(.025, .5, .975)) sigma_ci &lt;- quantile(sampling_vals$sigma, c(.025, .5, .975)) ci &lt;- qnorm(c(.025, .5, .975), mean = mu_ci[2], sd = sigma_ci[2]) data.frame(temp = seq(0, 30, by = .1)) %&gt;% mutate(prob = map_dbl(temp, ~dnorm(., mean = ci[2], sd = sigma_ci[2])), ci = if_else(temp &gt;= ci[1] &amp; temp &lt;= ci[3], &quot;Y&quot;, &quot;N&quot;)) %&gt;% ggplot(aes(x = temp, y = prob)) + geom_area(aes(y = if_else(ci == &quot;N&quot;, prob, 0)), fill = &quot;firebrick&quot;, show.legend = FALSE) + geom_line() + geom_vline(xintercept = ci[2], linetype = 2) + theme_minimal() + scale_x_continuous(breaks = seq(0, 30, 5)) + theme(panel.grid.minor = element_blank()) + labs(title = &quot;Posterior temperature probability&quot;, subtitle = glue(&quot;mu = {ci[2] %&gt;% scales::number(accuracy = .1)}, 95%-CI (&quot;, &quot;{ci[1] %&gt;% scales::number(accuracy = .1)}, &quot;, &quot;{ci[3] %&gt;% scales::number(accuracy = .1)})&quot;)) What is the probability the temperature is at least 18? pred_temp &lt;- rnorm(1000, mean = sampling_vals$mu, sampling_vals$sigma) scales::percent(sum(pred_temp &gt;= 18) / length(pred_temp)) ## [1] &quot;54%&quot; "],["gamma-poisson.html", "3.7 Gamma Poisson", " 3.7 Gamma Poisson Given the following counts of weekday sandwich sales, what is the expected value? y &lt;- c(50, 65, 72, 63, 70) Count data have a Poisson distribution, \\(y_i|\\lambda \\sim Pois(\\lambda)\\), with expected value \\(\\lambda\\) and PMF \\(f(y_i | \\lambda) = e^{-\\lambda}\\frac{\\lambda^{y_i}}{y_i!}\\). Using Bayes’ Theorem, the posterior distribution of \\(\\lambda\\) given evidence \\(\\textbf{y}\\) is equal to the joint likelihood of \\(\\lambda\\) and \\(\\textbf{y}\\) divided by the likelihood of \\(\\textbf{y}\\). \\[ f(\\lambda |\\textbf{y}) = \\frac{f(\\mathbf{y}|\\lambda) f(\\lambda)}{\\int_\\Lambda f(\\mathbf{y}|\\lambda) f(\\lambda) d\\lambda} \\] The conditional likelihood, \\(f(\\textbf{y}|\\lambda)\\), is the sum-product of the Poisson distribution PMF. \\[ \\begin{align} f(\\textbf{y}|\\lambda) = f(y_i,\\ldots, y_n | \\lambda) &amp;= \\prod_i f(y_i | \\lambda) \\\\ &amp;= \\prod_i e^{-\\lambda}\\frac{\\lambda^{y_i}}{y_i!} \\end{align} \\] The prior distribution, \\(f(\\lambda)\\), should have only positive values. Model it with the gamma distribution, \\(\\lambda|a,b = \\mathrm{Gamma}(a,b)\\). \\[ f(\\lambda) = f(\\lambda | a,b) = \\frac{b^a \\lambda^{a-1} e^{-b\\lambda}}{\\Gamma(a)} \\] where \\(\\Gamma\\) is the gamma function3. Substituting into Bayes’ Theorem and simplifying, you have this nightmare: \\[ f(\\lambda |\\textbf{y}) = \\frac{\\lambda^{a + \\sum_i y_i-1}e^{-(b+n)\\lambda}}{\\int_0^\\infty \\lambda^{a + \\sum_i y_i-1}e^{-(b+n)\\lambda} d\\lambda} \\] However, there is good news. The integration in the denominator removes the dependence on \\(\\lambda\\), so \\(f(\\lambda |\\textbf{y}, a, b)\\) is proportional to the numerator up to a constant. \\[ f(\\lambda |\\textbf{y}) \\propto f(\\textbf{y} | \\lambda) f(\\lambda) \\] Since \\(f(\\lambda |\\textbf{y})\\) is a PMF, it integrates (sums) to 1 and you can always figure out the constant later. What makes this good news is that this has the form of the PDF of the gamma distribution. \\[ \\begin{equation} \\lambda | \\textbf{y}, a, b \\sim \\mathrm{Gamma}(a + \\sum_i y_i, b + n) \\tag{3.1} \\end{equation} \\] Equation (3.1) is the posterior distribution of \\(\\lambda\\). We combined a gamma prior with the Poisson likelihood of evidence, \\(\\textbf{y}\\), to produce a gamma posterior. We call priors that produce posteriors of the same form, conjugate priors for the Poisson likelihood. Conjugate priors are popular because of their computational convenience. Return to the sandwich sales data. We need values to plug into Equation (3.1). For the gamma distribution, \\(E(X) = a / b\\) and \\(\\mathrm{Var}(X) = a / b^2\\). You might guess from intuition that mean daily sandwich sales are 70 +/- 5. Interpreting +/- 5 as a 95% CI and using the rule of thumb that a 95% CI is 2 SD, \\(\\mathrm{Var} = (2.5)^2 = 6.25\\). Solve for \\(a = 784\\) and \\(b = 11.2\\). We also have \\(\\sum_i y_i = 320\\) and \\(n = 5\\). \\[ \\lambda | \\textbf{y}, a, b \\sim \\mathrm{Gamma}(784 + 320, 11.2 + 5) \\sim \\mathrm{Gamma}(1104, 16.2) \\] The posterior \\(E(y) = 1104 / 16.2 = 68.1\\) and \\(\\mathrm{Var}(y) = 1104 / 16.2^2 = 4.2\\). Use the gamma distribution function to get the posterior 95% credible interval. # Prior distribution qgamma(p = c(.025, .975), 784, 11.2) ## [1] 65.18520 74.98392 # Posterior distribution qgamma(p = c(.025, .975), 784 + 320, 11.2 + 5) ## [1] 64.18701 72.22621 Whereas the prior expected mean daily sandwich sales was 70 (95% CI: 65, 75), the posterior is 68 (95% CI: 64, 72). Compare this to classical statistics: \\(E(y) = \\bar{y} = 64\\), \\(SE = \\sqrt{\\bar{y} / n} = 3.6\\): # Classical estimate qnorm(p = c(.025, .975), 64, 3.6) ## [1] 56.94413 71.05587 You might think that the reasonable Bayesian outcome was predicated on good \\(a\\) and \\(b\\) priors, but no. Suppose \\(a = .01\\) and \\(b = .01\\). The posterior is still reasonable. # Conservative prior qgamma(p = c(.025, .975), .01 + 320, .01 + 5) ## [1] 57.06689 71.05964 tibble( lambda = seq(0, 80, .1), `Gamma(.01, .01)` = dgamma(lambda, .01, .01), `Gamma(.01 + 320, .01 + 5)` = dgamma(lambda, .01 + 320, .01 + 5), `Gamma(784, 11.2)` = dgamma(lambda, 784, 11.2), `Gamma(784 + 320, 11.2 + 5)` = dgamma(lambda, 784 + 320, 11.2 + 5) ) %&gt;% pivot_longer(-lambda) %&gt;% mutate(name = fct_inorder(name)) %&gt;% mutate(prior = if_else(str_detect(name, &quot;\\\\+&quot;), &quot;posterior&quot;, &quot;prior&quot;)) %&gt;% ggplot(aes(x = lambda, y = value, color = name, linetype = prior)) + geom_line(linewidth = 1) + scale_color_manual(values = c(rep(&quot;seagreen&quot;, 2), rep(&quot;firebrick&quot;, 2))) + labs(color = NULL, linetype = NULL, y = &quot;density&quot;, title = &quot;Posteriors from Conservative and Informed Priors.&quot;) The Bayesian posterior approaches the classical \\(\\bar{y}\\) with increasing sample size. \\[ E(\\lambda|\\textbf{y}, a, b) = \\frac{a + \\sum_i y_i}{b + n} = \\frac{a + n \\bar{y}}{b + n} \\] Taking the limit, \\(\\lim_{n \\rightarrow \\infty} E(\\lambda|\\textbf{y}, a, b) = \\bar{y}\\). The central credible interval is the standard Bayesian credible interval. But when the posterior distribution is not perfectly symmetric, the shortest credible interval capturing x% of the distribution might have different endpoints. Our example has a pretty symmetric distribution, but let’s calculate the highest density region (HDR) anyway. pp &lt;- seq(0.01, .99, by = .0001) x &lt;- map_dbl(pp, ~qgamma(., 784 + 320, 11.2 + 5)) hdrcde::hdr(x, prob = 95)$hdr ## [,1] [,2] ## 95% 64.40785 71.86582 The posterior predictive distribution of a predicted value, \\(\\tilde{y}\\) is \\[ f(\\tilde{y} | x) = \\int f(\\tilde{y}|\\lambda) f(\\lambda | \\textbf{y}) d\\lambda \\] Our sandwich example has a well defined functional solution: the expected value from \\(\\mathrm{Gamma}(1104, 16.2)\\) is \\(1104/16.2 = 68\\). Had we not known this, we could have simulated posterior values (Monte Carlo simulation) and calculated the mean and variance. The procedure is to take a random sample of perhaps 1,000 \\(\\lambda\\) values from the gamma posterior distribution, then for each \\(\\lambda\\) draw a single random \\(\\tilde{y}\\) from the Poisson distribution. a &lt;- 1104 b &lt;- 16.2 set.seed(1234) # random sample of lambdas, and a single random y_tilde for each lambda lambda_r &lt;- rgamma(1000, a, b) y_tilde &lt;- rpois(1000, lambda_r) # posterior predictive distribution mean(y_tilde) ## [1] 68.252 quantile(y_tilde, c(.025, .975)) ## 2.5% 97.5% ## 50.975 85.000 So on any given day, the predicted value of sandwich sales is 68.3 with 95% prediction interval 51.0, 85.0. The probability of exceeding 80 sandwiches, \\(P(\\tilde{y} &gt; 80 | \\textbf{y})\\), is mean(y_tilde &gt; 80) = 8.3%, and 99% of the time, sandwich sales will be less than quantile(y_tilde, .99) = 89. You can also predict individual weekdays. Suppose you take a \\(\\mathrm{Gamma}(700, 10)\\) distribution as your prior. day_tbl &lt;- tibble( dow = fct_inorder(c(&quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;)), d = c(50, 65, 72, 63, 70) ) %&gt;% mutate( post_a = 700 + d, post_b = 10 + 1, post_mean = post_a / post_b, post_lci = qgamma(.025, post_a, post_b), post_uci = qgamma(.975, post_a, post_b) ) day_tbl ## # A tibble: 5 × 7 ## dow d post_a post_b post_mean post_lci post_uci ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mon 50 750 11 68.2 63.4 73.1 ## 2 Tue 65 765 11 69.5 64.7 74.6 ## 3 Wed 72 772 11 70.2 65.3 75.2 ## 4 Thu 63 763 11 69.4 64.5 74.4 ## 5 Fri 70 770 11 70 65.1 75.0 What is the probability that Mon sales are less than Tue? set.seed(123) lambda_r_mon &lt;- rgamma(1000, 750, 11) lambda_r_tue &lt;- rgamma(1000, 765, 11) # posterior probability mean(lambda_r_mon &lt; lambda_r_tue) ## [1] 0.664 Which day of the week has the highest sandwich sales? set.seed(12345) lambda_r &lt;- tibble( r_mon = rgamma(1000, 750, 11), r_tue = rgamma(1000, 765, 11), r_wed = rgamma(1000, 772, 11), r_thu = rgamma(1000, 763, 11), r_fri = rgamma(1000, 770, 11), r_dow = pmap(list(r_mon, r_tue, r_wed, r_thu, r_fri), function(m, t, w, r, f) c(m, t, w, r, f)), max_dow_idx = map_dbl(r_dow, ~which.max(.)), max_dow = map_chr(max_dow_idx, ~c(&quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;)[.]) ) lambda_r %&gt;% janitor::tabyl(max_dow) ## max_dow n percent ## Fri 248 0.248 ## Mon 77 0.077 ## Thu 188 0.188 ## Tue 190 0.190 ## Wed 297 0.297 The gamma function is a generic function, just like sin, cos, etc., and is a kind of generalized factorial.↩︎ "],["normal.html", "3.8 Normal", " 3.8 Normal Suppose you have a sample, \\(\\textbf{y}\\), from a normally distribution population of unknown mean and precision, \\(\\mu\\) and \\(\\tau\\): \\(y_i|\\mu, \\tau \\sim N(\\mu, \\tau)\\).4 Assume a normal prior for \\(\\mu \\sim N(\\mu_0, \\tau_0)\\), and a gamma prior for \\(\\tau \\sim \\text{Gamma}(a, b)\\) since it takes only positive values. The PDF for \\(y_i\\) is \\(f(y_i | \\mu, \\tau) = \\frac{\\tau^{.5}}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{\\tau}{2} (y_i - \\mu)^2 \\right)\\). We’ll derive posterior distributions for \\(\\mu\\) and \\(\\tau\\) separately. Using Bayes’ Theorem, the posterior distribution of \\(\\mu|y\\) is the joint likelihood of \\(y\\) and \\(\\mu\\) divided by the likelihood of \\(y\\), \\[ f(\\mu|y) = \\frac{f(y|\\mu)f(\\mu)}{\\int_\\mu f(y|\\mu)f(\\mu)d\\mu} \\] The conditional likelihood, \\(f(\\mu|y)\\), is the sum-product of the normal distribution PDF. We can take \\(\\tau\\) as given initially. \\[ \\begin{align} f(y|\\mu) &amp;= \\prod_i \\frac{\\tau^{(1/2)}}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{\\tau}{2} (y_i - \\mu)^2 \\right) \\\\ &amp;\\propto \\prod_i \\exp\\left(-\\frac{\\tau}{2} (y_i - \\mu)^2 \\right) \\\\ &amp;\\propto \\exp \\left( -\\frac{\\tau}{2} \\sum_i(y_i - \\mu)^2 \\right) \\end{align} \\] The prior PDF for \\(\\mu\\) is the normal distribution. Again we take \\(\\tau\\) as given initially. \\[ \\begin{align} f(\\mu) &amp;= \\frac{\\tau_0^{1/2}}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{\\tau_0}{2} (\\mu - \\mu_0)^2 \\right) \\\\ &amp;\\propto \\exp\\left(-\\frac{\\tau_0}{2} (\\mu - \\mu_0)^2 \\right) \\end{align} \\] Substitute into Bayes’ Theorem. Since we are working with proportions, we can throw out the denominator and say \\(f(\\mu|y) \\propto f(y|\\mu)f(\\mu)\\). Plugging in and solving, we get \\[ \\begin{equation} \\mu|y \\sim N\\left(\\frac{n\\tau\\bar{y} + \\tau_0\\mu_0}{n\\tau + \\tau_0}, n\\tau + \\tau_0 \\right) \\tag{3.2} \\end{equation} \\] Using Bayes’ Theorem, the posterior distribution of \\(\\tau|y\\) is the joint likelihood of \\(y\\) and \\(\\tau\\) divided by the likelihood of \\(y\\), \\[ f(\\tau|y) = \\frac{f(y|\\tau)f(\\tau)}{\\int_\\tau f(y|\\mu)f(\\tau)d\\tau} \\] The conditional likelihood, \\(f(\\tau|y)\\), is the sum-product of the gamma distribution PDF. This time we take \\(\\mu\\) as given. \\[ \\begin{align} f(y|\\tau) &amp;= \\prod_i \\frac{\\tau^{(1/2)}}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{\\tau}{2} (y_i - \\mu)^2 \\right) \\\\ &amp;= \\frac{\\tau^{n/2}}{(2\\pi)^{n/2}} \\exp\\left(-\\frac{\\tau}{2} \\sum_i (y_i - \\mu)^2 \\right) \\\\ &amp;\\propto \\tau^{n/2} \\exp \\left( -\\frac{\\tau}{2} \\sum_i(y_i - \\mu)^2 \\right) \\end{align} \\] The prior PDF for \\(\\tau\\) is the gamma distribution. Pull the constant out to work with proportionality. \\[ \\begin{align} f(\\tau) &amp;= \\frac{b^a \\tau^{a-1} e^{-b\\tau}}{\\Gamma(a)} \\\\ &amp;\\propto \\tau^{a-1}e^{-b\\tau} \\end{align} \\] Substitute into Bayes’ Theorem. Since we are working with proportions, we can throw out the denominator and say \\(f(\\tau|y) \\propto f(y|\\tau)f(\\tau)\\). Plugging in and solving, we get \\[ \\begin{equation} \\tau|y \\sim \\text{Gamma}\\left(a + n/2, b + \\frac{1}{2} \\sum_i(y_i - \\mu)^2 \\right) \\tag{3.3} \\end{equation} \\] We have \\(y_i|\\mu,\\tau \\sim N(\\mu,\\tau)\\) with conjugate priors \\(\\mu \\sim N(\\mu_0, \\tau_0)\\) and \\(\\tau \\sim \\text{Gamma}(a,b)\\) and conditional posterior distributions shown in Eqns (3.2) and (3.3). Returning to Eqn (3.2), you can see how \\(E[\\mu] \\rightarrow \\bar{y}\\) as the sample size grows. Below, the terms divided by \\(n\\) disappear, leaving just \\(\\bar{y}\\). \\[ \\begin{align} E[\\mu|\\tau, y] &amp;= \\frac{n\\tau\\bar{y} + \\tau_0\\mu_0}{n\\tau + \\tau_0} \\\\ &amp;= \\frac{\\bar{y}\\tau + \\mu_0\\tau_0/n}{\\tau + \\tau_0/n} \\\\ &amp;\\sim \\bar{y} \\end{align} \\] The posterior mean estimator of \\(\\tau\\) is the ratio of the posterior gamma distribution parameters. Again, as the sample size increases, terms divided by \\(n\\) disappear. \\[ \\begin{align} E[\\tau|\\mu,y] &amp;= \\frac{a + n/2}{b + \\frac{1}{2} \\sum_i(y_i - \\mu)^2} \\\\ &amp;= \\frac{2a/n + 1}{2b/n + \\sum_i(y_i - \\mu)^2 / n} \\\\ &amp;\\sim \\frac{1}{\\sum_i(y_i - \\mu)^2} \\end{align} \\] The problem here is that you never know \\(\\mu\\) or \\(\\tau\\), so you cannot use the posterior formulas directly. Instead, you need to use sampling. In particular, you use the Gibbs sampler. Set \\(\\mu\\) and \\(\\tau\\) to some initial values and use the posterior equations to estimate new values for \\(\\mu\\) and \\(\\tau\\), then repeat. This is called Markov Chain Monte Carlo (MCMC) simulation because you are chaining the simulations. The method of sampling from a conditional posterior is called the Gibbs sampler. Let’s apply this using anthropological data collected by Nancy Howell of human height. # downloaded this from # https://github.com/rmcelreath/rethinking/blob/master/data/Howell1.csv howell &lt;- read_delim(&quot;input/Howell1.csv&quot;, delim = &quot;;&quot;, show_col_types = FALSE) %&gt;% filter(age &gt;= 18) %&gt;% mutate(male = factor(male, labels = c(&quot;women&quot;, &quot;men&quot;))) howell %&gt;% ggplot(aes(sample = height, color = male)) + stat_qq() + geom_qq_line() From prior knowledge, we know average human height is about \\(175 \\pm 10\\) cm. Using the \\(\\pm\\) = 2SD, the variance \\(5^2\\). Use vague priors of \\(\\mu \\sim N(\\mu_0 = 175, \\tau_0 = 1/5^2)\\) and \\(\\tau = \\sim \\text{Gamma}(a = .01, b = .01)\\). Start by assigning starting values, \\(\\mu*\\) and \\(\\tau*\\)). Given \\(\\tau = \\tau*\\), sample a new value of \\(\\mu*\\) from the normal distribution. Given \\(\\mu = \\mu*\\), sample a new value of \\(\\tau*\\) from the gamma distribution. Then repeat. gibbs_normal &lt;- function(y, mu_0, tau_0, a, b, n_iter){ n &lt;- length(y) y_mean &lt;- mean(y) mu_sample &lt;- tau_sample &lt;- numeric(n_iter) # starting values mu_sample[1] &lt;- mean(y) tau_sample[1] &lt;- 1 / var(y) # Gibbs sampler for(i in 2:n_iter){ # mu tau &lt;- tau_sample[i-1] mean_mu &lt;- (n * y_mean * tau + mu_0 * tau_0) / (n * tau + tau_0) precision_mu &lt;- n * tau + tau_0 mu_sample[i] &lt;- rnorm(1, mean_mu, 1 / sqrt(precision_mu)) # tau mu &lt;- mu_sample[i-1] tau_sample[i] &lt;- rgamma(1, a + n/2, b + .5 * sum((y - mu)^2)) } return(list(mu = mu_sample, tau = tau_sample)) } set.seed(12345) sample_m &lt;- gibbs_normal(howell[howell$male == &quot;men&quot;,]$height, 175, 1/5^2, .01, .01, 10^3) sample_w &lt;- gibbs_normal(howell[howell$male == &quot;women&quot;,]$height, 175, 1/5^2, .01, .01, 10^3) # posterior mu mean(sample_m$mu); quantile(sample_m$mu, c(.025, .975)) ## [1] 160.5146 ## 2.5% 97.5% ## 159.6103 161.4231 mean(sample_w$mu); quantile(sample_w$mu, c(.025, .975)) ## [1] 149.6363 ## 2.5% 97.5% ## 148.9256 150.3641 # posterior probability that men are taller than women on average mean(sample_m$mu &gt; sample_w$mu) ## [1] 1 # posterior probability that a random man is taller than a random woman tilde_m &lt;- rnorm(10^3, mean(sample_m$mu), sqrt(1/mean(sample_m$tau))) tilde_w &lt;- rnorm(10^3, mean(sample_w$mu), sqrt(1/mean(sample_w$tau))) mean(tilde_m &gt; tilde_w) ## [1] 0.909 tibble( iter = rep(1:10^3, 2), sex = c(rep(&quot;men&quot;, 10^3), rep(&quot;women&quot;, 10^3)), mu = c(sample_m$mu, sample_w$mu), tau = c(sample_m$tau, sample_w$tau), ) %&gt;% pivot_longer(cols = c(mu, tau)) %&gt;% ggplot(aes(x = value, color = sex)) + geom_density() + facet_wrap(facets = vars(name), scales = &quot;free&quot;) + labs(title = &quot;Posterior Distributions&quot;, x = NULL, color = NULL) tibble( iter = rep(1:10^3, 2), sex = c(rep(&quot;men&quot;, 10^3), rep(&quot;women&quot;, 10^3)), tilde = c(tilde_m, tilde_w) ) %&gt;% ggplot(aes(x = tilde, color = sex)) + geom_density() + labs(title = &quot;Posterior Predictive Distributions&quot;, x = NULL, color = NULL) It may take some time to converge on a solution. This convergence is called burn-in and is often discarded when describing the posterior. Slow mixing may occur if there is high autocorrelation in the Gibbs sample, resulting in slow exploration of the sample space of the posterior. In Bayesian statistics, the normal distribution is parameterized with the inverse of variance, called the precision, \\(\\tau = 1 / \\sigma^2\\).↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
