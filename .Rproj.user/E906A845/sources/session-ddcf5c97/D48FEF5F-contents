# Bayesian Statistics

```{r include=FALSE}
library(tidyverse)
library(patchwork)
library(glue)
library(scales)
```

Bayesian inference estimates the probability, $\theta$, that an hypothesis is true. It differs from classical (aka, frequentist) inference in its insistence that *all* uncertainties be described by probabilities. Bayesian inference updates the prior probability distribution in light of new information.

Bayesian inference differs from classical inference in three respects. Both methods assume a data generating mechanism expressed as a likelihood. However, classical inference assumes the nature of the mechanism is infinitely repeatable while Bayesian inference treats each event as unique. classical estimates a single population parameter (usually the mean), while Bayesian inference estimates a parameter _distribution_. Finally, the machinery of classical inference is maximizing likelihood and for Bayesian inference it is updating the prior distribution.

Bayesian inference builds on Bayes' Theorem, so let's start there.

## Bayes' Theorem

Bayes' Theorem, is the inverse conditional probability, the probability of the condition given the observed outcome. It reorganizes the relationship between joint probability and conditional probability, $P(\theta D) = P(\theta|D)P(D) = P(D|\theta)P(\theta)$, into 

$$P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}.$$

In common English, the probability that $\theta$ is after observing $D$ is equal to the probability of observing $D$ when $\theta$ is true divided by the probability of observing $D$ under _any_ circumstance.

* $P(\theta)$ is the strength of your belief in $\theta$ *prior* to considering the data $D$.

* $P(D|\theta)$ is the *likelihood* of observing $D$ from a generative model with parameter $\theta$. Note that the likelihood is a probability density, and is not quite the same as probability. For a continuous variable, likelihoods can sum to greater than 1. E.g., `dbinom(seq(1, 100, 1), 100, .5)` sums to `r sum(dbinom(seq(1, 100, 1), 100, .5))`, but `dnorm(seq(0,50,.001), 10, 10)` sums to `r scales::number(sum(dnorm(seq(0,50,.001), 10, 10)), accuracy = 1)`.

* $P(D)$ is the likelihood of observing $D$ from *any* prior. It is the *marginal distribution*, or *prior predictive distribution* of $D$. The likelihood divided by the marginal distribution is the proportional adjustment made to the prior in light of the data.

* $P(\theta|D)$ is the strength of your belief in $\theta$ *posterior* to considering $D$.

Bayes' Theorem is useful for evaluating medical tests. A test's *sensitivity* is its probability of yielding a positive result $D$ when condition $\theta$ exists. $P(D|\theta)$ is a test sensitivity, $\mathrm{sens}$. $P(\theta)$ is the probability of $\theta$ prior to the test (e.g., the general rate in society), $\mathrm{prior}$. The numerator of Bayes' Theorem, the joint probability $P(D \theta) = P(D|\theta)P(\theta)$, is $\mathrm{sens\times prior}$. A test's *specificity* is the probability of observing negative test result $\hat{D}$ when the condition does not exist, $\hat{\theta}$. The specificity is the compliment of a *false positive* test result, $P(\hat{D} | \hat{\theta}) = 1 - P(D | \hat{\theta})$. The denominator of Bayes' Theorem is the overall probability of a positive test result, $P(D) = P(D|\theta)P(\theta) + P(D|\hat\theta)P(\hat\theta)$ or in terms of sensitivity and specificity, $P(D) = \mathrm{(sens \times prior) + (1 - spec)(1 - prior)}$.

**Example.** Suppose E. Coli is typically present in 4.5% of samples, and an E. Coli screen has a sensitivity of 0.95 and a specificity of 0.99. Given a positive test result, what is the probability that E. Coli is actually present?

$$P(\theta|D) = \frac{.95\cdot .045}{.95\cdot .045 + (1 - .99)(1 - .045)} = \frac{.04275}{.04275 + .00955} = \frac{.04275}{.05230} = 81.7\%.$$

The elements of Bayes' Theorem come directly from the contingency table. The first row is the positive test result. The probability of E. Coli is the joint probability of E. coli and a positive test divided by the probability of a positive test

|      |E. Coli|Safe   |Total  |
|------|-------|-------|-------|
|+ Test|.95 * .045 = 0.04275|.01 * .955 = 0.00955|0.05230|
|- Test|.05 * .045 = 0.00225|.99 * .955 = 0.94545|0.94770|
|Total |0.04500|0.95500|1.00000|

## Bayesian Inference

Bayesian inference extends the logic of Bayes' Theorem by replacing the prior probability *estimate* that $\theta$ is true with a prior probability *distribution* that $\theta$ is true. Rather than saying, "I am *x*% certain $\theta$ is true," you say "I believe the probability that $\theta$ is true is somewhere in a range that has maximum likelihood at *x*%".

$$
f(\theta | D) = \frac{f(D|\theta) f(\theta)}{\int_\Theta f(D|\theta) f(\theta) d\theta}
$$

This formula expresses the posterior distribution of $\theta$ as a function of the prior distribution and new information.

Let $\Pi(\theta)$ be the prior probability function. $\Pi(\theta)$ has a PMF or PDF $P(\theta) = f(\theta)$, and a set of conditional distributions, $\{f_\theta(D) = f(D|\theta), \theta \in \Omega\}$, called the *generative model*. $f_\theta(D)$ is the *likelihood* of observing $D$ given $\theta$. The numerator is the product of the likelihood and the PMF/PDF, $f_\theta(D)P(\theta)$, the joint distribution of $(D, \theta)$. The denominator is the marginal distribution, or prior predictive distribution, of $D$: $m(D) = \int_\Omega f_\theta(D)P(\theta) d\theta$. For discrete cases, replace the integral with a sum, $m(D) = \sum\nolimits_\Omega f_\theta(D) P(\theta)$. The *posterior* probability distribution of $\theta$, conditioned on the observance of $D$, $\Pi(\cdot|D)$, is the joint density, $f_\theta(D) P(\theta),$ divided by the the marginal density, $m(D)$.

$$P(\theta | D) = \frac{f_\theta(D) P(\theta)}{m(D)}$$

The numerator makes the posterior proportional to the prior. The denominator is a normalizing constant that scales the likelihood into a proper density function (whose values sum to 1).

It is easier to see how observed evidence shifts the probabilities of the priors into their posterior probabilities by working with discrete priors first. From there it is straight-forward to grasp the more abstract case of continuous prior and posterior distributions.

## Discrete Cases

Suppose you have a string of numbers $[1,1,1,1,0,0,1,1,1,0]$ (7 ones and 3 zeros) produced by a Bernoulli random number generator. What parameter $p$ was used in the Bernoulli function?^[Example borrowed from [chris' sandbox](http://chrisstrelioff.ws/sandbox/2014/12/11/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations.html)]

```{r}
D <- c(1, 1, 1, 1, 0, 0, 1, 1, 1, 0)
```

Your best guess is $p = 0.7$, but how confident are you? Posit eleven competing priors, $\theta = [.0, .1, \ldots, 1]$ with equal prior probabilities, $P(\theta) = [1/11, \ldots]$.

```{r}
theta <- seq(0, 1, by = 0.1)
prior <- rep(1/11, 11)
```

Using a Bernoulli generative model, the likelihood of observing 7 ones and 3 zeros are $P(D|\theta) = \theta^7 + (1-\theta)^3.$

```{r fig.height=2.5}
likelihood <- theta^7 * (1 - theta)^3

data.frame(theta, likelihood) %>% 
  ggplot() +
  geom_segment(aes(x = theta, xend = theta, y = 0, yend = likelihood), 
               linetype = 2, color = "steelblue") +
  geom_point(aes(x = theta, y = likelihood), color = "steelblue", size = 3) +
  scale_x_continuous(breaks = theta) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank()) +
  labs(title = expression(paste("Maximum likelihood of observing D is at ", theta, " = 0.7.")),
       x = expression(theta), y = expression(f[theta](D)))
```

The posterior probability is the likelihood divided by the marginal probability of observing $D$ multiplied by the prior, $P(\theta|D) = \frac{P(D|\theta)}{P(D)}\cdot P(\theta).$ In this case, the marginal probability is straight-forward to calculate: it is the sum-product of the priors and their associated likelihoods.

```{r}
posterior <- likelihood / sum(likelihood * prior) * prior
```

```{r fig.height=3, echo=FALSE}
data.frame(theta, prior, posterior) %>% 
  pivot_longer(cols = c(prior, posterior), values_to = "pi") %>%
  ggplot() +
  geom_point(aes(x = theta, y = pi, color = name), size = 1) +
  geom_line(aes(x = theta, y = pi, color = name), size = 1) +
  scale_x_continuous(breaks = theta) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank()) +
  labs(title = "Posterior probabilities are adjusted priors.", 
       color = NULL,
       x = expression(theta), y = "P")
```

What would the posterior look like if we started with an educated guess on $P(\theta)$ that more heavily weights $\theta = 0.7$?

```{r}
prior <- c(.05, .05, .05, .05, .05, .10, .15, .20, .15, .10, .05)
posterior <- likelihood / sum(likelihood * prior) * prior
```

```{r fig.height=3, echo=FALSE}
data.frame(theta, prior, posterior) %>% 
  pivot_longer(cols = c(prior, posterior), values_to = "pi") %>%
  ggplot() +
  geom_point(aes(x = theta, y = pi, color = name), size = 1) +
  geom_line(aes(x = theta, y = pi, color = name), size = 1) +
  scale_x_continuous(breaks = theta) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank()) +
  labs(title = "Smarter priors increase posterior probability at maximum.", 
       color = NULL, x = expression(theta), y = "P")
```

What if we now employ a larger data set? To see, generate a sample of 100 Bernoulli(.7) observations.

```{r}
D100 <- rbernoulli(100, p = 0.7) %>% as.numeric()
likelihood <- theta^sum(D100) * (1 - theta)^(length(D100)-sum(D100))
posterior <- likelihood / sum(likelihood * prior) * prior
```

```{r fig.height=3, echo=FALSE}
data.frame(theta, prior, posterior) %>% 
  pivot_longer(cols = c(prior, posterior), values_to = "pi") %>%
  ggplot() +
  geom_point(aes(x = theta, y = pi, color = name), size = 1) +
  geom_line(aes(x = theta, y = pi, color = name), size = 1) +
  scale_x_continuous(breaks = theta) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank()) +
  labs(title = "More observations tighten the posterior distribution.",
       color = NULL, x = expression(theta), y = "P")
```

What would it look like if it also had more competing hypotheses, $\theta \in (0, .01, .02, \ldots, 1)$.

```{r}
theta <- seq(0, 1, by = .01)
prior <- rep(1/100, 101)
likelihood <- theta^sum(D100) * (1 - theta)^(length(D100)-sum(D100))
posterior <- likelihood / sum(likelihood * prior) * prior
```

```{r fig.height=3, echo=FALSE}
data.frame(theta, prior, posterior) %>% 
  pivot_longer(cols = c(prior, posterior), values_to = "pi") %>%
  ggplot() +
  geom_point(aes(x = theta, y = pi, color = name), size = 1) +
  geom_line(aes(x = theta, y = pi, color = name), size = 1) +
  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank()) +
  labs(title = "More priors smooths the distribution.", 
       color = NULL, x = expression(theta), y = "P")
```

## Continuous Cases

Continuing the example of inferring the parameter $p$ used in the Bernoulli process, what if we considered *all* values between 0 and 1?^[[Chris's Sandbox again](http://chrisstrelioff.ws/sandbox/2014/12/11/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations.html).]

When prior beliefs are best described in continuous distributions, express them using the beta, gamma, or normal distribution so that the posterior distributions are conjugates of the prior distributions with new parameter values. Otherwise, the marginal distribution is difficult to calculate. In this case, use the beta distribution, described by shape parameters, $\alpha$ and $\beta$.

$$P(\theta|D,\alpha,\beta) = \frac{f_\theta(D) P(\theta|\alpha,\beta)}{\int_0^1 f_\theta(D)P(\theta|\alpha, \beta)d\theta}$$

As with the discrete case, the numerator is the likelihood of observing $D$ if $\theta$ is true multiplied by the prior probability, but now the prior is a Beta($\alpha$, $\beta$) distribution. The denominator, sometimes called the *evidence*, is the marginal probability of $D$.

The likelihood of observing $D$ = $a$ successes and $b$ non-successes given a success probability of $p$ = $\theta$ is

$$f_\theta(D) = \theta^a(1-\theta)^b$$

The prior distribution is the probability density function of the beta distribution

$$P(\theta|\alpha,\beta) = \frac{1}{\mathrm{B}(\alpha, \beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}$$

where $\mathrm{B}(\alpha, \beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)}$ is the beta function.

The marginal distribution is

$$\int_0^1 f_\theta(D)P(\theta|\alpha, \beta)d\theta = \frac{\mathrm{B}(\alpha + a, \beta + b)}{\mathrm{B}(\alpha, \beta)}$$

Putting this all together, the posterior distribution is

$$P(\theta|D, \alpha, \beta) = \frac{1}{\mathrm{B}(\alpha + a, \beta + b)} \theta^{\alpha-1+a}(1-\theta)^{\beta-1+b}$$

The posterior equals the prior with shape parameters incremented by the observed counts, $a$ and $b.$

```{r}
plot_bayes <- function(alpha, beta, a, b) {

  prior_ev <- (alpha / (alpha + beta)) %>% round(2)
  posterior_ev <- ((alpha + a) / (alpha + beta + a + b)) %>% round(2)
  dat <- data.frame(theta = seq(0, 1, by = .01)) %>%
    mutate(prior = (1 / beta(alpha, beta)) * theta^(alpha-1) * (1-theta)^(beta-1),
           prior_ci = theta > qbeta(.025, alpha, beta) & 
                      theta < qbeta(.975, alpha, beta),
           likelihood = theta^a * (1-theta)^b,
           posterior = (1 / beta(alpha + a, beta + b)) * theta^(alpha-1+a) * (1-theta)^(beta-1+b),
           posterior_ci = theta > qbeta(.025, alpha + a, beta + b) & 
                      theta < qbeta(.975, alpha + a, beta + b))
  
  p_prior <- dat %>%
    ggplot(aes(x = theta, y = prior)) + 
    geom_line(color = "steelblue") +
    geom_area(aes(alpha = prior_ci), fill = "steelblue") +
    geom_vline(xintercept = prior_ev, color = "steelblue") +
    scale_alpha_manual(values = c(.1, .5)) +
    theme_minimal() +
    theme(legend.position = "none") +
    labs(x = NULL)
  
  p_likelihood <- dat %>%
    ggplot(aes(x = theta, y = likelihood)) + 
    geom_line(color = "steelblue") +
    theme_minimal() +
    theme(legend.position = "none") +
    labs(x = NULL)
  
  p_posterior <- dat %>%
    ggplot(aes(x = theta, y = posterior)) + 
    geom_line(color = "steelblue") +
    geom_area(aes(alpha = posterior_ci), fill = "steelblue") +
    geom_vline(xintercept = posterior_ev, color = "steelblue") +
    scale_alpha_manual(values = c(.1, .5))  +
    theme_minimal() +
    theme(legend.position = "none") +
    labs(x = expression(theta))
  
  out <- p_prior / p_likelihood / p_posterior +
    plot_annotation(
      title = glue("Beta({alpha}, {beta}) prior with observed evidence a = {a} ",
                   "and b = {b}"),
      subtitle = "with shaded 95% credible interval.",
      caption = glue("Prior expected value = {prior_ev}; Posterior expected ",
                     "value = {posterior_ev}"))
  out
}
```

Suppose you claim complete ignorance and take a uniform Beta(1, 1) prior. Recall that you observed a = 7 ones and b = 3 zeros. The posterior expected value is still pretty close!

```{r fig.height=7}
plot_bayes(alpha = 10, beta = 10, a = 7, b = 3)
```

Suppose you had prior reason to believe *p* = 0.7. You would model that as $\alpha$ = 7, $\beta$ = 3. The prior probability distribution would be $P(\theta|\alpha = 7,\beta = 3) = \frac{1}{\mathrm{B}(7, 3)}\theta^{7-1}(1-\theta)^{3-1}$. Then after observing a = 7 ones and b = 3 zeros, the posterior probability distribution would be $P(\theta|\alpha = 7+7,\beta = 3+3) = \frac{1}{\mathrm{B}(7+7, 3+3)}\theta^{7+7-1}(1-\theta)^{3+3-1}$.

```{r fig.height=7}
plot_bayes(alpha = 7, beta = 3, a = 7, b = 3)
```

## Bayes Factors

The Bayes Factor (BF) is a measure of the relative evidence of one model over another. Take another look at Bayes' formula:

$$P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}.$$

Suppose you want to compare how two models explain and observed data outcome, $D$. Model $M_1:f_1(D|\theta_1)$ says the observed data $D$ was produced by a generative model with pdf $f_1$ parameterized by $\theta_2$. Model $M_2:f_2(D|\theta_2)$ says it was produced by a generative model with pdf $f_2$ parameterized by $\theta_2$. In each model you specify a prior probability distribution for the parameter

If you take the ratio of the posterior probabilities, the posterior odds, the $P(D)$ terms cancel and you have

$$\frac{P(\theta_1|D)}{P(\theta_2|D)} = \frac{P(D|\theta_1)}{P(D|\theta_2)} \cdot \frac{P(\theta_1)}{P(\theta_2)}$$

The posterior odds equals the ratio of the likelihoods multiplied by the prior odds. That likelihood ratio is the Bayes Factor (BF). Rearranging, BF is the odds ratio of the posterior and prior odds.

$$BF = \frac{P(D|\theta_1)}{P(D|\theta_2)} = \mathrm{\frac{Posterior Odds}{Prior Odds}}$$

Return to the example of observing $D$ = 7 ones and 3 zeros. You can compare an hypothesized $\theta$ of .5 to a completely agnostic model where $\theta$ is uniform over [0, 1]. The likelihood of observing $D$ when $\theta$ = .5 is $P(D|\theta_1) = 5^7(1-.5)^3$ = `r scales::number(dbinom(7, 10, .5), accuracy = .001)`. The likelihood of observing $D$ where $\theta$ is uniform on [0, 1] is $P(D|\theta_2) = \int_0^1 \binom{10}{3}q^7(1-q)^3dq$

```{r}
.5^1 * .5^1
dbinom(1, 1, .5)
dbinom(11, 11, .5)
beta(11, 11)
```



with a uniform Beta(1, 1) prior (i.e., complete agnosticism). 


The Bayes factor at $\theta$ = .7 quantifies how much the odds of H0: $\theta$ = .7 over H1: $\hat{\theta}$ = .7.

```{r}
prior <- function(theta, alpha, beta) {
  (1 / beta(alpha, beta)) * theta^(alpha-1) * (1-theta)^(beta-1)
}
posterior <- function(theta, alpha, beta, a, b) {
  (1 / beta(alpha + a, beta + b)) * theta^(alpha-1+a) * (1-theta)^(beta-1+b)
}

prior(.5, 115, 85) 
posterior(.5, 1, 1, 10, 10)

posterior(.5, 1, 1, 10, 10) / prior(.5, 1, 1) 

1 / beta(115, 85)

# Posterior Distribution 
1/beta(1+10, 1+10) * .5^(1-1+10) * (1-.5)^(1-1+10)
dbeta(.5, 11, 11)

# Prior Beta Distributions
1/beta(1, 1) * .5^(1-1) * (1-.5)^(1-1)
dbeta(.5, 1, 1)


dbeta(.5, 115, 85)
```


The Bayes factor measures how much your prior belief is altered by the evidence. It is the ratio of the likelihoods at some hypothesized value before and after observing the data. In this case, our confidence increased by a factor of...

```{r collapse=TRUE}
theta <- 0.5

alpha <- 1
beta <- 1
a <- 10
b <- 10

(prior_likelihood <- (1 / beta(alpha, beta)) * theta^(alpha-1) * (1-theta)^(beta-1))
(posterior_likelihood <- (1 / beta(alpha + a, beta + b)) * theta^(alpha-1+a) * (1-theta)^(beta-1+b))
(bayes_factor <- posterior_likelihood / prior_likelihood)

# 3.7 on alpha = beta = 1
# 1.91 on alpha = beta = 4
```

## A Gentler Introduction

This section is my notes from DataCamp course [Fundamentals of Bayesian Data Analysis in R](https://learn.datacamp.com/courses/fundamentals-of-bayesian-data-analysis-in-r). It is an intuitive approach to Bayesian inference.

Suppose you purchase 100 ad impressions on a web site and receive 13 clicks. How would you describe the click rate? The Frequentist approach would be to construct a 95% CI around the click proportion.

```{r}
(ad_prop_test <- prop.test(13, 100))
```

```{r fig.height=3, warning=FALSE, echo=FALSE}
ad_prop_test <- prop.test(13, 100)
click_rate_mu <- ad_prop_test$estimate
click_rate_se <- sqrt(click_rate_mu*(1-click_rate_mu)) / sqrt(100)
data.frame(pi = seq(0, .30, by = .01)) %>%
  mutate(
    likelihood = dnorm(pi, click_rate_mu, click_rate_se),
    # observed = map(pi, ~ c(., 1 - .) * 100),
    # expected = map(13/100, ~c(., 1 - .) * 100),
    # x2 = map2_dbl(observed, expected, ~sum((.x - .y)^2 / .y)),
    # likelihood = dchisq(x2, 1),
    # pcs = pchisq(x2, 1),
    ci_low = if_else(pi <= ad_prop_test$conf.int[1], likelihood, NA_real_),
    ci_high = if_else(pi >= ad_prop_test$conf.int[2], likelihood, NA_real_)
  ) %>%
  ggplot(aes(x = pi)) +
  geom_line(aes(y = likelihood)) +
  geom_area(aes(y = ci_low), fill = "firebrick", alpha = .4) +
  geom_area(aes(y = ci_high), fill = "firebrick", alpha = .4) +
  geom_vline(xintercept = click_rate_mu, linetype = 2) +
  theme_minimal() +
  scale_x_continuous(breaks = seq(0, .3, .05)) +
  theme(panel.grid.minor = element_blank()) +
  labs(title = "Frequentist proportion test for 13 clicks in 100 impressions", 
       subtitle = glue("p = {click_rate_mu}, 95%-CI (",
       "{ad_prop_test$conf.int[1] %>% scales::number(accuracy = .01)}, ",
       "{ad_prop_test$conf.int[2] %>% scales::number(accuracy = .01)})"),
       x = expression(theta))
```

How might you model this using Bayesian reasoning? One way is to run 1,000 experiments that sample 100 ad impression events from an `rbinom()` generative model using a uniform prior distribution of 0-30% click probability. The resulting 1,000 row data set of click probabilities and sampled click counts forms a joint probability distribution. This method of Bayesian analysis is called *rejection sampling* because you sample across the whole parameter space, then condition on the observed evidence.

```{r}
df_sim <- data.frame(click_prob = runif(1000, 0.0, 0.3))
df_sim$click_n <- rbinom(1000, 100, df_sim$click_prob)
```

```{r echo=FALSE}
df_sim %>% 
  mutate(is_13 = factor(click_n == 13, levels = c(TRUE, FALSE))) %>%
  ggplot(aes(x = click_prob, y = click_n, color = is_13)) +
  geom_point(alpha = 0.6, show.legend = FALSE) +
  geom_hline(yintercept = 13, color = "steelblue", linetype = 1, size = .5) +
  scale_color_manual(values = c("TRUE" = "steelblue", "FALSE" = "gray80")) +
  theme_minimal() +
  labs(title = "Joint probability of observed clicks and click probability",
       subtitle = "with conditioning on 13 observed clicks.",
       y = "clicks per 100 ads",
       x = expression(theta))
```

Condition the joint probability distribution on the 13 observed clicks to update your prior. The `quantile()` function returns the median and the .025 and .975 percentile values - the *credible interval*.

```{r}
# median and credible interval
(sim_ci <- df_sim %>% filter(click_n == 13) %>% pull(click_prob) %>% 
  quantile(c(.025, .5, .975)))
```

Your posterior click rate likelihood is `r scales::percent(sim_ci[2], accuracy = .1)` with 95 credible interval [`r scales::percent(sim_ci[1], accuracy = .1)`, `r scales::percent(sim_ci[3], accuracy = .1)`]. Here is the density plot of the `r df_sim %>% filter(click_n == 13) %>% nrow()` simulations that produced the 13 clicks. The median and 95% credible interval are marked.

```{r echo=FALSE}
df_sim %>% 
  filter(click_n == 13) %>% 
  ggplot(aes(x = click_prob)) +
  geom_density() +
  geom_vline(xintercept = sim_ci[2]) +
  geom_vline(xintercept = sim_ci[1], linetype = 2) +
  geom_vline(xintercept = sim_ci[3], linetype = 2) +
  scale_x_continuous(breaks = seq(0, .3, .05)) +
  theme_minimal() +
  labs(title = "Posterior click likelihood distribution", 
       subtitle = glue("p = {sim_ci[2] %>% scales::number(accuracy = .01)}, 95%-CI (",
       "{sim_ci[1] %>% scales::number(accuracy = .01)}, ",
       "{sim_ci[3] %>% scales::number(accuracy = .01)})"),
       x = expression(theta), y = "density (likelihood)")
```

That's pretty close to the frequentist result! Instead of running 1,000 experiments with randomly selected click probabilities and randomly selected click counts based on those probabilities, you could define a discrete set of candidate click probabilities, e.g. values between 0 and 0.3 incremented by .01, and calculate the click probability density for the 100 ad impressions. This method of Bayesian analysis is called *grid approximation*.

```{r}
df_bayes <- expand.grid(
  click_prob = seq(0, .3, by = .001), 
  click_n = 0:100
) %>%
  mutate(
    prior = dunif(click_prob, min = 0, max = 0.3),
    likelihood = dbinom(click_n, 100, click_prob),
    probability = likelihood * prior / sum(likelihood * prior)
  )
```

```{r echo=FALSE}
df_bayes %>% 
  mutate(is_13 = factor(click_n == 13, levels = c(TRUE, FALSE))) %>%
  filter(probability > .0001) %>%
  ggplot(aes(x = click_prob, y = click_n, color = is_13)) +
  geom_point(alpha = 0.6, show.legend = FALSE) +
  geom_hline(yintercept = 13, color = "steelblue", linetype = 1, size = .5) +
  scale_color_manual(values = c("TRUE" = "steelblue", "FALSE" = "gray80")) +
  # scale_color_gradient(low = "white", high = "steelblue", guide = "colorbar") +
  theme_minimal() +
  labs(title = "Joint probability of clicks and click probability.",
       subtitle = "with conditioning on 13 observed clicks.",
       y = "clicks per 100 ads",
       x = expression(theta))
```

Condition the joint probability distribution on the 13 observed clicks to update your prior. 

```{r}
df_bayes_13 <- df_bayes %>% filter(click_n == 13) %>%
  mutate(posterior = probability / sum(probability))
```

Instead of using the `quantile()` function on these values to measure the median and *credible interval*, resample the posterior probability to create a distribution.

```{r}
sampling_idx <- sample(
  1:nrow(df_bayes_13), 
  size = 10000, 
  replace = TRUE, 
  prob = df_bayes_13$posterior
)

sampling_vals <- df_bayes_13[sampling_idx, ]

(df_bayes_ci <- quantile(sampling_vals$click_prob, c(.025, .5, .975)))
```

```{r echo=FALSE, warning=FALSE}
df_bayes %>%
  filter(click_n == 13) %>%
  mutate(likelihood = probability / sum(probability),
         ci_low = if_else(click_prob < df_bayes_ci[1], likelihood, NA_real_),
         ci_high = if_else(click_prob > df_bayes_ci[3], likelihood, NA_real_), 
         ci = if_else(click_prob >= df_bayes_ci[1] & 
                        click_prob <= df_bayes_ci[3], "Y", "N")) %>%
  ggplot(aes(x = click_prob, y = likelihood)) +
  geom_line() +
  geom_area(aes(y = ci_low), fill = "firebrick", alpha = .4) +
  geom_area(aes(y = ci_high), fill = "firebrick", alpha = .4) +
  geom_vline(xintercept = df_bayes_ci[2], linetype = 2) +
  theme_minimal() +
  scale_x_continuous(breaks = seq(0, .3, .05)) +
  theme(panel.grid.minor = element_blank()) +
  labs(title = "Posterior click probability", 
       subtitle = glue("p = {df_bayes_ci[2] %>% scales::number(accuracy = .01)}, 95%-CI (",
       "{df_bayes_ci[1] %>% scales::number(accuracy = .01)}, ",
       "{df_bayes_ci[3] %>% scales::number(accuracy = .01)})"),
       x = expression(theta))
```

You can use a Bayesian model to estimate multiple parameters. Suppose you want to predict the water temperature in a lake on Jun 1 based on 5 years of prior water temperatures. 

```{r}
temp <- c(19, 23, 20, 17, 23)
```

You model the water temperature as a normal distribution, $\mathrm{N}(\mu, \sigma^2)$ with a prior distribution $\mu = \mathrm{N}(18, 5^2)$ and $\sigma = \mathrm{unif}(0, 10)$ based on past experience.

Using the grid approximation approach, construct a grid of candidate $\mu$ values from 8 to 30 degrees incremented by .5 degrees, and candidate $\sigma$ values from .1 to 10 incremented by .1 - a 4,500 row data frame. 

```{r}
mdl_grid <- expand_grid(mu = seq(8, 30, by = 0.5),
                        sigma = seq(.1, 10, by = 0.1))
```

For each combination of $\mu$ and $\sigma$, the *prior* probabilities are the densities from $\mu = \mathrm{N}(18, 5^2)$ and $\sigma = \mathrm{unif}(0, 10)$. The combined prior is their product. The *likelihoods* are the products of the probabilities of observing each `temp` given the candidate $\mu$ and $\sigma$ values.

```{r}
mdl_grid_2 <- mdl_grid %>%
  mutate(
    mu_prior = map_dbl(mu, ~dnorm(., mean = 18, sd = 5)),
    sigma_prior = map_dbl(sigma, ~dunif(., 0, 10)),
    prior = mu_prior * sigma_prior, # combined prior,
    likelihood = map2_dbl(mu, sigma, ~dnorm(temp, .x, .y) %>% prod()),
    posterior = likelihood * prior / sum(likelihood * prior)
  )
```

```{r echo=FALSE}
mdl_grid_2 %>%
  ggplot(aes(x = mu, y = sigma, fill = posterior)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "steelblue", guide = "colorbar") +
  theme_minimal() +
  labs(title = "Joint probability of mu and sigma.",
       x = expression(mu), y = expression(sigma))
```

Calculate a credible interval by drawing 10,000 samples from the grid with sampling probability equal to the calculated posterior probabilities. Use the `quantile()` function to estimate the median and .025 and .975 quantile values.

```{r fig.height=3}
sampling_idx <- sample(1:nrow(mdl_grid), size = 10000, replace = TRUE, prob = mdl_grid$posterior)
sampling_vals <- mdl_grid[sampling_idx, c("mu", "sigma")]
mu_ci <- quantile(sampling_vals$mu, c(.025, .5, .975))
sigma_ci <- quantile(sampling_vals$sigma, c(.025, .5, .975))
ci <- qnorm(c(.025, .5, .975), mean = mu_ci[2], sd = sigma_ci[2])

data.frame(temp = seq(0, 30, by = .1)) %>%
  mutate(prob = map_dbl(temp, ~dnorm(., mean = ci[2], sd = sigma_ci[2])),
         ci = if_else(temp >= ci[1] & temp <= ci[3], "Y", "N")) %>%
  ggplot(aes(x = temp, y = prob)) +
  geom_area(aes(y = if_else(ci == "N", prob, 0)), 
            fill = "firebrick", show.legend = FALSE) +
  geom_line() +
  geom_vline(xintercept = ci[2], linetype = 2) +
  theme_minimal() +
  scale_x_continuous(breaks = seq(0, 30, 5)) +
  theme(panel.grid.minor = element_blank()) +
  labs(title = "Posterior temperature probability", 
       subtitle = glue("mu = {ci[2] %>% scales::number(accuracy = .1)}, 95%-CI (",
       "{ci[1] %>% scales::number(accuracy = .1)}, ",
       "{ci[3] %>% scales::number(accuracy = .1)})"))
```

What is the probability the temperature is at least 18?

```{r}
pred_temp <- rnorm(1000, mean = sampling_vals$mu, sampling_vals$sigma)
scales::percent(sum(pred_temp >= 18) / length(pred_temp))
```

## Gamma Poisson

Given the following counts of weekday sandwich sales, what is the expected value? 

```{r}
y <- c(50, 65, 72, 63, 70)
```

Count data have a Poisson distribution, $y_i|\lambda \sim Pois(\lambda)$, with expected value $\lambda$ and PMF $f(y_i | \lambda) = e^{-\lambda}\frac{\lambda^{y_i}}{y_i!}$. Using Bayes' Theorem, the posterior distribution of $\lambda$ given evidence $\textbf{y}$ is equal to the joint likelihood of $\lambda$ and $\textbf{y}$ divided by the likelihood of $\textbf{y}$.

$$
f(\lambda |\textbf{y}) = \frac{f(\mathbf{y}|\lambda) f(\lambda)}{\int_\Lambda f(\mathbf{y}|\lambda) f(\lambda) d\lambda}
$$

The joint likelihood, $f(\textbf{y}|\lambda)$, is the sum-product of the Poisson distribution PMF.

$$
\begin{align}
f(\textbf{y}|\lambda) = f(y_i,\ldots, y_n | \lambda) &= \prod_i f(y_i | \lambda) \\
&= \prod_i e^{-\lambda}\frac{\lambda^{y_i}}{y_i!}
\end{align}
$$

The prior distribution, $f(\lambda)$, should have only positive values. Model it with the [gamma distribution](https://mpfoley73.github.io/probability/random-variables-and-distributions.html#gamma), $\lambda|a,b = \mathrm{Gamma}(a,b)$.

$$
f(\lambda) = f(\lambda | a,b) = \frac{b^a \lambda^{a-1} e^{-b\lambda}}{\Gamma(a)}
$$

where $\Gamma$ is the gamma function^[The gamma function is a generic function, just like sin, cos, etc., and is a kind of generalized factorial.]. Substituting into Bayes' Theorem and simplifying, you have this nightmare:

$$
f(\lambda |\textbf{y}) = \frac{\lambda^{a + \sum_i y_i-1}e^{-(b+n)\lambda}}{\int_0^\infty \lambda^{a + \sum_i y_i-1}e^{-(b+n)\lambda} d\lambda}
$$

However, there is good news. The integration in the denominator removes the dependence on $\lambda$, so $f(\lambda |\textbf{y}, a, b)$ is proportional to the numerator up to a constant.

$$
f(\lambda |\textbf{y}) \propto f(\textbf{y} | \lambda) f(\lambda)
$$

Since $f(\lambda |\textbf{y})$ is a PMF, it integrates (sums) to 1 and you can always figure out the constant later. What makes this good news is that this has the form of the PDF of the gamma distribution. 

$$
\begin{equation} 
\lambda | \textbf{y}, a, b \sim \mathrm{Gamma}(a + \sum_i y_i, b + n)
(\#eq:gamma-posterior)
\end{equation} 
$$

Equation \@ref(eq:gamma-posterior) is the posterior distribution of $\lambda$. We combined a gamma prior with the Poisson likelihood of evidence, $\textbf{y}$, to produce a gamma posterior. We call priors that produce posteriors of the same form, *conjugate priors* for the Poisson likelihood. Conjugate priors are popular because of their computational convenience.

Return to the sandwich sales data. We need values to plug into Equation \@ref(eq:gamma-posterior). For the gamma distribution, $E(X) = a / b$ and $\mathrm{Var}(X) = a / b^2$. You might guess from intuition that mean daily sandwich sales are 70 +/- 5. Interpreting +/- 5 as a 95% CI and using the rule of thumb that a 95% CI is 2 SD, $\mathrm{Var} = (2.5)^2 = 6.25$. Solve for $a = 784$ and $b = 11.2$. We also have $\sum_i y_i = 320$ and $n = 5$.

$$
\lambda | \textbf{y}, a, b \sim \mathrm{Gamma}(784 + 320, 11.2 + 5) \sim \mathrm{Gamma}(1104, 16.2)
$$

The posterior $E(y) = 1104 / 16.2 = 68.1$ and $\mathrm{Var}(y) = 1104 / 16.2^2 = 4.2$. Use the gamma distribution function to get the posterior 95% _credible_ interval.

```{r collapse=TRUE}
# Prior distribution
qgamma(p = c(.025, .975), 784, 11.2)

# Posterior distribution
qgamma(p = c(.025, .975), 784 + 320, 11.2 + 5)
```

Whereas the prior expected mean daily sandwich sales was 70 (95% CI: 65, 75), the posterior is 68 (95% CI: 64, 72). Compare this to classical statistics: $E(y) = \bar{y} = 64$, $SE = \sqrt{\bar{y} / n} = 3.6$:

```{r collapse=TRUE}
# Classical estimate
qnorm(p = c(.025, .975), 64, 3.6)
```

You might think that the reasonable Bayesian outcome was predicated on good $a$ and $b$ priors, but no. Suppose $a = .01$ and $b = .01$. The posterior is still reasonable.

```{r collapse=TRUE}
# Conservative prior
qgamma(p = c(.025, .975), .01 + 320, .01 + 5)
```

```{r}
tibble(
  lambda = seq(0, 80, .1),
  `Gamma(.01, .01)` = dgamma(lambda, .01, .01),
  `Gamma(.01 + 320, .01 + 5)` = dgamma(lambda, .01 + 320, .01 + 5),
  `Gamma(784, 11.2)` = dgamma(lambda, 784, 11.2),
  `Gamma(784 + 320, 11.2 + 5)` = dgamma(lambda, 784 + 320, 11.2 + 5)
) %>%
  pivot_longer(-lambda) %>%
  mutate(name = fct_inorder(name)) %>%
  mutate(prior = if_else(str_detect(name, "\\+"), "posterior", "prior")) %>%
  ggplot(aes(x = lambda, y = value, color = name, linetype = prior)) +
  geom_line(linewidth = 1) +
  scale_color_manual(values = c(rep("seagreen", 2), rep("firebrick", 2))) +
  labs(color = NULL, linetype = NULL, y = "density",
       title = "Posteriors from Conservative and Informed Priors.")
```

The Bayesian posterior approaches the classical $\bar{y}$ with increasing sample size.

$$
E(\lambda|\textbf{y}, a, b) = \frac{a + \sum_i y_i}{b + n} = \frac{a + n \bar{y}}{b + n}
$$

Taking the limit, $\lim_{n \rightarrow \infty} E(\lambda|\textbf{y}, a, b) = \bar{y}$.

The *central* credible interval is the standard Bayesian credible interval. But when the posterior distribution is not perfectly symmetric, the shortest credible interval capturing x% of the distribution might have different endpoints. Our example has a pretty symmetric distribution, but let's calculate the *highest density region (HDR)* anyway.

```{r collapse=TRUE}
pp <- seq(0.01, .99, by = .0001)
x <- map_dbl(pp, ~qgamma(., 784 + 320, 11.2 + 5))
hdrcde::hdr(x, prob = 95)$hdr
```

The posterior predictive distribution of a predicted value, $\tilde{y}$ is

$$
f(\tilde{y} | x) = \int f(\tilde{y}|\lambda) f(\lambda | \textbf{y}) d\lambda
$$

Our sandwich example has a well defined functional solution: the expected value from $\mathrm{Gamma}(1104, 16.2)$ is $1104/16.2 = 68$. Had we not known this, we could have simulated posterior values (Monte Carlo simulation) and calculated the mean and variance. The procedure is to take a random sample of perhaps 1,000 $\lambda$ values from the gamma posterior distribution, then for each $\lambda$ draw a single random $\tilde{y}$ from the Poisson distribution. 

```{r collapse=TRUE}
a <- 1104 
b <- 16.2 

set.seed(1234)

# random sample of lambdas, and a single random y_tilde for each lambda
lambda_r <- rgamma(1000, a, b)
y_tilde <- rpois(1000, lambda_r)

# posterior predictive distribution
mean(y_tilde)
quantile(y_tilde, c(.025, .975))
```

So on any given day, the predicted value of sandwich sales is `r round(mean(y_tilde), 1)` with 95% prediction interval `r quantile(y_tilde, c(.025, .975)) %>% comma(.1) %>% paste(collapse = ", ")`. The probability of exceeding 80 sandwiches, $P(\tilde{y} > 80 | \textbf{y})$, is `mean(y_tilde > 80)` = `r mean(y_tilde > 80) %>% percent(.1)`, and 99% of the time, sandwich sales will be less than `quantile(y_tilde, .99)` = `r quantile(y_tilde, .99)`.

You can also predict individual weekdays. Suppose you take a $\mathrm{Gamma}(700, 10)$ distribution as your prior.

```{r}
day_tbl <- tibble(
  dow = fct_inorder(c("Mon", "Tue", "Wed", "Thu", "Fri")),
  d = c(50, 65, 72, 63, 70)
) %>%
  mutate(
    post_a = 700 + d,
    post_b = 10 + 1,
    post_mean = post_a / post_b,
    post_lci = qgamma(.025, post_a, post_b),
    post_uci = qgamma(.975, post_a, post_b)
  )

day_tbl
```

What is the probability that Mon sales are less than Tue?

```{r collapse=TRUE}
set.seed(123)
lambda_r_mon <- rgamma(1000, 750, 11)
lambda_r_tue <- rgamma(1000, 765, 11)

# posterior probability 
mean(lambda_r_mon < lambda_r_tue)
```

Which day of the week has the highest sandwich sales?

```{r collapse=TRUE}
set.seed(12345)

lambda_r <- tibble(
  r_mon = rgamma(1000, 750, 11),
  r_tue = rgamma(1000, 765, 11),
  r_wed = rgamma(1000, 772, 11),
  r_thu = rgamma(1000, 763, 11),
  r_fri = rgamma(1000, 770, 11),
  r_dow = pmap(list(r_mon, r_tue, r_wed, r_thu, r_fri), 
                   function(m, t, w, r, f) c(m, t, w, r, f)),
  max_dow_idx = map_dbl(r_dow, ~which.max(.)),
  max_dow = map_chr(max_dow_idx, ~c("Mon", "Tue", "Wed", "Thu", "Fri")[.])
)

lambda_r %>% janitor::tabyl(max_dow)
```

## Normal

Suppose you have a sample, $\textbf{y}$, from a normally distribution population of unknown mean and precision, $\mu$ and $\tau$: $y_i|\mu, \tau \sim N(\mu, \tau)$.^[In Bayesian statistics, the normal distribution is parameterized with the inverse of variance, called the *precision*, $\tau = 1 / \sigma^2$.] Assume a normal prior for $\mu \sim N(\mu_0, \tau_0)$, and a gamma prior for $\tau \sim \text{Gamma}(a, b)$ since it takes only positive values. The PDF for $y_i$ is $f(y_i | \mu, \tau) = \frac{\tau^{.5}}{\sqrt{2\pi}} \exp\left(-\frac{\tau}{2} (y_i - \mu)^2 \right)$. We'll derive posterior distributions for $\mu$ and $\tau$ separately. 

Using Bayes' Theorem, the posterior distribution of $\mu|y$ is the joint likelihood of $y$ and $\mu$ divided by the likelihood of $y$, 

$$
f(\mu|y) = \frac{f(y|\mu)f(\mu)}{\int_\mu f(y|\mu)f(\mu)d\mu}
$$

The joint likelihood, $f(\mu|y)$, is the sum-product of the normal distribution PDF. We can take $\tau$ as given initially.

$$
\begin{align}
f(y|\mu) &= \prod_i \frac{\tau^{(1/2)}}{\sqrt{2\pi}} \exp\left(-\frac{\tau}{2} (y_i - \mu)^2 \right) \\
&\propto \prod_i \exp\left(-\frac{\tau}{2} (y_i - \mu)^2 \right) \\
&\propto \exp \left( -\frac{\tau}{2} \sum_i(y_i - \mu)^2 \right)
\end{align}
$$


The prior PDF for $\mu$ is the normal distribution, so $f(\mu) \propto \exp\left(-\frac{\tau_0}{2} (\mu - \mu_0)^2 \right)$. But since we are working with proportions, we can throw out the denominator and say $f(\mu|y) \propto f(y|\mu)f(\mu). Plugging in and solving, we get

$$
\begin{equation}
\mu|y \sim N\left(\frac{n\tau\bar{y} + \tau_0\mu_0}{n\tau + \tau_0}, n\tau + \tau_0 \right)
(\#eq:mu-posterior)
\end{equation}
$$

The prior PDF for \tau$ is the gamma distribution, so $
The derivation above took $\tau$ as given, but now we need to derive its posterior too. 

$$
f(\lambda |\textbf{D}) = \frac{\lambda^{a + \sum_i D_i-1}e^{-(b+n)\lambda}}{\int_0^\infty \lambda^{a + \sum_i D_i-1}e^{-(b+n)\lambda} d\lambda}
$$

However, there is good news. The integration in the denominator removes the dependence on $\lambda$, so $f(\lambda |\textbf{D}, a, b)$ is proportional to the numerator up to a constant.

$$
f(\lambda |\textbf{D}) \propto f(\textbf{D} | \lambda) f(\lambda)
$$

Since $f(\lambda |\textbf{D})$ is a PMF, you integrates (sums) to 1 and you can always figure out the constant later. What makes this good news is that this has the form of the PDF of the gamma distribution. 

$$
\begin{equation} 
\lambda | \textbf{D}, a, b \sim \mathrm{Gamma}(a + \sum_i D_i, b + n)
(\#eq:gamma-posterior)
\end{equation} 
$$

Equation \@ref(eq:gamma-posterior) is the posterior distribution of $\lambda$. We combined a gamma prior with the Poisson likelihood of evidence, $D$, to produce a gamma posterior. We call priors that produce posteriors of the same form, *conjugate priors* for the Poisson likelihood. Conjugate priors are popular because of their computational convenience.

