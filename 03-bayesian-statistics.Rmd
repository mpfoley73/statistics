# Bayesian Statistics

```{r include=FALSE}
library(tidyverse)
library(patchwork)
library(glue)
library(scales)

source("./setup.R")
```

Bayesian inference is an alternative to classical (aka, frequentist) inference. Both methods assume a data generating mechanism expressed as a likelihood, but while classical inference treats the mechanism as infinitely repeatable, Bayesian inference treats each event as unique. Instead of estimating a single population parameter (usually the mean), Bayesian inference estimates the parameter _distribution_. In fact Bayesian inference insists that *all* uncertainties be described by probabilities. Finally, whereas the machinery of classical inference is maximizing likelihood, Bayesian inference updates a prior probability distribution in light of the new information.

## Bayes' Theorem

Bayes' Theorem is the inverse conditional probability, the probability of the condition given the observed outcome. It reorganizes the relationship between joint probability and conditional probability.

$$
\begin{align}
P(\theta D) = P(\theta|D)P(D) &= P(D|\theta)P(\theta) \\
P(\theta|D) &= \frac{P(D|\theta)P(\theta)}{P(D)}
\end{align}
$$

The probability of $\theta$ after observing $D$ is equal to the probability of observing $D$ when $\theta$ is true divided by the probability of observing $D$ under _any_ circumstance.

* Think of $P(\theta)$ as the strength of your belief *prior* to considering $D$.

* $P(D|\theta)$ is the *likelihood* of observing $D$ from a generative model with parameter $\theta$. Likelihoods are probability densities, and are not quite the same as probabilities. For continuous variables, likelihoods will sum to greater than 1.^[`dbinom(seq(1, 100, 1), 100, .5)` sums to `r sum(dbinom(seq(1, 100, 1), 100, .5))`, but `dnorm(seq(0,50,.001), 10, 10)` sums to `r scales::number(sum(dnorm(seq(0,50,.001), 10, 10)), accuracy = 1)`.]

* $P(D)$ is the likelihood of observing $D$ from *any* prior. It is the *marginal distribution*, or *prior predictive distribution* of $D$. The likelihood divided by the marginal distribution is the proportional adjustment made to the prior in light of the data.

* $P(\theta|D)$ is the strength of your belief *posterior* to considering $D$.

One illustration of Bayes' Theorem is interpreting medical tests. $P(D|\theta)$ is the test's *sensitivity*, the probability of a positive test result $D$ when the condition $\theta$ in fact exists. $P(\theta)$ is the probability prior to testing, the general rate. The numerator of Bayes' Theorem is the joint probability, the probability of having the condition and testing positive, $P(D \theta) = P(D|\theta)P(\theta)$. However, there is another way to test positive - the false positive, $P(D | \hat{\theta})$! A test's *specificity* is the probability of a negative test result when the condition does not exist. Specificity is the compliment of the false positive, $P(\hat{D} | \hat{\theta}) = 1 - P(D | \hat{\theta})$. The denominator of Bayes' Theorem is the overall probability of a positive test result. 

$$
\begin{align}
P(\theta|D) &= \frac{P(D|\theta)P(\theta)}{P(D)} \\ 
&= \frac{P(D|\theta)P(\theta)}{P(D|\theta)P(\theta) + P(D|\hat\theta)P(\hat\theta)} \\
&= \frac{\text{sensitivity} \cdot \text{prior}}{\text{sensitivity} \cdot \text{prior} + (1 - \text{specificity}) \cdot (1 - \text{prior})}
\end{align}
$$

Suppose E. Coli is typically present in $P(\theta)$ = 4.5% of samples, and an E. Coli screen has a sensitivity of $P(D|\theta)$ = 95% and a specificity of 1 - $P(D|\hat\theta)$ = 99%. Given a positive test result, what is the probability that E. Coli is actually present?

$$P(\theta|D) = \frac{.95\cdot .045}{.95\cdot .045 + (1 - .99)(1 - .045)} = \frac{.04275}{.05230} = 81.7\%.$$

The elements of Bayes' Theorem come directly from the contingency table. The first row is the positive test result. The probability of E. Coli is the joint probability of E. Coli and a positive test divided by the probability of a positive test.

```{r echo=FALSE}
tribble(
  ~` `, ~`E. Coli`, ~Safe, ~Total,
  "Positive Test", ".95 * .045 = 0.04275", ".01 * .955 = 0.00955", "0.05230",
  "Negative Test", ".05 * .045 = 0.00225", ".99 * .955 = 0.94545", "0.94770",
  "Total", "0.04500", "0.95500", "1.00000"
) %>%
  flextable::flextable() %>%
  flextable::theme_box() %>%
  flextable::align(align = "center", part = "header") %>%
  flextable::align(j = 2:4, align = "right") %>%
  flextable::bg(i = 1, j = c(2, 4), bg = "lightgoldenrod") %>%
  flextable::autofit()
```

## Bayesian Inference

Bayesian inference extends the logic of Bayes' Theorem by replacing the prior probability *estimate* that $\theta$ is true with a prior probability *distribution* that $\theta$ is true. Rather than saying, "I am *x*% certain $\theta$ is true," you say "I believe the probability that $\theta$ is true is somewhere in a range that has maximum likelihood at *x*%".

$$
f(\theta | D) = \frac{f(D|\theta) f(\theta)}{\int_\Theta f(D|\theta) f(\theta) d\theta}
$$

This formula expresses the posterior distribution of $\theta$ as a function of the prior distribution and new information. Let $\Pi(\theta)$ be the prior probability function. $\Pi(\theta)$ has a PMF or PDF $f(\theta)$, and a set of conditional distributions, $\{P=f(D|\theta), \theta \in \Omega\}$, called the *generative model*. $f_\theta(D)$ is the *likelihood* of observing $D$ given $\theta$. The *posterior* probability distribution of $\theta$, conditioned on the observance of $D$, is the joint distribution of $D$ and $\theta$ (aka *joint density*, the product of the likelihood and the prior) divided by the *marginal distribution* of $D$ (aka *marginal density* or *prior predictive distribution*). For discrete cases, replace the integral with a sum. The numerator makes the posterior proportional to the prior. The denominator is a normalizing constant that scales the likelihood into a proper density function (whose values sum to 1).

It is easier to see how observed evidence shifts the probabilities of the priors into their posterior probabilities by working with discrete priors first. From there it is straight-forward to grasp the more abstract case of continuous prior and posterior distributions.

## Bayes Factors

The Bayes Factor (BF) is a measure of the relative evidence of one model over another. Take another look at Bayes' formula:

$$P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}.$$

Suppose you want to compare how two models explain and observed data outcome, $D$. Model $M_1:f_1(D|\theta_1)$ says the observed data $D$ was produced by a generative model with pdf $f_1$ parameterized by $\theta_2$. Model $M_2:f_2(D|\theta_2)$ says it was produced by a generative model with pdf $f_2$ parameterized by $\theta_2$. In each model you specify a prior probability distribution for the parameter

If you take the ratio of the posterior probabilities, the posterior odds, the $P(D)$ terms cancel and you have

$$\frac{P(\theta_1|D)}{P(\theta_2|D)} = \frac{P(D|\theta_1)}{P(D|\theta_2)} \cdot \frac{P(\theta_1)}{P(\theta_2)}$$

The posterior odds equals the ratio of the likelihoods multiplied by the prior odds. That likelihood ratio is the Bayes Factor (BF). Rearranging, BF is the odds ratio of the posterior and prior odds.

$$BF = \frac{P(D|\theta_1)}{P(D|\theta_2)} = \mathrm{\frac{Posterior Odds}{Prior Odds}}$$

Return to the example of observing $D$ = 7 ones and 3 zeros. You can compare an hypothesized $\theta$ of .5 to a completely agnostic model where $\theta$ is uniform over [0, 1]. The likelihood of observing $D$ when $\theta$ = .5 is $P(D|\theta_1) = 5^7(1-.5)^3$ = `r scales::number(dbinom(7, 10, .5), accuracy = .001)`. The likelihood of observing $D$ where $\theta$ is uniform on [0, 1] is $P(D|\theta_2) = \int_0^1 \binom{10}{3}q^7(1-q)^3dq$

```{r}
.5^1 * .5^1
dbinom(1, 1, .5)
dbinom(11, 11, .5)
beta(11, 11)
```



with a uniform Beta(1, 1) prior (i.e., complete agnosticism). 


The Bayes factor at $\theta$ = .7 quantifies how much the odds of H0: $\theta$ = .7 over H1: $\hat{\theta}$ = .7.

```{r}
prior <- function(theta, alpha, beta) {
  (1 / beta(alpha, beta)) * theta^(alpha-1) * (1-theta)^(beta-1)
}
posterior <- function(theta, alpha, beta, a, b) {
  (1 / beta(alpha + a, beta + b)) * theta^(alpha-1+a) * (1-theta)^(beta-1+b)
}

prior(.5, 115, 85) 
posterior(.5, 1, 1, 10, 10)

posterior(.5, 1, 1, 10, 10) / prior(.5, 1, 1) 

1 / beta(115, 85)

# Posterior Distribution 
1/beta(1+10, 1+10) * .5^(1-1+10) * (1-.5)^(1-1+10)
dbeta(.5, 11, 11)

# Prior Beta Distributions
1/beta(1, 1) * .5^(1-1) * (1-.5)^(1-1)
dbeta(.5, 1, 1)


dbeta(.5, 115, 85)
```


The Bayes factor measures how much your prior belief is altered by the evidence. It is the ratio of the likelihoods at some hypothesized value before and after observing the data. In this case, our confidence increased by a factor of...

```{r collapse=TRUE}
theta <- 0.5

alpha <- 1
beta <- 1
a <- 10
b <- 10

(prior_likelihood <- (1 / beta(alpha, beta)) * theta^(alpha-1) * (1-theta)^(beta-1))
(posterior_likelihood <- (1 / beta(alpha + a, beta + b)) * theta^(alpha-1+a) * (1-theta)^(beta-1+b))
(bayes_factor <- posterior_likelihood / prior_likelihood)

# 3.7 on alpha = beta = 1
# 1.91 on alpha = beta = 4
```

## Gamma Poisson

The gamma and Poisson distributions are used to model count data. Consider the following counts of weekday sandwich sales. What is the expected value of sales? 

```{r}
y <- c(50, 65, 72, 63, 70)
```

Count data have a Poisson distribution, $y_i|\lambda \sim Pois(\lambda)$, with expected value $\lambda$ and PMF $f(y_i | \lambda) = e^{-\lambda}\frac{\lambda^{y_i}}{y_i!}$. Using Bayes' Theorem, the posterior distribution of $\lambda$ given evidence $\textbf{y}$ is the joint likelihood of $\lambda$ and $\textbf{y}$ divided by the likelihood of $\textbf{y}$.

$$
f(\lambda |\textbf{y}) = \frac{f(\mathbf{y}|\lambda) f(\lambda)}{\int_\Lambda f(\mathbf{y}|\lambda) f(\lambda) d\lambda}
$$

The conditional likelihood, $f(\textbf{y}|\lambda)$, is the sum-product of the Poisson distribution PMF.

$$
\begin{align}
f(\textbf{y}|\lambda) = f(y_i,\ldots, y_n | \lambda) &= \prod_i f(y_i | \lambda) \\
&= \prod_i e^{-\lambda}\frac{\lambda^{y_i}}{y_i!}
\end{align}
$$

The prior distribution, $f(\lambda)$, should take on only positive values. Model it with the [gamma distribution](https://mpfoley73.github.io/probability/random-variables-and-distributions.html#gamma), $\lambda|a,b = \mathrm{Gamma}(a,b)$.

$$
f(\lambda) = f(\lambda | a,b) = \frac{b^a \lambda^{a-1} e^{-b\lambda}}{\Gamma(a)}
$$

where $\Gamma$ is the gamma function^[The gamma function is a generic function, just like sin, cos, etc., and is a kind of generalized factorial.]. Substituting into Bayes' Theorem and simplifying, you have this nightmare:

$$
f(\lambda |\textbf{y}) = \frac{\lambda^{a + \sum_i y_i-1}e^{-(b+n)\lambda}}{\int_0^\infty \lambda^{a + \sum_i y_i-1}e^{-(b+n)\lambda} d\lambda}
$$

However, there is good news. The integration in the denominator removes the dependence on $\lambda$, so $f(\lambda |\textbf{y}, a, b)$ is proportional to the numerator up to a constant.

$$
f(\lambda |\textbf{y}) \propto f(\textbf{y} | \lambda) f(\lambda)
$$

Since $f(\lambda |\textbf{y})$ is a PMF, it integrates (sums) to 1 and you can always figure out the constant later. What makes this good news is that this has the form of the PDF of the gamma distribution. 

$$
\begin{equation} 
\lambda | \textbf{y}, a, b \sim \mathrm{Gamma}(a + \sum_i y_i, b + n)
(\#eq:gamma-posterior)
\end{equation} 
$$

Equation \@ref(eq:gamma-posterior) is the posterior distribution of $\lambda$. We combined a gamma prior with the Poisson likelihood of evidence, $\textbf{y}$, to produce a gamma posterior. We call priors that produce posteriors of the same form, *conjugate priors* for the likelihood. Conjugate priors are popular because of their computational convenience.

Return to the sandwich sales data. We need values to plug into Equation \@ref(eq:gamma-posterior). For the gamma distribution, $E(X) = a / b$ and $\mathrm{Var}(X) = a / b^2$. You might guess from intuition that mean daily sandwich sales are 70 +/- 5. Interpreting +/- 5 as a 95% CI and using the rule of thumb that a 95% CI is 2 SD, $\mathrm{Var} = (2.5)^2 = 6.25$. Solve for $a = 784$ and $b = 11.2$. We also have $\sum_i y_i = 320$ and $n = 5$.

$$
\lambda | \textbf{y}, a, b \sim \mathrm{Gamma}(784 + 320, 11.2 + 5) \sim \mathrm{Gamma}(1104, 16.2)
$$

The posterior $E(y) = 1104 / 16.2 = 68.1$ and $\mathrm{Var}(y) = 1104 / 16.2^2 = 4.2$. Use the gamma distribution function to get the posterior 95% _credible_ interval.

```{r collapse=TRUE}
# Prior distribution
qgamma(p = c(.025, .975), 784, 11.2)

# Posterior distribution
qgamma(p = c(.025, .975), 784 + 320, 11.2 + 5)
```

Whereas the prior expected mean daily sandwich sales was 70 (95% CI: 65, 75), the posterior is 68 (95% CI: 64, 72). Compare this to classical statistics: $E(y) = \bar{y} = 64$, $SE = \sqrt{\bar{y} / n} = 3.6$:

```{r collapse=TRUE}
# Classical estimate
qnorm(p = c(.025, .975), 64, 3.6)
```

You might think that the reasonable Bayesian outcome was predicated on good $a$ and $b$ priors, but no. Suppose $a = .01$ and $b = .01$. The posterior is still reasonable.

```{r collapse=TRUE}
# Informal prior
qgamma(p = c(.025, .975), .01 + 320, .01 + 5)
```

```{r}
tibble(
  lambda = seq(0, 80, .1),
  `Gamma(.01, .01)` = dgamma(lambda, .01, .01),
  `Gamma(.01 + 320, .01 + 5)` = dgamma(lambda, .01 + 320, .01 + 5),
  `Gamma(784, 11.2)` = dgamma(lambda, 784, 11.2),
  `Gamma(784 + 320, 11.2 + 5)` = dgamma(lambda, 784 + 320, 11.2 + 5)
) %>%
  pivot_longer(-lambda) %>%
  mutate(name = fct_inorder(name)) %>%
  mutate(prior = if_else(str_detect(name, "\\+"), "posterior", "prior")) %>%
  ggplot(aes(x = lambda, y = value, color = name, linetype = prior)) +
  geom_line(linewidth = 1) +
  scale_color_manual(values = c(rep("seagreen", 2), rep("firebrick", 2))) +
  labs(color = NULL, linetype = NULL, y = "density",
       title = "Posteriors from Conservative and Informed Priors.")
```

The Bayesian posterior approaches the classical $\bar{y}$ with increasing sample size.

$$
E(\lambda|\textbf{y}, a, b) = \frac{a + \sum_i y_i}{b + n} = \frac{a + n \bar{y}}{b + n}
$$

Taking the limit, $\lim_{n \rightarrow \infty} E(\lambda|\textbf{y}, a, b) = \bar{y}$.

The *central* credible interval is the standard Bayesian credible interval. But when the posterior distribution is not perfectly symmetric, the shortest credible interval capturing x% of the distribution might have different endpoints. Our example has a pretty symmetric distribution, but let's calculate the *highest density region (HDR)* anyway.

```{r collapse=TRUE}
pp <- seq(0.01, .99, by = .0001)
x <- map_dbl(pp, ~qgamma(., 784 + 320, 11.2 + 5))
hdrcde::hdr(x, prob = 95)$hdr
```

The posterior predictive distribution of a predicted value, $\tilde{y}$ is

$$
f(\tilde{y} | x) = \int f(\tilde{y}|\lambda) f(\lambda | \textbf{y}) d\lambda
$$

Our sandwich example has a well defined functional solution: the expected value from $\mathrm{Gamma}(1104, 16.2)$ is $1104/16.2 = 68$. Had we not known this, we could have simulated posterior values (Monte Carlo simulation) and calculated the mean and variance. The procedure is to take a random sample of perhaps 1,000 $\lambda$ values from the gamma posterior distribution, then for each $\lambda$ draw a single random $\tilde{y}$ from the Poisson distribution. 

```{r collapse=TRUE}
a <- 1104 
b <- 16.2 

set.seed(1234)

# random sample of lambdas, and a single random y_tilde for each lambda
lambda_r <- rgamma(1000, a, b)
y_tilde <- rpois(1000, lambda_r)

# posterior predictive distribution
mean(y_tilde)
quantile(y_tilde, c(.025, .975))
```

So on any given day, the predicted value of sandwich sales is `r round(mean(y_tilde), 1)` with 95% prediction interval `r quantile(y_tilde, c(.025, .975)) %>% comma(.1) %>% paste(collapse = ", ")`. The probability of exceeding 80 sandwiches, $P(\tilde{y} > 80 | \textbf{y})$, is `mean(y_tilde > 80)` = `r mean(y_tilde > 80) %>% percent(.1)`, and 99% of the time, sandwich sales will be less than `quantile(y_tilde, .99)` = `r quantile(y_tilde, .99)`.

You can also predict individual weekdays. Suppose you take a $\mathrm{Gamma}(700, 10)$ distribution as your prior.

```{r}
day_tbl <- tibble(
  dow = fct_inorder(c("Mon", "Tue", "Wed", "Thu", "Fri")),
  d = c(50, 65, 72, 63, 70)
) %>%
  mutate(
    post_a = 700 + d,
    post_b = 10 + 1,
    post_mean = post_a / post_b,
    post_lci = qgamma(.025, post_a, post_b),
    post_uci = qgamma(.975, post_a, post_b)
  )

day_tbl
```

What is the probability that Mon sales are less than Tue?

```{r collapse=TRUE}
set.seed(123)
lambda_r_mon <- rgamma(1000, 750, 11)
lambda_r_tue <- rgamma(1000, 765, 11)

# posterior probability 
mean(lambda_r_mon < lambda_r_tue)
```

Which day of the week has the highest sandwich sales?

```{r collapse=TRUE}
set.seed(12345)

lambda_r <- tibble(
  r_mon = rgamma(1000, 750, 11),
  r_tue = rgamma(1000, 765, 11),
  r_wed = rgamma(1000, 772, 11),
  r_thu = rgamma(1000, 763, 11),
  r_fri = rgamma(1000, 770, 11),
  r_dow = pmap(list(r_mon, r_tue, r_wed, r_thu, r_fri), 
                   function(m, t, w, r, f) c(m, t, w, r, f)),
  max_dow_idx = map_dbl(r_dow, ~which.max(.)),
  max_dow = map_chr(max_dow_idx, ~c("Mon", "Tue", "Wed", "Thu", "Fri")[.])
)

lambda_r %>% janitor::tabyl(max_dow)
```

Use the Deviance Information Criterion (DIC) to evaluate whether the day means differ from each other. 

$$
DIC = p_D + \overline{D(\theta)}
$$

where $p_D = \overline{D(\theta)} - D(\hat{\theta})$ and $D(\theta) = -2 \log (f(y|\theta)) + C$.

Evaluate $\overline{D(\theta)}$ by producing samples from each distribution and evaluating the likelihoods of the data based on each realization and taking the mean of -2 log-likelihood.

```{r}
# Reset the example. Sandwich counts by dow.
y <- c(50, 65, 72, 63, 70)

# Priors
a <- .01
b <- .01

# Posteriors
post <- list(
  rgamma(10^3, a+y[1], b+length(y[1])),
  rgamma(10^3, a+y[2], b+length(y[2])),
  rgamma(10^3, a+y[3], b+length(y[3])),
  rgamma(10^3, a+y[4], b+length(y[4])),
  rgamma(10^3, a+y[5], b+length(y[5]))
)

# -2 * Mean log-likelihood
ll <- 
  dpois(y[1], post[[1]], log = TRUE) +
  dpois(y[2], post[[2]], log = TRUE) +
  dpois(y[3], post[[3]], log = TRUE) +
  dpois(y[4], post[[4]], log = TRUE) +
  dpois(y[5], post[[5]], log = TRUE)
(mean_D <- mean(-2 * ll))

# D(theta-bar) is the likelihood of the data based on the posterior means of p.
(D_mean <- -2 * (
  dpois(y[1], (a+y[1]) / (b+length(y[1])), log = TRUE) +
  dpois(y[2], (a+y[2]) / (b+length(y[2])), log = TRUE) +
  dpois(y[3], (a+y[3]) / (b+length(y[3])), log = TRUE) +
  dpois(y[4], (a+y[4]) / (b+length(y[4])), log = TRUE) +
  dpois(y[5], (a+y[5]) / (b+length(y[5])), log = TRUE) 
))

#p_D and DIC from equation
(p_D <- mean_D - D_mean)
(DIC <- p_D + mean_D)

# Repeat these steps for a single model of all groups
post_group <- rgamma(10^3, a+sum(y), b+length(y))
ll_group <- 
  dpois(y[1], post_group, log = TRUE) +
  dpois(y[2], post_group, log = TRUE) +
  dpois(y[3], post_group, log = TRUE) +
  dpois(y[4], post_group, log = TRUE) +
  dpois(y[5], post_group, log = TRUE)
(mean_D_group <- mean(-2 * ll_group))
(D_mean_group <- -2 * (
  dpois(y[1], (a+sum(y)) / (b+length(y)), log = TRUE) +
  dpois(y[2], (a+sum(y)) / (b+length(y)), log = TRUE) +
  dpois(y[3], (a+sum(y)) / (b+length(y)), log = TRUE) +
  dpois(y[4], (a+sum(y)) / (b+length(y)), log = TRUE) +
  dpois(y[5], (a+sum(y)) / (b+length(y)), log = TRUE)
))
(p_D_group <- mean_D_group - D_mean_group)
(DIC_group <- p_D_group + mean_D_group)
```

The DIC for the weekday specific model is `r DIC` and for the one common group model it is `r DIC_group`. The DIC for one common group model is smaller, so we do not have enough statistical evidence for two groups. 

## Normal

### Population Estimate

Suppose you have a sample, $\textbf{y}$, from a normally distribution population of unknown mean and precision, $\mu$ and $\tau$: $y_i|\mu, \tau \sim N(\mu, \tau)$.^[In Bayesian statistics, the normal distribution is parameterized with the inverse of variance, called the *precision*, $\tau = 1 / \sigma^2$.] Assume a normal prior for $\mu \sim N(\mu_0, \tau_0)$, and a gamma prior for $\tau \sim \text{Gamma}(a, b)$ since it takes only positive values. The PDF for $y_i$ is $f(y_i | \mu, \tau) = \frac{\tau^{.5}}{\sqrt{2\pi}} \exp\left(-\frac{\tau}{2} (y_i - \mu)^2 \right)$. We'll derive posterior distributions for $\mu$ and $\tau$ separately. 

Using Bayes' Theorem, the posterior distribution of $\mu|y$ is the joint likelihood of $y$ and $\mu$ divided by the likelihood of $y$, 

$$
f(\mu|y) = \frac{f(y|\mu)f(\mu)}{\int_\mu f(y|\mu)f(\mu)d\mu}
$$

The conditional likelihood, $f(\mu|y)$, is the sum-product of the normal distribution PDF. We can take $\tau$ as given initially.

$$
\begin{align}
f(y|\mu) &= \prod_i \frac{\tau^{(1/2)}}{\sqrt{2\pi}} \exp\left(-\frac{\tau}{2} (y_i - \mu)^2 \right) \\
&\propto \prod_i \exp\left(-\frac{\tau}{2} (y_i - \mu)^2 \right) \\
&\propto \exp \left( -\frac{\tau}{2} \sum_i(y_i - \mu)^2 \right)
\end{align}
$$

The prior PDF for $\mu$ is the normal distribution. Again we take $\tau$ as given initially.

$$
\begin{align}
f(\mu) &= \frac{\tau_0^{1/2}}{\sqrt{2\pi}} \exp\left(-\frac{\tau_0}{2} (\mu - \mu_0)^2 \right) \\
&\propto \exp\left(-\frac{\tau_0}{2} (\mu - \mu_0)^2 \right)  
\end{align}
$$

Substitute into Bayes' Theorem. Since we are working with proportions, we can throw out the denominator and say $f(\mu|y) \propto f(y|\mu)f(\mu)$. Plugging in and solving, we get

$$
\begin{equation}
\mu|y \sim N\left(\frac{n\tau\bar{y} + \tau_0\mu_0}{n\tau + \tau_0}, n\tau + \tau_0 \right)
(\#eq:mu-posterior)
\end{equation}
$$

Using Bayes' Theorem, the posterior distribution of $\tau|y$ is the joint likelihood of $y$ and $\tau$ divided by the likelihood of $y$, 

$$
f(\tau|y) = \frac{f(y|\tau)f(\tau)}{\int_\tau f(y|\mu)f(\tau)d\tau}
$$

The conditional likelihood, $f(\tau|y)$, is the sum-product of the gamma distribution PDF. This time we take $\mu$ as given.

$$
\begin{align}
f(y|\tau) &= \prod_i \frac{\tau^{(1/2)}}{\sqrt{2\pi}} \exp\left(-\frac{\tau}{2} (y_i - \mu)^2 \right) \\
&= \frac{\tau^{n/2}}{(2\pi)^{n/2}} \exp\left(-\frac{\tau}{2} \sum_i (y_i - \mu)^2 \right) \\
&\propto \tau^{n/2} \exp \left( -\frac{\tau}{2} \sum_i(y_i - \mu)^2 \right)
\end{align}
$$

The prior PDF for $\tau$ is the gamma distribution. Pull the constant out to work with proportionality.

$$
\begin{align}
f(\tau) &= \frac{b^a \tau^{a-1} e^{-b\tau}}{\Gamma(a)} \\
&\propto \tau^{a-1}e^{-b\tau}
\end{align}
$$

Substitute into Bayes' Theorem. Since we are working with proportions, we can throw out the denominator and say $f(\tau|y) \propto f(y|\tau)f(\tau)$. Plugging in and solving, we get

$$
\begin{equation}
\tau|y \sim \text{Gamma}\left(a + n/2, b + \frac{1}{2} \sum_i(y_i - \mu)^2 \right)
(\#eq:tau-posterior)
\end{equation}
$$

We have $y_i|\mu,\tau \sim N(\mu,\tau)$ with conjugate priors $\mu \sim N(\mu_0, \tau_0)$ and $\tau \sim \text{Gamma}(a,b)$ and conditional posterior distributions shown in Eqns \@ref(eq:mu-posterior) and \@ref(eq:tau-posterior). Returning to Eqn \@ref(eq:mu-posterior), you can see how $E[\mu] \rightarrow \bar{y}$ as the sample size grows. Below, the terms divided by $n$ disappear, leaving just $\bar{y}$.

$$
\begin{align}
E[\mu|\tau, y] &= \frac{n\tau\bar{y} + \tau_0\mu_0}{n\tau + \tau_0} \\
&= \frac{\bar{y}\tau + \mu_0\tau_0/n}{\tau + \tau_0/n} \\
&\sim \bar{y}
\end{align}
$$

The posterior mean estimator of $\tau$ is the ratio of the posterior gamma distribution parameters. Again, as the sample size increases, terms divided by $n$ disappear.

$$
\begin{align}
E[\tau|\mu,y] &= \frac{a + n/2}{b + \frac{1}{2} \sum_i(y_i - \mu)^2} \\ 
&= \frac{2a/n + 1}{2b/n + \sum_i(y_i - \mu)^2 / n} \\
&\sim \frac{1}{\sum_i(y_i - \mu)^2}
\end{align}
$$

The problem here is that you never know $\mu$ or $\tau$, so you cannot use the posterior formulas directly. Instead, you need to use sampling. In particular, you use the *Gibbs sampler*. Set $\mu$ and $\tau$ to some initial values and use the posterior equations to estimate new values for $\mu$ and $\tau$, then repeat. This is called Markov Chain Monte Carlo (MCMC) simulation because you are chaining the simulations. The method of sampling from a conditional posterior is called the Gibbs sampler.

Let's apply this using anthropological data collected by Nancy Howell of human height. 

```{r}
# downloaded this from 
# https://github.com/rmcelreath/rethinking/blob/master/data/Howell1.csv
howell <- read_delim("input/Howell1.csv", delim = ";", show_col_types = FALSE) %>%
  filter(age >= 18) %>%
  mutate(male = factor(male, labels = c("women", "men")))

howell %>%
  ggplot(aes(sample = height, color = male)) +
  stat_qq() +
  geom_qq_line()
```

From prior knowledge, we know average human height is about $175 \pm 10$ cm. Using the $\pm$ = 2SD, the variance $5^2$. Use vague priors of $\mu \sim N(\mu_0 = 175, \tau_0 = 1/5^2)$ and $\tau = \sim \text{Gamma}(a = .01, b = .01)$. Start by assigning starting values, $\mu*$ and $\tau*$). Given $\tau = \tau*$, sample a new value of $\mu*$ from the normal distribution. Given $\mu = \mu*$, sample a new value of $\tau*$ from the gamma distribution. Then repeat.

```{r collapse=TRUE}
gibbs_normal <- function(y, mu_0, tau_0, a, b, n_iter){
  n <- length(y)
  y_mean <- mean(y)
  
  mu_sample <- tau_sample <- numeric(n_iter)
  
  # starting values
  mu_sample[1] <- mean(y)
  tau_sample[1] <- 1 / var(y)

  # Gibbs sampler
  for(i in 2:n_iter){
    # mu
    tau <- tau_sample[i-1]
    mean_mu <- (n * y_mean * tau + mu_0 * tau_0) / (n * tau + tau_0)
    precision_mu <- n * tau + tau_0
    mu_sample[i] <- rnorm(1, mean_mu, 1 / sqrt(precision_mu))
    # tau
    mu <- mu_sample[i-1]
    tau_sample[i] <- rgamma(1, a + n/2, b + .5 * sum((y - mu)^2))
  }
  
  return(list(mu = mu_sample, tau = tau_sample))
}

set.seed(12345)
sample_m <- gibbs_normal(howell[howell$male == "men",]$height, 175, 1/5^2, .01, .01, 10^3)
sample_w <- gibbs_normal(howell[howell$male == "women",]$height, 175, 1/5^2, .01, .01, 10^3)

# posterior mu
mean(sample_m$mu); quantile(sample_m$mu, c(.025, .975))
mean(sample_w$mu); quantile(sample_w$mu, c(.025, .975))

# posterior probability that men are taller than women on average
mean(sample_m$mu > sample_w$mu)

# posterior probability that a random man is taller than a random woman
tilde_m <- rnorm(10^3, mean(sample_m$mu), sqrt(1/mean(sample_m$tau)))
tilde_w <- rnorm(10^3, mean(sample_w$mu), sqrt(1/mean(sample_w$tau)))
mean(tilde_m > tilde_w)

tibble(
  iter = rep(1:10^3, 2),
  sex = c(rep("men", 10^3), rep("women", 10^3)),
  mu = c(sample_m$mu, sample_w$mu),
  tau = c(sample_m$tau, sample_w$tau),
) %>%
  pivot_longer(cols = c(mu, tau)) %>% 
  ggplot(aes(x = value, color = sex)) + 
  geom_density() + 
  facet_wrap(facets = vars(name), scales = "free") +
  labs(title = "Posterior Distributions", x = NULL, color = NULL)

tibble(
  iter = rep(1:10^3, 2),
  sex = c(rep("men", 10^3), rep("women", 10^3)),
  tilde = c(tilde_m, tilde_w)
) %>%
  ggplot(aes(x = tilde, color = sex)) + 
  geom_density() + 
  labs(title = "Posterior Predictive Distributions", x = NULL, color = NULL)
```

It may take some time to converge on a solution. This convergence is called *burn-in* and is often discarded when describing the posterior. *Slow mixing* may occur if there is high autocorrelation in the Gibbs sample, resulting in slow exploration of the sample space of the posterior.

### Regression

Returning to the Howell data, suppose you want to fit a linear model, $\text{Weight}_i = a + b \text{Height} + \epsilon_i$. In Bayesian regression, this is expressed as $\text{Weight} | \mu_i \sim N(\mu_i, \tau)$ where $\mu_i = a + b\text{Height}$. You can construct a Gibbs sampler to estimate the model, but there is already a package for that, MCMCglmm.

```{r warning=FALSE, message=FALSE}
library(MCMCglmm)

set.seed(12345)

mdl_1 <- MCMCglmm(
  weight ~ height, 
  data = howell,
  family = "gaussian",
  nitt = 11000, # iterations
  burnin = 1000, # burn-in period to throw out
  thin = 10, 
  # could omit this prior since it is non-informative
  prior = list(B = list(mu = c(0, 0), V = c(100^2, 100^2)*diag(2))),
  verbose = FALSE
)

summary(mdl_1)

# Probability beta is >0
mean(mdl_1$Sol[, 2] > 0)

# Posterior inference.
new_data <- tibble(height = seq(130, 185, .1), weight = 0)

cred_intvl <- predict(mdl_1, newdata = , type = "response", interval = "confidence")

pred_intvl <- predict(mdl_1, newdata = , type = "response", interval = "prediction")

howell %>% 
  bind_cols(cred_intvl, pred_intvl, .name_repair = "unique") %>% 
  ggplot(aes(x = height)) +
  geom_ribbon(aes(ymin = `lwr...9`, ymax = `upr...10`), fill = "lightgoldenrod", alpha = .5) +
  geom_ribbon(aes(ymin = `lwr...6`, ymax = `upr...7`), fill = "goldenrod", alpha = .5) +
  geom_line(aes(y = `fit...5`), color = "goldenrod", linewidth = 1) +
  geom_point(aes(y = weight)) +
  labs(y = "weight", title = "95% CI and PI.")
```

You can extend this to multivariate models.

```{r warning=FALSE}
set.seed(12345)

mdl_2 <- MCMCglmm(
  weight ~ height*male, 
  data = howell,
  family = "gaussian",
  nitt = 11000, # iterations
  burnin = 1000, # burn-in period to throw out
  thin = 10, 
  #prior = list(B = list(mu = c(0, 0), V = c(100^2, 100^2)*diag(2))), 
  verbose = FALSE
)
```

Bayesian statistics has its analog to Akaike's Information Criterion (AIC) called Deviance Information Criterion (DIC).

$$
DIC = p_D + \overline{D(\theta)}
$$

where $p_D = \overline{D(\theta)} - D(\hat{\theta})$ and $D(\theta) = -2 \log (f(y|\theta)) + C$. The value of DIC has no real meaning, but for comparison purposes, lower is better. A difference of at least 3 is considered sufficient evidence to choose one model over another. Here, `mdl_1$DIC` = `r mdl_1$DIC` and `mdl_2$DIC` = `r mdl_2$DIC`. The first model is better, so conclude that there is no statistical evidence that the correlation between weight and height depends on sex ($\Delta$DIC = `r mdl_2$DIC - mdl_1$DIC`).

```{r}
mdl_3 <- MCMCglmm(
  weight ~ age, 
  data = howell,
  family = "gaussian",
  nitt = 11000, # iterations
  burnin = 1000, # burn-in period to throw out
  thin = 10, 
  #prior = list(B = list(mu = c(0, 0), V = c(100^2, 100^2)*diag(2))), 
  verbose = FALSE
)
summary(mdl_3)
mdl_1$DIC
mdl_3$DIC
```

### Cluster Analysis

You can use Bayes for mixtures. Suppose you have a mixture of $z_i \in [1, 2, 3]$ types of grains with mean diameters of 3.8, 5, and 8mm. Given a grain of diameter $x_i$, what is its class, $z_i$?

```{r}
cinderella <- tibble(
  z = c(rep(1, 200), rep(2, 100), rep(3, 300)),
  x = c(rnorm(200, 3.8, .25), rnorm(100, 5, .5), rnorm(300, 8, 1))
)
```

You might start with the assumption that $(x_i|z_i, \mu_{z_i}, \tau_{z_i}) \sim N(\mu_{z_i}, \tau_{z_i})$ and assign conjugate priors $\mu_k \sim N(\mu_{k0}, \tau_{k0})$ and $\tau_k \sim \text{Gamma}(a_k, b_k)$ for $k = 1, \ldots, 3$. To estimate the class probabilities of each grain, you need to start with a prior. $Pr(z_i = k) = 1/K = 1/3$ is a good prior.

The problem becomes

$$
\begin{align}
Pr(z_i = k|x_i, \mu_k, \tau_k) &= \frac{f(x_i|z_i = k, \mu_k, \tau_k) Pr(z_i = k)}{\sum_k f(x_i|z_i = k, \mu_k, \tau_k) Pr(z_i = k)} \\
&= \frac{f(x_i|z_i = k, \mu_k, \tau_k)}{\sum_k f(x_i|z_i = k, \mu_k, \tau_k)}
\end{align}
$$

`NMixMCMC()` assigns initial values for $\mu s$, $\tau s$ and $z_i s$. Given $\mu s$ and $\tau s$, it samples $z_i s$ from the assumed distribution. For each group, $k$, sample $\mu_k$ and then $\tau_k$ from the conjugate conditional posterior distribution. Then repeat until convergence.

```{r collapse=TRUE}
library(mixAK)

mdl_mix <- NMixMCMC(
  y0 = cinderella$x,
  nMCMC = c(burn = 1000, keep = 1000, thin = 1, info = 100),
  prior = list(priorK = "fixed", Kmax = 3)
)

# posterior means mu
c(mdl_mix[[1]]$poster.mean.mu * sd(cinderella$x)) + mean(cinderella$x)

# posterior means for SD
sqrt(c(unlist(mdl_mix[[1]]$poster.mean.Sigma)))*sd(cinderella$x)

# estimated class frequency distribution
round(mdl_mix[[1]]$poster.mean.w, 2)

# estimated probability per class of first few seeds
mdl_mix[[1]]$poster.comp.prob_u %>% head()

# Marginal plot
NMixPredDensMarg(mdl_mix[[1]], lgrid = 150) %>% plot()
```

In this case we new how many classes were in the data. If that was unknown, you could use reversible-jump MCMC which produces a posterior distribution for the number of components.

## Proportions and the Beta Binomial

Suppose $y = 35$ of $n = 50$ seeds germinate within 72hrs. What is the expected germination probability, $p$, of a single seed? Seed germination can be modeled as a Bernoulli data generating process where events occur with probability, $p$, and the observed $y = \sum_i^n y_i$ is the sum of $n$ Bernoulli trials. The question becomes what $p$ was used in the Bernoulli function?

The classical approach is to construct a 95% CI around $p$ with a [one-sample proportion test](https://mpfoley73.github.io/statistics/one-sample-proportion-z-test.html).

```{r}
y <- 35
n <- 50

prop.test(y, n)
```

The Bayesian approach is to posit an expected distribution of $p$ *prior* to observing the data, then update the distribution based on the relative *likelihood* of observing the data given the values in the distribution. The likelihood of $y$ events in $n$ trials follows the binomial distribution, $y|p \sim \text{Bin}(n, p)$.

$$
f(y|p) = {n \choose y} p^y (1-p)^{n-y}
$$

With no prior expectations, take a uniform prior - all values of $p$ are equally likely. To get a feel for the approach, try this with *discrete* $p$ values first.

```{r collapse=TRUE, warning=FALSE}
# Explore the parameter space [0,1] in discrete .01 increments.
p <- seq(0, 1, by = 0.01)

# Prior distribution is uniform, 1/101 for all p's.
prior <- rep(1/length(p), length(p))

# Binomial likelihood of each p given 35/50 successes.
likelihood <- dbinom(35, 50, p)

# Bayes' Theorem: posterior = joint density / marginal density.
joint_density <- likelihood * prior
marginal_density <- sum(joint_density)
posterior <- joint_density / marginal_density

# 95% credible interval
(pi <- sum(posterior * p) / sum(posterior))
(ci <- p[c(min(which(cumsum(posterior) > .025)),
          max(which(cumsum(posterior) < .975)))])

tibble(p, prior, update = likelihood / marginal_density / 101, posterior) %>%
  pivot_longer(c(prior, update, posterior)) %>%
  mutate(name = factor(name, levels = c("prior", "update", "posterior"))) %>%
  ggplot(aes(x = p)) +
  geom_line(aes(y = value, color = name)) +
  geom_area(data = tibble(p, posterior, y = if_else(p <= ci[1], posterior, NA_real_)),
            aes(y = y), fill = "#619CFF") +
  geom_area(data = tibble(p, posterior, y = if_else(p >= ci[2], posterior, NA_real_)),
            aes(y = y), fill = "#619CFF") +
  geom_vline(aes(xintercept = pi), color = "#619CFF") +
  scale_x_continuous(breaks = round(c(seq(0, 1, 1), ci, pi), 4)) +
  labs(y = "density", color = NULL, title = "From Prior to Posterior",
       subtitle = "Data: y=35, n=50. Posterior equals update.")
```

This was good, but we could have done better. The discrete values for the uniform prior limited the precision of the 95% CI. The better path is to model continuous values with the [beta](https://mpfoley73.github.io/probability/random-variables-and-distributions.html#beta) distribution, $p \sim \text{Beta}(a, b)$. The PDF of the beta distribution is

$$
\begin{align}
f(p) &= \frac{1}{\text{B}(a,b)} p^{a-1} (1-p)^{b-1} \\
&\propto p^{a-1} (1 - p)^{b -1}
(\#eq:beta-prior)
\end{align}
$$

where $a$ is the success count, $b$ the failure count, and $\text{B} = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$ is the beta function. Equation \@ref(eq:beta-prior) is called the beta prior. The beta prior is a *conjugate* prior, meaning the posterior distribution is also a beta distribution, so the $\frac{1}{\text{B}(a,b)}$ cancels out and the proportional form is all you need. The uniform prior distribution would be modeled with $\text{Beta}(1, 1)$.

The *likelihood* of observing $y$ successes in $n$ trials given $p$ follows the binomial PMF. The constant binomial coefficient can be discarded.

$$
\begin{align}
f(y|p) &= {n \choose y} p^y (1-p)^{n-y} \\
&\propto p^y (1-p)^{n-y}
(\#eq:beta-likelihood)
\end{align}
$$

The product of the prior and likelihood is the joint density. The marginal density is the integral of the joint density over all $p$.

$$
\begin{equation}
\int_{p=0}^1f(y|p) f(p) dp = \frac{\text{B}(a + y, b + (n-y))}{\text{B}(a, b)}
(\#eq:beta-marginal)
\end{equation}
$$

Using the proportional forms, the denominator of Bayes' Theorem can be discarded, so the posterior distribution is just the joint density.

$$
\begin{align}
f(p|y) & \propto f(y|p)f(p) \\
& \propto p^y (1-p)^{n-y} p^{a-1} (1 - p)^{b -1} \\
& \propto p^{a+y-1}(1-p)^{b+n-y-1} \\
& \sim \text{Beta}(a + y, b + (n - y))
(\#eq:beta-posterior)
\end{align}
$$

The expected value of the beta distribution is $a / (a + b)$. Updated with the observed data, the expected value is

$$
E(p|y) = \frac{a+y}{(a+y)+(b+(n-y))} = \frac{a + y}{a + b + n} = \frac{a + n \bar{y}}{a + b + n}
$$

As the sample size increases, the $a$ and $b$ prior parameter values become less important and $E(p|y)$ converges on the classical result of $\bar{y}$. The code chunk below repeats the exercise above. This time that we can calculate a continuous 95% CI. Notice also that the marginal density never actually factored into the solution.

```{r collapse=TRUE, warning=FALSE}
# These discrete p's are for illustrating the distributions now.
p <- seq(0, 1, by = 0.001)

# The prior distribution is uniform, Beta(1, 1).
a <- 1
b <- 1
prior <- dbeta(p, a, b)

# Instead of calculating the likelihood, joint density, marginal density, and
# finally the posterior, we can go straight to the posterior.
posterior <- dbeta(p, a + y, b + (n - y))

# 95% credible interval. This time we don't need p - we can go straight to the soln.
(pi <- (a + y) / ((a + y) + (b + (n - y))))
(ci <- qbeta(c(.025, .975), a + y, b + (n - y)))

# Construct the components anyway, just to graph them.
likelihood <- dbinom(y, n, p)
joint_density <- likelihood * prior
marginal_density <- sum(joint_density)
# Trickery to get discretely calculated joint and marginal densities to sum to
# same area as prior and posterior distributions.
update <- likelihood / (sum(likelihood) / sum(posterior))

tibble(p, prior, update, posterior) %>%
  pivot_longer(c(prior, update, posterior)) %>%
  mutate(name = factor(name, levels = c("prior", "update", "posterior"))) %>%
  ggplot(aes(x = p)) +
  geom_line(aes(y = value, color = name)) +
  geom_area(data = tibble(p, posterior, y = if_else(p <= ci[1], posterior, NA_real_)),
            aes(y = y), fill = "#619CFF") +
  geom_area(data = tibble(p, posterior, y = if_else(p >= ci[2], posterior, NA_real_)),
            aes(y = y), fill = "#619CFF") +
  geom_vline(aes(xintercept = pi), color = "#619CFF") +
  scale_x_continuous(breaks = round(c(seq(0, 1, 1), ci, pi), 4)) +
  labs(y = "density", color = NULL, title = glue("From Prior to Posterior, using Beta({a}, {b}) prior."),
       subtitle = "Data: y=35, n=50.")
```

Suppose your prior was better. You had taken a small sample of 10 and 7 seeds had germinated. The corroborating evidence centers on 70% and the credible interval tightens.

```{r echo=FALSE, warning=FALSE}
# These discrete p's are for illustrating the distributions now.
p <- seq(0, 1, by = 0.001)

# The prior distribution is uniform, Beta(1, 1).
a <- 7
b <- 3
prior <- dbeta(p, a, b)

# Instead of calculating the likelihood, joint density, marginal density, and
# finally the posterior, we can go straight to the posterior.
posterior <- dbeta(p, a + y, b + (n - y))

# 95% credible interval. This time we don't need p - we can go straight to the soln.
pi <- (a + y) / ((a + y) + (b + (n - y)))
ci <- qbeta(c(.025, .975), a + y, b + (n - y))

# Construct the components anyway, just to graph them.
likelihood <- dbinom(y, n, p)
joint_density <- likelihood * prior
marginal_density <- sum(joint_density)
# Trickery to get discretely calculated joint and marginal densities to sum to
# same area as prior and posterior distributions.
update <- likelihood / (sum(likelihood) / sum(posterior))

tibble(p, prior, update, posterior) %>%
  pivot_longer(c(prior, update, posterior)) %>%
  mutate(name = factor(name, levels = c("prior", "update", "posterior"))) %>%
  ggplot(aes(x = p)) +
  geom_line(aes(y = value, color = name)) +
  geom_area(data = tibble(p, posterior, y = if_else(p <= ci[1], posterior, NA_real_)),
            aes(y = y), fill = "#619CFF") +
  geom_area(data = tibble(p, posterior, y = if_else(p >= ci[2], posterior, NA_real_)),
            aes(y = y), fill = "#619CFF") +
  geom_vline(aes(xintercept = pi), color = "#619CFF") +
  scale_x_continuous(breaks = round(c(seq(0, 1, 1), ci, pi), 4)) +
  labs(y = "density", color = NULL, title = glue("From Prior to Posterior, using Beta({a}, {b}) prior."),
       subtitle = "Data: y=35, n=50.")
```

## Zero-Inflation and Latent Variables

Suppose 100 persons report the number of days per week they consume alcohol (0 - 7). What is the posterior estimated probability of a person having a drink on any given day? The figure below shows a likely distribution. The number of drinking days will fall into binomial distribution _if_ they drink at all. However, many will be non-drinkers. This phenomena is common and is called zero-inflation.

```{r}
days <- 0:7
respondents <- c(22, 6, 18, 23, 18, 10, 3, 0)

tibble(days, respondents) %>%
  ggplot(aes(x = days, y = respondents)) +
  geom_col() +
  labs(title = "Drinking Days Have Binomial Dist., Except for Zero Days")
```

Use the data to estimate the binomial $p$ parameter. Using the $\text{Beta}(1, 1)$ prior, the posterior would be $\text{Beta}(252, 450)$.

```{r collapse=TRUE}
# Actual drinking days per week
(drinking_days <- sum(days * respondents))

# Possible drinking days per week
(n <- 7 * 100)

# Beta posterior
a <- 1 + drinking_days
b <- 1 + (n - drinking_days)

# Posterior mean and 95% CI.
a / (a + b)
qbeta(c(.025, .975), a, b)
```

Calculate the predicted count for each day level (0 - 7). The predictive distribution of $\tilde{y}$ is $f(\tilde{y}|y) = \int f(\tilde{y}|p) f(p|y) dp$. Instead of integrating for the solution, run a simulation. Sample a thousand $p$'s from the posterior beta distribution, then use them sample a thousand event counts from the binomial distribution. 

```{r collapse=TRUE}
set.seed(12345)

sampled_p <- rbeta(10^3, a, b)

# mean and 95% CI predicted number of drinking days.
y_tilde <- rbinom(10^3, 7, sampled_p)
mean(y_tilde)
quantile(y_tilde, c(.025, .975))

# Mean and 95% CI for predicted number of respondents having [0,7] drinking days.
# For the 10^3 p's, simulate event count for 100 respondents drawing 7 items from 
# a binomial distribution.
sims <- map(sampled_p, ~rbinom(100, 7, .))
# Count the number of times [0,7] comes up. Average this across the 10^3 experiments. 
means <- map_dbl(days, function(x) map_int(sims, ~sum(. == x)) %>% mean())
lcl <- map_dbl(days, function(x) map_int(sims, ~sum(. == x)) %>% quantile(.025))
ucl <- map_dbl(days, function(x) map_int(sims, ~sum(. == x)) %>% quantile(.975))

tibble(days, respondents, means, lcl, ucl) %>%
  ggplot(aes(x = days)) +
  geom_col(aes(y = respondents)) +
  geom_point(aes(y = means)) +
  geom_errorbar(aes(ymin = lcl, ymax = ucl), width = .2) +
  scale_x_continuous(breaks = 0:7)
```

This is obviously not good, but there is a way to deal with zero inflation. Let $z_i = 1$ if respondent $i$ drinks alcohol at all. Then $(x_i|z_i = 1, p) \sim \text{Bin}(7, p)$, otherwise $P(x_i =0|z_i = 0) = 1$. Model the probability of $z_i = 0$ with the Bernoulli distribution, $z_i|\omega \sim \text{Bern}(\omega)$ where $\omega \sim \text{Beta}(a_\omega, b_\omega)$. $z_i$ is a latent variable, an unobserved attribute.

Implement a Gibbs Sampler with uniform priors for $p \sim \text{Beta}(a = 1, b = 1)$ and $\omega \sim \text{Beta}(a_\omega = 1, b_\omega = 1)$. Start with the assumption that if $x_i = 0$ then $z_i = 0$. For each respondent, if $x_i > 0$ then $z_i = 1$, otherwise 

$$
P(z_i = 1| x_i = 0, p, \omega) = \frac{(1-p)^7 \omega}{(1-\omega) + (1-p)^7 \omega}
$$

```{r}
set.seed(12345)

# Days consuming alcohol last week [0-7] for 100 respondents.
x <- rep(0:7, respondents)

# Start with assumption that if they reported 0, they _never_ drink (x_i=0 -> z_i=0).
z <- as.numeric(x >= 1)

# Run 1,000 iterations.
ITER <- 10^3

sim_p <- numeric(ITER)
sim_omega <- numeric(ITER)
sim_z <- matrix(nrow = length(x), ncol = ITER)

# Gibbs sampler.
for(iter in 1:ITER) {
  # Use uniform Beta(1,1) prior for whether or not the person is a drinker. Sample 
  # omega given the count of non-zero z's.
  omega <- rbeta(1, 1 + sum(z), 1 + (length(z) - sum(z)))
  
  # Sample p from beta distribution given the respondent count _when z=1_.
  # Use vague Beta(1,1) prior for probability of drinking on any given day.
  # Sample p given reported drinking days and number of people who are drinkers
  # on each day.
  p <- rbeta(1, 1 + sum(x), 1 + (7 * sum(z) - sum(x)))
  
  # Updated probability that respondent is a drinker. 
  # P(z=1|x>0) = 1 or P(z=1|x=0, p, omega).
  prob_z_eq_1 <- 
    as.numeric(x >= 1) * 1.0 + 
    as.numeric(x == 0) * ((1 - p)^7 * omega) / ((1 - omega) + ((1 - p)^7 * omega))

  z <- rbinom(100, 1, prob_z_eq_1)
  
  # update monitors
  sim_p[iter] <- p
  sim_omega[iter] <- omega
  sim_z[, iter] <- z
}

# Estimated probability of being an alcohol drinker.
mean(sim_omega)
quantile(sim_omega, c(.025, .975))

# Of the alcohol drinkers, the probability of drinking on any given day.
mean(sim_p)
quantile(sim_p, c(.025, .975))

# mean and 95% CI predicted number of [0,7] drinking days.
z_post_pred <- map(1:ITER, ~rbinom(100, 1, sim_omega))
x_post_pred <- map(1:ITER, ~rbinom(100, 7, sim_p))
# Override the probability with zero when z == 0.
x_post_pred <- map2(x_post_pred, z_post_pred, ~ if_else(.y == 0, 0, .x))
# Count the number of times [0,7] comes up. Average this across the 10^3 experiments.
means <- map_dbl(days, function(x) map_int(x_post_pred, ~sum(. == x)) %>% mean())
lcl <- map_dbl(days, function(x) map_int(x_post_pred, ~sum(. == x)) %>% quantile(.025))
ucl <- map_dbl(days, function(x) map_int(x_post_pred, ~sum(. == x)) %>% quantile(.975))

tibble(days, respondents, means, lcl, ucl) %>%
  ggplot(aes(x = days)) +
  geom_col(aes(y = respondents)) +
  geom_point(aes(y = means)) +
  geom_errorbar(aes(ymin = lcl, ymax = ucl), width = .2) +
  scale_x_continuous(breaks = 0:7)
```

Let's try another example of modeling with zero inflation. 94 tourists report how many fish they caught during their visit. Estimate the distribution of fish caught. Start with a vague prior, $\lambda \sim \text{Gamma}(a = .1, b = .1)$. The posterior distribution of $\lambda|y \sim \text{Gamma}(a + \sum_i{y_i}, b + n)$

```{r}
fish <- readr::read_csv("input/fish.csv", col_types = "i") %>%
  filter(!is.na(catch))

y <- tibble(catch = 0:max(fish$catch)) %>% 
  left_join(fish %>% count(catch), by = join_by(catch)) %>% 
  replace_na(list(n = 0)) %>%
  pull(n)

# Simulate 10^3 samples.
set.seed(12345)

# Vague priors for gamma(a, b).
a <- .01
b <- .01

# Sample gamma 10^3 times from the posterior distribution
sampled_lambda <- rgamma(10^3, a + sum(fish$catch), b + nrow(fish))
y_tilde <- rpois(10^3, sampled_lambda)

# mean and 95% CI predicted number of expected value of fish caught.
mean(y_tilde)
quantile(y_tilde, c(.025, .975))

# mean and 95% CI predicted number of [0, 9] fish caught.
# For 10^3 lambdas, create 94 samples from Poisson dist. 
sims <- map(sampled_lambda, ~rpois(nrow(fish), .))
# Count the number of times [0,10] comes up. Average this across the 10^3 experiments. 
catch <- 0:max(fish$catch)
means <- map_dbl(catch, function(x) map_int(sims, ~sum(. == x)) %>% mean())
lcl <- map_dbl(catch, function(x) map_int(sims, ~sum(. == x)) %>% quantile(.025))
ucl <- map_dbl(catch, function(x) map_int(sims, ~sum(. == x)) %>% quantile(.975))

tibble(catch, y, means, lcl, ucl) %>%
  ggplot(aes(x = catch)) +
  geom_col(aes(y = y)) +
  geom_point(aes(y = means)) +
  geom_errorbar(aes(ymin = lcl, ymax = ucl), width = .2) +
  scale_x_continuous(breaks = 0:10) +
  labs(title = "Zero inflation of fish caught.")
```

This doesn't look good. The probability of 27 people catching 0 fish when the expected number of fish is 2.9 is less than .001.

```{r}
# probability of 27 people with catching 0 fish given lambda = 2.9 is <.001.
(lambda_est <- (a + sum(fish$catch)) / (b + 94))
ppois(sum(fish$catch == 0), lambda = means[1], lower.tail = FALSE)
```

Add a binary latent variable to the model describing whether or not the person was fishing
Assign a uniform prior to the probability that the person was fishing
Derive posterior conditional distributions and construct a Gibbs sampler to estimate your model

```{r eval=FALSE}
set.seed(12345)

# Reported number of fish caught by 94 respondents.
x <- fish$catch

# Start with assumption that if they caught 0, they did not fish (x_i=0 -> z_i=0).
z <- as.numeric(x >= 1)

# Iterations to run.
ITER <- 10^3

sim_lambda <- numeric(ITER)
sim_omega <- numeric(ITER)
sim_z <- matrix(nrow = length(x), ncol = ITER)

# Gibbs sampler.
for(iter in 1:ITER) {
  # Use uniform Beta(1,1) prior for whether or not the person fished. Sample 
  # omega given the count of non-zero z's.
  omega <- rbeta(1, 1 + sum(z), 1 + (94 - sum(z)))
  
  # Use vague Gamma(.01,.01) prior for number of fish caught. Sample lambda 
  # given reported catches and number of people who fished.
  lambda <- rgamma(1, .01 + sum(x), .01 + sum(z))
  
  # Updated probability that respondent fishes.
  # E(z=1|x) = 1 or P(1|x=0, p, omega).
  prob_x_eq_0 <- exp(-lambda) * lambda^0 / factorial(0)
  prob_z_eq_1 <- 
    as.numeric(x >= 1) * 1.0 + 
    as.numeric(x == 0) * (prob_x_eq_0 * omega) / ((1 - omega) + (prob_x_eq_0 * omega))

  z <- rbinom(length(x), 1, prob_z_eq_1)

  # update monitors
  sim_lambda[iter] <- lambda
  sim_omega[iter] <- omega
  sim_z[, iter] <- z
}

# Estimate of tourists who fished.
mean(sim_omega)
quantile(sim_omega, c(.025, .975))

# Of those who fished, the expected number caught.
mean(sim_lambda)
quantile(sim_lambda, c(.025, .975))
# or is it this (E(X) = a/b? Not this either.
(.01 + sum(x)) / (.01 + sum(z))

# P(x>=94|lambda,omega)
# probability of catch = 27 given lambda = 2.9 is <.001
(lambda_est <- (a + sum(fish$catch)) / (b + 94))
ppois(sum(fish$catch == 0), lambda = means[1], lower.tail = FALSE)

# mean and 95% CI predicted number of [0,9] fish caught.
z_post_pred <- map(1:ITER, ~rbinom(length(z), 1, sim_omega))
x_post_pred <- map(1:ITER, ~rpois(length(x), sim_lambda))
# Override the count with zero when z == 0.
x_post_pred <- map2(x_post_pred, z_post_pred, ~ if_else(.y == 0, 0, .x))
# Count the number of times [0,9] comes up. Average this across the 10^3 experiments.
means <- map_dbl(0:9, function(x) map_int(x_post_pred, ~sum(. == x)) %>% mean())
lcl <- map_dbl(0:9, function(x) map_int(x_post_pred, ~sum(. == x)) %>% quantile(.025))
ucl <- map_dbl(0:9, function(x) map_int(x_post_pred, ~sum(. == x)) %>% quantile(.975))

tibble(fish = 0:9, y, means, lcl, ucl) %>%
  ggplot(aes(x = fish)) +
  geom_col(aes(y = y)) +
  geom_point(aes(y = means)) +
  geom_errorbar(aes(ymin = lcl, ymax = ucl), width = .2) +
  scale_x_continuous(breaks = 0:10)
```



## DIC

Use the Deviance Information Criterion (DIC) to compare group means. 

$$
DIC = p_D + \overline{D(\theta)}
$$

where $p_D = \overline{D(\theta)} - D(\hat{\theta})$ and $D(\theta) = -2 \log (f(y|\theta)) + C$.

Evaluate $\overline{D(\theta)}$ by producing samples from each distribution and evaluating the likelihoods of the data based on each realization and taking the mean of -2 log-likelihood.

```{r}
# Two samples with success rates 35/50 and 15/20
y <- c(35, 15)
n <- c(50, 20)

# Priors
a <- 1
b <- 1

# Posteriors
post <- list(
  rbeta(10^3, a+y[1], b+n[1]-y[1]),
  rbeta(10^3, a+y[2], b+n[2]-y[2])
)

# -2 * Mean log-likelihood
ll <- 
  dbinom(y[1], n[1], post[[1]], log = TRUE) +
  dbinom(y[2], n[2], post[[2]], log = TRUE)
(mean_D <- mean(-2 * ll))

# D(theta-bar) is the likelihood of the data based on the posterior means of p.
(D_mean <- -2 * (
  dbinom(y[1], n[1], (a+y[1]) / (a+y[1] + b+n[1]-y[1]), log = TRUE) +
  dbinom(y[2], n[2], (a+y[2]) / (a+y[2] + b+n[2]-y[2]), log = TRUE)
))

#p_D and DIC from equation
(p_D <- mean_D - D_mean)
(DIC <- p_D + mean_D)

# Repeat these steps for a single model of both groups
post_group <- rbeta(10^3, a+sum(y), b+sum(n)-sum(y))
ll_group <- 
  dbinom(y[1], n[1], post_group, log = TRUE) +
  dbinom(y[2], n[2], post_group, log = TRUE)
(mean_D_group <- mean(-2 * ll_group))
(D_mean_group <- -2 * (
  dbinom(y[1], n[1], (a+sum(y)) / (a+sum(y) + b+sum(n)-sum(y)), log = TRUE) +
  dbinom(y[2], n[2], (a+sum(y)) / (a+sum(y) + b+sum(n)-sum(y)), log = TRUE)
))
(p_D_group <- mean_D_group - D_mean_group)
(DIC_group <- p_D_group + mean_D_group)
```

The DIC for the two group model is `r DIC` and for the one common group model it is `r DIC_group`. The DIC for one common group model is smaller, so we do not have enough statistical evidence for two groups. If the DIC for the group-specific model is at least 3 units smaller than that for the common model, there is sufficient statistical evidence for difference between groups.

## Appendix: Gentle Introduction

Suppose you purchase 100 ad impressions on a web site and receive 13 clicks. How would you describe the click rate? The classical approach is to construct a 95% CI around the click proportion with a [one-sample proportion test](https://mpfoley73.github.io/statistics/one-sample-proportion-z-test.html).^[This section is my notes from DataCamp course [Fundamentals of Bayesian Data Analysis in R](https://learn.datacamp.com/courses/fundamentals-of-bayesian-data-analysis-in-r). It is an intuitive approach to Bayesian inference.]

```{r}
(ad_prop_test <- prop.test(13, 100))
```

```{r fig.height=3, warning=FALSE, echo=FALSE}
mu <- ad_prop_test$estimate
se <- sqrt(mu * (1 - mu)) / sqrt(100)
theta <- seq(0.01, .30, by = .001)
likelihood <- dnorm(theta, mu, se)
# chisq test
tibble(
  theta,
  e_1 = 13,
  e_2 = 87,
  o_1 = theta * 100,
  o_2 = 100 - o_1,
  x2 = (o_1 - e_1)^2 / e_1 + (o_2 - e_2)^2 / e_2,
  likelihood = pchisq(x2, 1, lower.tail = FALSE),
  lcl = if_else(theta <= ad_prop_test$conf.int[1], likelihood, NA_real_),
  ucl = if_else(theta >= ad_prop_test$conf.int[2], likelihood, NA_real_)
) %>%
  ggplot(aes(x = theta)) +
  geom_line(aes(y = likelihood)) +
  geom_area(aes(y = lcl), fill = "firebrick", alpha = .4) +
  geom_area(aes(y = ucl), fill = "firebrick", alpha = .4) +
  geom_vline(xintercept = mu, linetype = 2) +
  theme_minimal() +
  scale_x_continuous(breaks = seq(0, .3, .05)) +
  theme(panel.grid.minor = element_blank()) +
  labs(title = "Frequentist proportion test for 13 clicks in 100 impressions", 
       subtitle = glue("p = {mu}, 95%-CI (",
       "{ad_prop_test$conf.int[1] %>% scales::number(accuracy = .01)}, ",
       "{ad_prop_test$conf.int[2] %>% scales::number(accuracy = .01)})"),
       x = expression(theta))
```

How might you estimate $\theta$ with Bayesian statistics? One approach is to run 1,000 experiments that sample 100 ad impression events from an `rbinom()` generative model using a uniform prior distribution of 0-30% click probability. The resulting 1,000 row data set of click probabilities is the prior predictive distribution of $D$, the denominator of Bayes' Theorem. The subset where $D = 13$ is the likelihood, $P(D|\theta)$. The sampled $\theta$'s are the prior, $P(\theta) = \text{unif}(.0, .3)$. Their product is the joint probability distribution, $P(D|\theta)P(\theta)$, the numerator of Bayes' Theorem. This method is called *rejection sampling* because you sample across the whole parameter space, then condition on the observed evidence.

```{r}
df_sim <- tibble(
  click_prob = runif(1000, 0.0, 0.3),
  click_n = rbinom(1000, 100, click_prob)
)

df_sim %>% 
  mutate(is_13 = factor(click_n == 13, levels = c(TRUE, FALSE))) %>%
  ggplot(aes(x = click_prob, y = click_n, color = is_13)) +
  geom_point(alpha = 0.6, show.legend = FALSE) +
  geom_hline(yintercept = 13, color = "steelblue", linetype = 1, linewidth = .5) +
  scale_color_manual(values = c("TRUE" = "steelblue", "FALSE" = "gray80")) +
  scale_y_continuous(breaks = c(seq(0, 40, 10), 13)) +
  labs(title = "Joint probability of observed clicks and click probability",
       subtitle = "with conditioning on 13 observed clicks.",
       y = "clicks per 100 ads",
       x = expression(theta))
```

Condition the joint probability distribution on the `r df_sim %>% filter(click_n == 13) %>% nrow()` rows that produced 13 observed clicks to update the prior. The `quantile()` function returns the median and the .025 and .975 percentile values - the *credible interval*.

```{r}
# median and credible interval
(sim_ci <- df_sim %>% filter(click_n == 13) %>% pull(click_prob) %>% 
  quantile(c(.025, .5, .975)))
```

The posterior click rate likelihood is `r percent(sim_ci[2], .1)` with 95% credible interval (`r percent(sim_ci[1], .1)`, `r percent(sim_ci[3], .1)`). Here is the density plot of the `r df_sim %>% filter(click_n == 13) %>% nrow()` simulations that produced the 13 clicks. The median and 95% credible interval are marked.

```{r}
df_sim %>% 
  filter(click_n == 13) %>%
  ggplot(aes(x = click_prob)) +
  geom_density() +
  geom_vline(xintercept = sim_ci[2]) +
  geom_vline(xintercept = sim_ci[1], linetype = 2) +
  geom_vline(xintercept = sim_ci[3], linetype = 2) +
  # coord_cartesian(xlim = c(0, .3)) +
  scale_x_continuous(breaks = c(seq(0, .3, .05), sim_ci), labels = percent_format(.1)) +
  labs(title = "Posterior click likelihood distribution", 
       subtitle = glue("p = {percent(sim_ci[2], .1)}, 95%-CI (",
       "{percent(sim_ci[1], .1)}, {percent(sim_ci[3], .1)})"),
       x = expression(theta), y = "density (likelihood)")
```

That's pretty close to the classical result! Instead of sampling, you could define a discrete set of candidate click probabilities and calculate the click probability density for the 100 ad impressions. This method is called *grid approximation*.

```{r}
df_bayes <- expand.grid(
  click_prob = seq(0.01, .30, by = .001), 
  click_n = 0:100
) %>%
  mutate(
    prior = dunif(click_prob, min = 0, max = 0.3),
    likelihood = dbinom(click_n, 100, click_prob),
    probability = likelihood * prior / sum(likelihood * prior)
  )

df_bayes %>% 
  mutate(is_13 = factor(click_n == 13, levels = c(TRUE, FALSE))) %>%
  # filter(probability > .0001) %>%
  ggplot(aes(x = click_prob, y = click_n, color = is_13)) +
  geom_point(aes(color = probability), show.legend = FALSE, size = 1) +
  geom_hline(yintercept = 13, color = "steelblue", linetype = 1, linewidth = .5) +
  coord_cartesian(ylim = c(0, 50)) +
  scale_y_continuous(breaks = c(seq(0, 50, 10), 13)) +
  scale_color_gradient(low = "#FFFFFF", high = "steelblue") +
  # scale_color_manual(values = c("TRUE" = "steelblue", "FALSE" = "gray20")) +
  labs(title = "Joint probability of clicks and click probability.",
       subtitle = "with conditioning on 13 observed clicks.",
       y = "clicks per 100 ads",
       x = expression(theta))
```

Condition the joint probability distribution on the 13 observed clicks to update your prior. Resample the posterior probability to create a distribution.

```{r}
df_bayes_13 <- df_bayes %>% filter(click_n == 13) %>%
  mutate(posterior = probability / sum(probability))

sampling_idx <- sample(
  1:nrow(df_bayes_13), 
  size = 10000, 
  replace = TRUE, 
  prob = df_bayes_13$posterior
)

sampling_vals <- df_bayes_13[sampling_idx, ]

(df_bayes_ci <- quantile(sampling_vals$click_prob, c(.025, .5, .975)))
```

```{r echo=FALSE, warning=FALSE}
df_bayes %>%
  filter(click_n == 13) %>%
  mutate(likelihood = probability / sum(probability),
         lcl = if_else(click_prob < df_bayes_ci[1], likelihood, NA_real_),
         ucl = if_else(click_prob > df_bayes_ci[3], likelihood, NA_real_), 
         ci = if_else(click_prob >= df_bayes_ci[1] & 
                        click_prob <= df_bayes_ci[3], "Y", "N")) %>%
  ggplot(aes(x = click_prob, y = likelihood)) +
  geom_line() +
  geom_area(aes(y = lcl), fill = "firebrick", alpha = .4) +
  geom_area(aes(y = ucl), fill = "firebrick", alpha = .4) +
  geom_vline(xintercept = df_bayes_ci[2], linetype = 2) +
  theme_minimal() +
  scale_x_continuous(breaks = seq(0, .3, .05)) +
  theme(panel.grid.minor = element_blank()) +
  labs(title = "Posterior click probability", 
       subtitle = glue("p = {df_bayes_ci[2] %>% scales::number(accuracy = .01)}, 95%-CI (",
       "{df_bayes_ci[1] %>% scales::number(accuracy = .01)}, ",
       "{df_bayes_ci[3] %>% scales::number(accuracy = .01)})"),
       x = expression(theta))
```

You can use a Bayesian model to estimate multiple parameters. Suppose you want to predict the water temperature in a lake on Jun 1 based on 5 years of prior water temperatures. 

```{r}
temp <- c(19, 23, 20, 17, 23)
```

You model the water temperature as a normal distribution, $\mathrm{N}(\mu, \sigma^2)$ with a prior distribution $\mu = \mathrm{N}(18, 5^2)$ and $\sigma = \mathrm{unif}(0, 10)$ based on past experience.

Using the grid approximation approach, construct a grid of candidate $\mu$ values from 8 to 30 degrees incremented by .5 degrees, and candidate $\sigma$ values from .1 to 10 incremented by .1 - a 4,500 row data frame. 

```{r}
mdl_grid <- expand_grid(mu = seq(8, 30, by = 0.5),
                        sigma = seq(.1, 10, by = 0.1))
```

For each combination of $\mu$ and $\sigma$, the *prior* probabilities are the densities from $\mu = \mathrm{N}(18, 5^2)$ and $\sigma = \mathrm{unif}(0, 10)$. The combined prior is their product. The *likelihoods* are the products of the probabilities of observing each `temp` given the candidate $\mu$ and $\sigma$ values.

```{r}
mdl_grid_2 <- mdl_grid %>%
  mutate(
    mu_prior = map_dbl(mu, ~dnorm(., mean = 18, sd = 5)),
    sigma_prior = map_dbl(sigma, ~dunif(., 0, 10)),
    prior = mu_prior * sigma_prior, # combined prior,
    likelihood = map2_dbl(mu, sigma, ~dnorm(temp, .x, .y) %>% prod()),
    posterior = likelihood * prior / sum(likelihood * prior)
  )
```

```{r echo=FALSE}
mdl_grid_2 %>%
  ggplot(aes(x = mu, y = sigma, fill = posterior)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "steelblue", guide = "colorbar") +
  theme_minimal() +
  labs(title = "Joint probability of mu and sigma.",
       x = expression(mu), y = expression(sigma))
```

Calculate a credible interval by drawing 10,000 samples from the grid with sampling probability equal to the calculated posterior probabilities. Use the `quantile()` function to estimate the median and .025 and .975 quantile values.

```{r fig.height=3}
sampling_idx <- sample(1:nrow(mdl_grid), size = 10000, replace = TRUE, prob = mdl_grid$posterior)
sampling_vals <- mdl_grid[sampling_idx, c("mu", "sigma")]
mu_ci <- quantile(sampling_vals$mu, c(.025, .5, .975))
sigma_ci <- quantile(sampling_vals$sigma, c(.025, .5, .975))
ci <- qnorm(c(.025, .5, .975), mean = mu_ci[2], sd = sigma_ci[2])

data.frame(temp = seq(0, 30, by = .1)) %>%
  mutate(prob = map_dbl(temp, ~dnorm(., mean = ci[2], sd = sigma_ci[2])),
         ci = if_else(temp >= ci[1] & temp <= ci[3], "Y", "N")) %>%
  ggplot(aes(x = temp, y = prob)) +
  geom_area(aes(y = if_else(ci == "N", prob, 0)), 
            fill = "firebrick", show.legend = FALSE) +
  geom_line() +
  geom_vline(xintercept = ci[2], linetype = 2) +
  theme_minimal() +
  scale_x_continuous(breaks = seq(0, 30, 5)) +
  theme(panel.grid.minor = element_blank()) +
  labs(title = "Posterior temperature probability", 
       subtitle = glue("mu = {ci[2] %>% scales::number(accuracy = .1)}, 95%-CI (",
       "{ci[1] %>% scales::number(accuracy = .1)}, ",
       "{ci[3] %>% scales::number(accuracy = .1)})"))
```

What is the probability the temperature is at least 18?

```{r}
pred_temp <- rnorm(1000, mean = sampling_vals$mu, sampling_vals$sigma)
scales::percent(sum(pred_temp >= 18) / length(pred_temp))
```

## Further Reading

Gelman A, Carlin JB, Stern HS, Dunson DB, Vehtari A, Rubin DB. Bayesian data analysis. CRC press. The book any Bayesian statistician should have read at least once. Considered the definitive textbook on the topic.

McElreath R. Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman and Hall/CRC A somewhat more friendly introduction to Bayesian statistics.



